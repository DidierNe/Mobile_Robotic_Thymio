{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84588a6d-d18e-4ad6-8d06-e251d942fb7c",
   "metadata": {},
   "source": [
    "# [MICRO-452:] Project Report - Groupe 28\n",
    "**Authors:** Celest Angela Tjong, Adrien Louis Baptiste Dupont, Luca Sidoti Pinto, Didier Henri Neuenschwander\n",
    "**Supervisors:** Prof. Francesco Mondada\n",
    "Date: 17 Novembre 2023\n",
    "\n",
    "[MICRO-452]: **to be changed**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2258c98-b00d-4caf-a0a6-b64e2262d6e0",
   "metadata": {},
   "source": [
    "\n",
    "<span style=\"color:red; font-size:40px;\">use as few personal pronouns as possible (we, our, etc.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "034c8421-4f25-49b6-8bc0-f73cc5369cd5",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "* [1. Introduction](#introduction)\n",
    "* [2. Vision](#vision)\n",
    "    * [2.1. Subsection 1](#vision-subsection-1)\n",
    "    * [2.2. Subsection 2](#vision-subsection-2)\n",
    "* [3. Global Navigation](#global-navigation)\n",
    "* [4. Filtering](#filtering)\n",
    "* [5. Local Navigation](#local-navigation)\n",
    "* [6. Conclusion](#conclusion)\n",
    "lusion)\n",
    "clusion)\n",
    "usion)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd4e09dc-1142-46dd-8485-6bc9b292bf37",
   "metadata": {},
   "source": [
    "## 1 Introduction\n",
    "<a id=\"introduction\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df62e044-49e6-4d91-a7fa-89e01de6e9ff",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'numpy._DTypeMeta' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_13896/368503.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcolors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mPIL\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mImage\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\cv2\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    179\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    180\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 181\u001b[1;33m \u001b[0mbootstrap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\cv2\\__init__.py\u001b[0m in \u001b[0;36mbootstrap\u001b[1;34m()\u001b[0m\n\u001b[0;32m    173\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    174\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0msubmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0m__collect_extra_submodules\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDEBUG\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 175\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[0m__load_extra_py_code_for_module\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"cv2\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msubmodule\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDEBUG\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    176\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mDEBUG\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Extra Python code for\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msubmodule\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"is loaded\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\cv2\\__init__.py\u001b[0m in \u001b[0;36m__load_extra_py_code_for_module\u001b[1;34m(base, name, enable_debug_print)\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[0mnative_module\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodule_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m         \u001b[0mpy_module\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodule_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0menable_debug_print\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\importlib\\__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m    125\u001b[0m                 \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    126\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 127\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    128\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\cv2\\typing\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNumpyVersion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;34m\"1.20.0\"\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mversion_info\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m9\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 69\u001b[1;33m     \u001b[0mNumPyArrayGeneric\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtyping\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAny\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgeneric\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     70\u001b[0m \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m     \u001b[0mNumPyArrayGeneric\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'numpy._DTypeMeta' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import colors\n",
    "from PIL import Image\n",
    "import math\n",
    "import time\n",
    "from ipywidgets import interactive\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43bfb7cc-aff3-468a-855e-2ef86f760444",
   "metadata": {},
   "source": [
    "## 2 Vision\n",
    "<a id=\"vision\"></a>\n",
    "https://docs.opencv.org/4.x/df/d9d/tutorial_py_colorspaces.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2add8a4f-6198-4070-8728-8f1b016321d7",
   "metadata": {},
   "source": [
    "### Vision Subsection 1\n",
    "<a id=\"Vision-subsection-1\"></a>\n",
    "**Ask to TA if we have to describe every function used in the notebook?**\n",
    "1. **Color Space Conversion**:\n",
    "   -In order to have consistent result for the detection, it is standard to convert     The function begins by converting the input image from the BGR color space (standard in OpenCV) to the HSV color space using `cv2.cvtColor`. HSV (Hue, Saturation, Value) is often more effective for color filtering. Indeed it is particulary usefull for image processing  because it separates color information (hue) from intensity or lighting (value). Thus it allows the recognition to be less dependant of the lighting condition, as it is possible to modify theses parameter. In order to fix the variables of the color used, it is a good practise to calibrate the calibrate the HSV in case of light changes. It has been done with the function...\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "1. **Color Space Conversion**:\n",
    "   -In order to have consistent result for the detection, it is standard to convert     The function begins by converting the input image from the BGR color space (standard in OpenCV) to the HSV color space using `cv2.cvtColor`. HSV (Hue, Saturation, Value) is often more effective for color filtering. Indeed it is particulary usefull for image processing  because it separates color information (hue) from intensity or lighting (value). It allows the recognition to be less dependant of the lighting condition, as it is possible to modify theses parameter. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f86f29-1250-4c61-bda7-d2bb2c9bd996",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define color thresholds in HSV\n",
    "# Note: these thresholds may need to be adjusted for your specific image conditions\n",
    "lower_red_bound = np.array([120, 100, 70])\n",
    "upper_red_bound = np.array([255, 255, 255])\n",
    "lower_green_bound = np.array([60, 50, 100])\n",
    "upper_green_bound = np.array([100, 255, 255])\n",
    "lower_yellow_bound = np.array([0, 50, 120])\n",
    "upper_yellow_bound = np.array([40, 105, 255])\n",
    "lower_black_bound = np.array([0, 0, 0])\n",
    "upper_black_bound = np.array([255, 255, 130])\n",
    "lower_blue_bound = np.array([90, 80, 0])\n",
    "upper_blue_bound = np.array([105, 255, 255])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e07e93e-eef0-40ac-9d5a-eb0bd9dacd52",
   "metadata": {},
   "source": [
    "The first think to do is to preprocess the image for the colour object needed to be located. It is done by first creating a mask. The mask is created by specifying a range of colors (in HSV color space). Pixels within this color range are marked as 1 (or true), while all other pixels are marked as 0 (or false). it is created using `cv2.inRange` which filters out all colors except those within the specified `lower_color_bound` and `upper_color_bound`. This step isolated the specified color. \n",
    "Then the function `cv2.bitwise_and`, extract the area corresponding with the range of colour of the image given as input. It is done by comaparing each pixel of the image with the mask (same size of the image, comaparing with a logical &). \n",
    "Finally The color-filtered image is converted to grayscale using `cv2.cvtColor` because the subsequent edge detection step (Canny) requires a single-channel image.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ef7079-7a86-4716-8507-3cfe19dc34cb",
   "metadata": {},
   "source": [
    "In order to detect the different coloured form, it is common to beginby using a canny filter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa86aa6-a564-4a7d-81b0-1cb0807e6e8a",
   "metadata": {},
   "source": [
    "### Function: `detect_color_circle`\n",
    "\n",
    "#### Purpose:\n",
    "The `detect_color_circle` function is designed to detect circles of a specific color in an image. It employs color filtering, Canny edge detection, and the Hough Circle Transform to achieve this.\n",
    "\n",
    "#### Process:\n",
    "\n",
    "\n",
    "\n",
    "2. **Color Masking**:\n",
    "   - A mask is created using `cv2.inRange` which filters out all colors except those within the specified `lower_color_bound` and `upper_color_bound`. This step isolates the regions of the specified color.\n",
    "\n",
    "3. **Mask Application**:\n",
    "   - The mask is then applied to the original image using `cv2.bitwise_and`. This step ensures that only the parts of the image with the desired color are retained for further processing.\n",
    "\n",
    "4. **Grayscale Conversion**:\n",
    "   - The color-filtered image is converted to grayscale using `cv2.cvtColor` because the subsequent edge detection step (Canny) requires a single-channel image.\n",
    "\n",
    "5. **Canny Edge Detection**:\n",
    "   - `cv2.Canny` is applied to detect edges in the image. It works by identifying areas in the image where sharp changes in intensity occur. The function takes two threshold values (here, 100 and 200) that determine the sensitivity of the edge detection. Edges that are found are used as input for the circle detection.\n",
    "\n",
    "6. **Hough Circle Transform**:\n",
    "   - `cv2.HoughCircles` is used to detect circles in the image. It operates on the principle of the Hough Transform, which is a feature extraction technique used in image analysis. The function detects circles by finding sets of edge points that form a circular shape.\n",
    "   - Parameters like `param1` (higher threshold of the two passed to the Canny edge detector), `param2` (threshold for center detection in the Hough Transform), `minRadius`, and `maxRadius` control the sensitivity and size of the circles to be detected.\n",
    "\n",
    "7. **Output**:\n",
    "   - If circles are detected, the function returns a list of tuples, each containing the `(x, y)` coordinates of the center of a circle and its radius. If no circles are found, it returns an empty list.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b67ae3-5774-4f12-9345-9cc499b15b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import tdmclient Notebook environment:\n",
    "#import tdmclient.notebook\n",
    "#await tdmclient.notebook.start()\n",
    "# forward\n",
    "#motor_left_target= 100\n",
    "#motor_right_target= 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "926418ed-c1c2-4aa3-ac23-8f7630815551",
   "metadata": {},
   "source": [
    "Function that record a video and save it on the folder of the project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa1ae44-55a6-49f6-b96a-0854adea57c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "video_capture = cv2.VideoCapture(0, cv2.CAP_DSHOW)\n",
    "\n",
    "video_capture.set(cv2.CAP_PROP_FRAME_WIDTH, 1080)\n",
    "video_capture.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)\n",
    "# Obtenir les dimensions de la frame\n",
    "frame_width = int(video_capture.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(video_capture.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "# Définir le codec et créer un objet VideoWriter\n",
    "fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "out = cv2.VideoWriter('output.avi', fourcc, 20.0, (frame_width, frame_height))\n",
    "\n",
    "# Enregistrer la vidéo\n",
    "while video_capture.isOpened():\n",
    "    ret, frame = video_capture.read()\n",
    "    if ret:\n",
    "        # Écrire la frame dans le fichier\n",
    "        out.write(frame)\n",
    "\n",
    "        # Afficher la frame (si vous voulez voir le flux en temps réel)\n",
    "        cv2.imshow('Frame', frame)\n",
    "\n",
    "        # Quitter avec la touche 'q'\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "    else:\n",
    "        break\n",
    "\n",
    "# Libérer les ressources\n",
    "video_capture.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32ca27f-3812-4957-ba8b-73ae4c6c68b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Define a function to detect circles of a specific color\n",
    "def detect_color_circle(image, lower_color_bound, upper_color_bound):\n",
    "    # Convert to HSV color space\n",
    "    hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "    # Create a mask for the specified color\n",
    "    mask = cv2.inRange(hsv, lower_color_bound, upper_color_bound)\n",
    "\n",
    "    # Apply the mask to the original image\n",
    "    color_only = cv2.bitwise_and(image, image, mask=mask)\n",
    "\n",
    "    # Convert to grayscale for circle detection\n",
    "    gray = cv2.cvtColor(color_only, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Apply Canny edge detection to help with circle detection\n",
    "    edges = cv2.Canny(gray, 100, 200)\n",
    "\n",
    "    # Use Hough Transform to detect circles\n",
    "    circles = cv2.HoughCircles(edges, cv2.HOUGH_GRADIENT, 1, 20,\n",
    "                           param1=20, param2=15, minRadius=10, maxRadius=50)\n",
    "    \n",
    "    # If circles are detected, return the list of circles with x, y coordinates and radius\n",
    "    if circles is not None:\n",
    "        # Convert the (1, N, 3) array to (N, 3)\n",
    "        circles = np.uint16(np.around(circles[0, :]))          \n",
    "        return [(circle[0], circle[1], circle[2]) for circle in circles]\n",
    "    else:\n",
    "        return []\n",
    "    \n",
    "            \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f4f67d-21ce-4785-a6f9-de19714850bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_obstacle_mask(image, contours, kernel_size):\n",
    "    \"\"\"\n",
    "    Create a mask with zeros in the areas inside the dilated contours.\n",
    "\n",
    "    :param image: Input image.\n",
    "    :param contours: Contours to dilate and fill in the mask.\n",
    "    :param kernel_size: Size of the kernel used for dilation.\n",
    "    :return: Mask with zeros inside the dilated contours and ones elsewhere.\n",
    "    \"\"\"\n",
    "    # Create an empty mask of the same size as the image\n",
    "    h, w = image.shape[:2]\n",
    "    mask = np.ones((h, w), dtype=np.uint8)\n",
    "\n",
    "    # Perform dilation to increase the size of the black regions\n",
    "    kernel = np.ones((kernel_size, kernel_size), np.uint8)\n",
    "    for contour in contours:\n",
    "        # Create an individual mask for each contour\n",
    "        contour_mask = np.zeros((h, w), dtype=np.uint8)\n",
    "        cv2.fillPoly(contour_mask, [contour], 255)\n",
    "        contour_mask = cv2.dilate(contour_mask, kernel, iterations=1)\n",
    "        \n",
    "        # Combine the individual mask with the global mask\n",
    "        mask = cv2.bitwise_and(mask, cv2.bitwise_not(contour_mask))\n",
    "\n",
    "        #also add the contours\n",
    "        # Let's create a border around the image\n",
    "        border_size = 50\n",
    "        border_color = [0, 0, 0]  # Black border\n",
    "        # Use cv2.copyMakeBorder to add a border around the image\n",
    "        mask_with_border = cv2.copyMakeBorder(mask, border_size, border_size, border_size, border_size,\n",
    "                                           cv2.BORDER_CONSTANT, value=border_color)\n",
    "    \n",
    "    return mask_with_border\n",
    "\n",
    "\n",
    "# Now you have a mask with zeros in the obstacle areas and ones elsewhere\n",
    "# You can return this mask from your function or process it further as needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd427fec-563d-47a2-8b57-8ae7e5731492",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "def scale_contour(contour, scale):\n",
    "    M = cv2.moments(contour)\n",
    "    if M['m00'] == 0:\n",
    "        return contour\n",
    "    cx = int(M['m10']/M['m00'])\n",
    "    cy = int(M['m01']/M['m00'])\n",
    "    scaled_contour = np.zeros_like(contour)\n",
    "    for i, point in enumerate(contour):\n",
    "        x, y = point[0]\n",
    "        dx = x - cx\n",
    "        dy = y - cy\n",
    "        scaled_contour[i] = [[int(cx + dx * scale), int(cy + dy * scale)]]\n",
    "    return scaled_contour\n",
    "\n",
    "def detect_obstacle_contours(image, area_threshold, scale_factor):\n",
    "    hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "    mask_black = cv2.inRange(hsv, lower_black_bound, upper_black_bound)\n",
    "    contours, _ = cv2.findContours(mask_black, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    filtered_contours = [cnt for cnt in contours if cv2.contourArea(cnt) > area_threshold]\n",
    "\n",
    "    scaled_contours = [scale_contour(cnt, scale_factor) for cnt in filtered_contours]\n",
    "    \n",
    "    contour_image = image.copy()\n",
    "    cv2.drawContours(contour_image, filtered_contours, -1, (0, 255, 0), 2)\n",
    "    cv2.drawContours(contour_image, scaled_contours, -1, (0, 0, 255), 2)\n",
    "    return contour_image, filtered_contours, scaled_contours\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7151aada-0dc7-4d3d-9514-dbfca0ad35fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def detect_obstacle_contours(image, area_threshold, kernel_size):\n",
    "    \"\"\"\n",
    "    Detects and dilates obstacle contours in the given image.\n",
    "    :param image: Input image.\n",
    "    :param area_threshold: Area threshold for filtering contours.\n",
    "    :param kernel_size: Size of the kernel used for dilation.\n",
    "    :return: Image with obstacle contours drawn.\n",
    "    \"\"\"\n",
    "    hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "    mask_black = cv2.inRange(hsv, lower_black_bound, upper_black_bound)\n",
    "    contours, _ = cv2.findContours(mask_black, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    filtered_contours = [cnt for cnt in contours if cv2.contourArea(cnt) > area_threshold]\n",
    "    kernel = np.ones((kernel_size, kernel_size), np.uint8)\n",
    "    mask_dilated = cv2.dilate(mask_black, kernel, iterations=1)\n",
    "    dilated_contours, _ = cv2.findContours(mask_dilated, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    filtered_dilated_contours = [cnt for cnt in dilated_contours if cv2.contourArea(cnt) > area_threshold+10000]\n",
    "    contour_image = image.copy()\n",
    "    cv2.drawContours(contour_image, filtered_contours, -1, (0, 255, 0), 2)\n",
    "    cv2.drawContours(contour_image, filtered_dilated_contours, -1, (0, 0, 255), 2)\n",
    "    return contour_image, filtered_contours, filtered_dilated_contours\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13047d64-3da8-4f07-b297-a7fd8fc28292",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_obstacle_matrix(image, dilated_contours):\n",
    "    height, width = image.shape[:2]\n",
    "    obstacle_matrix = np.ones((height, width), dtype=np.uint8)\n",
    "\n",
    "    for contour in dilated_contours:\n",
    "        # Remplir chaque contour dilaté avec 0 (obstacle)\n",
    "        cv2.fillPoly(obstacle_matrix, [contour], 0)\n",
    "\n",
    "    return obstacle_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3675f92-3883-46ae-9b15-f1df15e42525",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_roi_from_circles(image, circles):\n",
    "    if circles is not None and len(circles) >= 4:\n",
    "        # Assurez-vous que les points sont dans le format correct\n",
    "        points = np.array([circle[:2] for circle in circles], dtype=np.float32)\n",
    "\n",
    "        # Calcul de la boîte englobante\n",
    "        rect = cv2.boundingRect(points)\n",
    "\n",
    "        # Recadrage de l'image\n",
    "        x, y, w, h = rect\n",
    "        cropped_image = image[y:y+h, x:x+w]\n",
    "        return cropped_image,(x, y, w, h)\n",
    "    else:\n",
    "        print(\"Nombre insuffisant de cercles détectés ou format incorrect.\")\n",
    "        return image,None "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e39f10-cfe2-4ce8-a98c-7aaf8a7ffec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_blue_rectangle_center(image, lower_yellow_bound, upper_yellow_bound):\n",
    "    # Convertir l'image en espace de couleur HSV\n",
    "    hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "    # Créer un masque pour la couleur bleue\n",
    "    mask = cv2.inRange(hsv, lower_yellow_bound, upper_yellow_bound)\n",
    "\n",
    "    # Trouver les contours dans le masque\n",
    "    contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    # Identifier le contour qui correspond au rectangle bleu\n",
    "    for contour in contours:\n",
    "        # Optionnel : vérifier si le contour est suffisamment grand ou a une forme spécifique\n",
    "\n",
    "        # Calculer la boîte englobante pour le contour\n",
    "        x, y, w, h = cv2.boundingRect(contour)\n",
    "\n",
    "        # Calculer le centre du rectangle\n",
    "        center = (x + w // 2, y + h // 2)\n",
    "        return center\n",
    "\n",
    "    return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee2f0da7-ba21-4f7f-8161-3b76a6e77880",
   "metadata": {},
   "source": [
    "In each frame, there are multiple elements that need to be displayed. The first category includes static elements such as the contours of obstacles and the goal point. Since these do not change over time, it is computationally more efficient to identify and locate them in the first frame, and then display them consistently in subsequent frames.\n",
    "\n",
    "On the other hand, elements related to the robot's localization must be determined in each frame. To gather information about its orientation and location, three circles are placed on the top of the robot. A green circle is located at the middle back, and two red circles are positioned on each side, with their centers aligned with the robot's ceeris. This arrangement facilitates the extraction of necessary information. By connecting the centers of the two red circles and creating a midpoint, a vector c thenan be formed by connecting this midpoint to the center of the green circ.It might also be feasible to use only two markers (circles), one at the back and one at the front, distinguished by their color. However, the marker at the front would hide the press button.le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c398cb-5d83-4997-9cf5-b3fdf0741126",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def capture_frame(video_capture):\n",
    "    \"\"\"\n",
    "    Capture a frame from the video capture object.\n",
    "    :param video_capture: The video capture object.\n",
    "    :return: The captured frame.\n",
    "    \"\"\"\n",
    "    ret, frame = video_capture.read()\n",
    "    if not ret:\n",
    "        raise RuntimeError(\"Failed to capture frame\")\n",
    "    return frame\n",
    "\n",
    "def first_frame(video_capture):\n",
    "    \"\"\"\n",
    "    Capture and process the initial frame.\n",
    "    :param video_capture: The video capture object.\n",
    "    :return: A tuple containing the cropped frame, cropping coordinates, and global obstacle matrix.\n",
    "    \"\"\"\n",
    "    frame = capture_frame(video_capture)\n",
    "    blue_circles = detect_color_circle(frame, lower_blue_bound, upper_blue_bound)\n",
    "    cropped_frame, cropping_coords = crop_roi_from_circles(frame, blue_circles[0:4])\n",
    "    contour_image = detect_obstacle_contours(cropped_frame, 1000, 50)\n",
    "    global_obstacle = create_obstacle_matrix(cropped_frame, contour_image[1])\n",
    "\n",
    "    plt.imshow(cv2.cvtColor(contour_image[0], cv2.COLOR_BGR2RGB))\n",
    "    plt.title('Initial Contours')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "    return cropped_frame, cropping_coords, global_obstacle\n",
    "\n",
    "def process_frame(video_capture, cropping_coords):\n",
    "    \"\"\"\n",
    "    Capture and process a single frame.\n",
    "    :param video_capture: The video capture object.\n",
    "    :param cropping_coords: The coordinates used for cropping the frame.\n",
    "    :return: A tuple containing the processed frame and robot_vector.\n",
    "    \"\"\"\n",
    "    frame = capture_frame(video_capture)\n",
    "    if cropping_coords:\n",
    "        x, y, w, h = cropping_coords\n",
    "        frame = frame[y:y+h, x:x+w]\n",
    "    # Detect red and green circles for robot position and orientation\n",
    "    robot_vector = None\n",
    "    red_circles = detect_color_circle(frame, lower_red_bound, upper_red_bound)\n",
    "    green_circles = detect_color_circle(frame, lower_green_bound, upper_green_bound)\n",
    "\n",
    "    if red_circles and green_circles and len(red_circles) >= 2:\n",
    "        # Calculate robot's position and orientation vector\n",
    "        # Calculate the midpoint between the centers of the red circles\n",
    "        midpoint = ((red_circles[0][0] + red_circles[1][0]) // 2,\n",
    "        (red_circles[0][1] + red_circles[1][1]) // 2)\n",
    "        # Calculate the directional vector\n",
    "        direction = np.array([midpoint[0] - green_circles[0][0], midpoint[1] - green_circles[0][1]])\n",
    "    \n",
    "        # Normalize and extend the vector\n",
    "        length = 100  # Additional length\n",
    "        direction = direction / np.linalg.norm(direction) * length\n",
    "    \n",
    "        # Calculate the new endpoint\n",
    "        new_endpoint = (int(green_circles[0][0] + direction[0]), int(green_circles[0][1] + direction[1]))\n",
    "    \n",
    "        # Draw the extended arrow\n",
    "        cv2.arrowedLine(frame, green_circles[0][:2], new_endpoint, (0, 0, 0), 3)\n",
    "\n",
    "        # Calculate the angle of orientation with respect to the x-axis\n",
    "        dx = green_circles[0][0] - midpoint[0]\n",
    "        dy = green_circles[0][1] - midpoint[1]\n",
    "        angle = math.atan2(dy, dx)\n",
    "        angle_degrees = math.degrees(angle)\n",
    "        if(angle_degrees>= 0):\n",
    "            angle_degrees =180 - angle_degrees\n",
    "        elif (angle_degrees < 0):\n",
    "            angle_degrees = -(180 + angle_degrees)\n",
    "        robot_vector = (midpoint[0], midpoint[1], angle_degrees) #information of the robot\n",
    "        # Optionally, display the angle\n",
    "        cv2.putText(frame, f'Angle: {angle_degrees:.2f} degrees', (10, 30), \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n",
    "     \n",
    "        cv2.putText(frame, f'Midpoint: ({midpoint[0]}, {midpoint[1]})', (10, 700),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n",
    "        cv2.drawContours(frame, contour_image[1], -1, (0, 255, 0), 2)\n",
    "        cv2.drawContours(frame, contour_image[2], -1, (0, 0, 255), 2)\n",
    "    return frame, robot_vector\n",
    "\n",
    "# Initialisation de la capture vidéo\n",
    "video_capture = cv2.VideoCapture(0, cv2.CAP_DSHOW)\n",
    "video_capture.set(cv2.CAP_PROP_FRAME_WIDTH, 1080)\n",
    "video_capture.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)\n",
    "\n",
    "# Traitement de la première frame\n",
    "cropped_frame, cropping_coords, global_obstacle = first_frame(video_capture)\n",
    "robot_vector = process_frame(video_capture, cropping_coords)\n",
    "# Traitement des frames suivantes sur demande\n",
    "# Par exemple, vous pouvez utiliser une boucle ou une commande utilisateur pour décider quand traiter une frame\n",
    "try:\n",
    "    while True:\n",
    "        command = input(\"Enter 'c' to capture and process a frame, 'q' to quit: \")\n",
    "        if command == 'c':\n",
    "            processed_frame, robot_vector = process_frame(video_capture, cropping_coords)\n",
    "            cv2.imshow(\"Processed Frame\", processed_frame)\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "        elif command == 'q':\n",
    "            break\n",
    "finally:\n",
    "    video_capture.release()\n",
    "    cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b509f8a7-a13d-4346-9c2a-30903697753c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def first_frame(frame):\n",
    "    \"\"\"\n",
    "    Process the initial frame to set up cropping coordinates and global obstacle matrix.\n",
    "    :param frame: The initial frame from the video capture.\n",
    "    :return: A tuple containing the cropped frame, cropping coordinates, and global obstacle matrix.\n",
    "    \"\"\"\n",
    "    blue_circles = detect_color_circle(frame, lower_blue_bound, upper_blue_bound)\n",
    "    cropped_frame, cropping_coords = crop_roi_from_circles(frame, blue_circles[0:4])\n",
    "    contour_image = detect_obstacle_contours(cropped_frame, 1000, 50)\n",
    "    global_obstacle = create_obstacle_matrix(cropped_frame, contour_image[1])\n",
    "    \n",
    "    plt.imshow(cv2.cvtColor(contour_image[0], cv2.COLOR_BGR2RGB))\n",
    "    plt.title('Initial Contours')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "    return cropped_frame, cropping_coords, global_obstacle\n",
    "\n",
    "def process_frame(frame, cropping_coords):\n",
    "    \"\"\"\n",
    "    Process a single frame to detect robot vector.\n",
    "    :param frame: The frame to be processed.\n",
    "    :param cropping_coords: The coordinates used for cropping the frame.\n",
    "    :return: robot_vector\n",
    "    \"\"\"\n",
    "    if cropping_coords:\n",
    "        x, y, w, h = cropping_coords\n",
    "        frame = frame[y:y+h, x:x+w]\n",
    "\n",
    "    # Detect red and green circles for robot position and orientation\n",
    "    robot_vector = None\n",
    "    red_circles = detect_color_circle(frame, lower_red_bound, upper_red_bound)\n",
    "    green_circles = detect_color_circle(frame, lower_green_bound, upper_green_bound)\n",
    "\n",
    "    if red_circles and green_circles and len(red_circles) >= 2:\n",
    "        # Calculate robot's position and orientation vector\n",
    "        # Calculate the midpoint between the centers of the red circles\n",
    "        midpoint = ((red_circles[0][0] + red_circles[1][0]) // 2,\n",
    "        (red_circles[0][1] + red_circles[1][1]) // 2)\n",
    "        # Calculate the directional vector\n",
    "        direction = np.array([midpoint[0] - green_circles[0][0], midpoint[1] - green_circles[0][1]])\n",
    "    \n",
    "        # Normalize and extend the vector\n",
    "        length = 100  # Additional length\n",
    "        direction = direction / np.linalg.norm(direction) * length\n",
    "    \n",
    "        # Calculate the new endpoint\n",
    "        new_endpoint = (int(green_circles[0][0] + direction[0]), int(green_circles[0][1] + direction[1]))\n",
    "    \n",
    "        # Draw the extended arrow\n",
    "        cv2.arrowedLine(frame, green_circles[0][:2], new_endpoint, (0, 0, 0), 3)\n",
    "\n",
    "        # Calculate the angle of orientation with respect to the x-axis\n",
    "        dx = green_circles[0][0] - midpoint[0]\n",
    "        dy = green_circles[0][1] - midpoint[1]\n",
    "        angle = math.atan2(dy, dx)\n",
    "        angle_degrees = math.degrees(angle)\n",
    "        if(angle_degrees>= 0):\n",
    "            angle_degrees =180 - angle_degrees\n",
    "        elif (angle_degrees < 0):\n",
    "            angle_degrees = -(180 + angle_degrees)\n",
    "        robot_vector = (midpoint[0], midpoint[1], angle_degrees) #information of the robot\n",
    "        # Optionally, display the angle\n",
    "        cv2.putText(frame, f'Angle: {angle_degrees:.2f} degrees', (10, 30), \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n",
    "     \n",
    "        cv2.putText(frame, f'Midpoint: ({midpoint[0]}, {midpoint[1]})', (10, 700),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n",
    "        cv2.drawContours(frame, contour_image[1], -1, (0, 255, 0), 2)\n",
    "        cv2.drawContours(frame, contour_image[2], -1, (0, 0, 255), 2)\n",
    "\n",
    "\n",
    "    return frame, robot_vector\n",
    "\n",
    "# Initialize video capture\n",
    "video_capture = cv2.VideoCapture(0, cv2.CAP_DSHOW)\n",
    "video_capture.set(cv2.CAP_PROP_FRAME_WIDTH, 1080)\n",
    "video_capture.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)\n",
    "\n",
    "ret, initial_frame = video_capture.read()\n",
    "if ret:\n",
    "    cropped_frame, cropping_coords, global_obstacle = first_frame(initial_frame)\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        ret, frame = video_capture.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Traiter la frame\n",
    "        processed_frame, robot_vector = process_frame(frame, cropping_coords)\n",
    "    \n",
    "\n",
    "        # Afficher la frame résultante\n",
    "        cv2.imshow(window_name, processed_frame)\n",
    "\n",
    "        # Pause pour maintenir la fréquence de mise à jour et vérifier la fermeture de la fenêtre\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "finally:\n",
    "    video_capture.release()\n",
    "    cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c51b5c-0df7-4528-99d5-a5e367371f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the recorded video file\n",
    "#video_path = 'output.avi'\n",
    "\n",
    "# Open the video file\n",
    "# video_capture = cv2.VideoCapture(video_path)\n",
    "\n",
    "# # Ensure that the video file opens correctly\n",
    "# if not video_capture.isOpened():\n",
    "#     print(\"Error: Unable to open video file.\")\n",
    "#     exit()\n",
    "\n",
    "video_capture = cv2.VideoCapture(0, cv2.CAP_DSHOW)\n",
    "\n",
    "video_capture.set(cv2.CAP_PROP_FRAME_WIDTH, 1080)\n",
    "video_capture.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)\n",
    "window_name = 'Robot Detection'\n",
    "cv2.namedWindow(window_name, cv2.WINDOW_NORMAL)\n",
    "\n",
    "# Resize the window (width, height)\n",
    "cv2.resizeWindow(window_name, 540, 360)  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Initial detection of obstacles and goal\n",
    "ret, initial_frame = video_capture.read()\n",
    "if ret:\n",
    "     #initial_frame, cropping_coords = crop_largest_white_area(initial_frame, 200000)\n",
    "     blue_circles = detect_color_circle(initial_frame, lower_blue_bound, upper_blue_bound)\n",
    "     # for (x, y, r) in blue_circles:\n",
    "     #         cv2.circle(initial_frame, (x, y), r, (255, 0, 0), 2)  # Dessiner en bleu\n",
    "     # #cv2.circle(, (blue_circles[0], blue_circles[1]), 2, (0, 0, 255), 3)\n",
    "     initial_frame ,cropping_coords= crop_roi_from_circles(initial_frame, blue_circles)\n",
    "     contour_image = detect_obstacle_contours(initial_frame, 1000, 50)\n",
    "     global_obstacle = create_obstacle_matrix(initial_frame,contour_image[1])\n",
    "     # center = find_blue_rectangle_center(initial_frame, lower_yellow_bound, upper_yellow_bound)\n",
    "     # if center:\n",
    "     #    # Store the coordinates of the detected yellow circle\n",
    "     #    yellow_circle_coords = center  # center est déjà un tuple (x, y)\n",
    "     #    radius = 10  # Rayon du cercle, vous pouvez ajuster cette valeur\n",
    "     #    color = (0, 255, 255)  # Couleur jaune en BGR\n",
    "     #    cv2.circle(initial_frame, yellow_circle_coords, radius, color, 3)\n",
    "    # Dessiner le cercle\n",
    "     initial_frame = np.flipud(initial_frame)\n",
    "     plt.imshow(cv2.cvtColor(contour_image[0], cv2.COLOR_BGR2RGB))\n",
    "     plt.title('Initial Contours')\n",
    "     plt.axis('off')\n",
    "     plt.show()\n",
    "\n",
    "\n",
    "# Robot update frequency (10 Hz)\n",
    "update_rate = 0.01  # 10 times per second\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Capture frame-by-frame\n",
    "        ret, frame = video_capture.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        if cropping_coords is not None:\n",
    "            x, y, w, h = cropping_coords\n",
    "            frame = frame[y:y+h, x:x+w]\n",
    "        # if cropping_coords:\n",
    "        #     x, y, w, h = cropping_coords\n",
    "        #     frame = frame[y:y+h, x:x+w]\n",
    "        # Update obstacle contours for the current frame\n",
    " \n",
    "        # Detect red and green circles\n",
    "        red_circles = detect_color_circle(frame, lower_red_bound, upper_red_bound)\n",
    "        green_circles = detect_color_circle(frame, lower_green_bound, upper_green_bound)\n",
    "\n",
    "        # if center:\n",
    "        #     cv2.circle(initial_frame, yellow_circle_coords, radius, color, 3)\n",
    "\n",
    "        if red_circles and green_circles:\n",
    "                    if len(red_circles) >= 2:\n",
    "                        # Calculate the midpoint between the centers of the red circles\n",
    "                        midpoint = ((red_circles[0][0] + red_circles[1][0]) // 2,\n",
    "                        (red_circles[0][1] + red_circles[1][1]) // 2)\n",
    "                        # Calculate the directional vector\n",
    "                        direction = np.array([midpoint[0] - green_circles[0][0], midpoint[1] - green_circles[0][1]])\n",
    "                    \n",
    "                        # Normalize and extend the vector\n",
    "                        length = 100  # Additional length\n",
    "                        direction = direction / np.linalg.norm(direction) * length\n",
    "                    \n",
    "                        # Calculate the new endpoint\n",
    "                        new_endpoint = (int(green_circles[0][0] + direction[0]), int(green_circles[0][1] + direction[1]))\n",
    "                    \n",
    "                        # Draw the extended arrow\n",
    "                        cv2.arrowedLine(frame, green_circles[0][:2], new_endpoint, (0, 0, 0), 3)\n",
    "\n",
    "                        # Calculate the angle of orientation with respect to the x-axis\n",
    "                        dx = green_circles[0][0] - midpoint[0]\n",
    "                        dy = green_circles[0][1] - midpoint[1]\n",
    "                        angle = math.atan2(dy, dx)\n",
    "                        angle_degrees = math.degrees(angle)\n",
    "                        if(angle_degrees>= 0):\n",
    "                            angle_degrees =180 - angle_degrees\n",
    "                        elif (angle_degrees < 0):\n",
    "                            angle_degrees = -(180 + angle_degrees)\n",
    "                        robot_vector = (midpoint[0], midpoint[1], angle_degrees) #information of the robot\n",
    "                        # Optionally, display the angle\n",
    "                        cv2.putText(frame, f'Angle: {angle_degrees:.2f} degrees', (10, 30), \n",
    "                                    cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n",
    "                     \n",
    "                        cv2.putText(frame, f'Midpoint: ({midpoint[0]}, {midpoint[1]})', (10, 700),\n",
    "                                    cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n",
    "        cv2.drawContours(frame, contour_image[1], -1, (0, 255, 0), 2)\n",
    "        cv2.drawContours(frame, contour_image[2], -1, (0, 0, 255), 2)\n",
    "       # cv2.circle(frame, (x, y), r, (0, 255, 255), 3)\n",
    "        # Frame dimensions\n",
    "\n",
    "        height, width = frame.shape[:2]\n",
    "\n",
    "        # Coordinates of the starting point of the reference frame (bottom right of the image)\n",
    "        origin_x, origin_y = width - 150, height - 70  # Adjustment for a length of 100\n",
    "        # Draw the X axis\n",
    "        cv2.line(frame, (origin_x, origin_y), (origin_x + 100, origin_y), (0, 0, 255), 2)\n",
    "        # Draw the Y axis\n",
    "        cv2.line(frame, (origin_x, origin_y), (origin_x, origin_y - 100), (0, 255, 0), 2)\n",
    "        # Mark the length on the X axis\n",
    "        cv2.putText(frame, \"100\", (origin_x + 100, origin_y + 15), cv2.FONT_HERSHEY_SIMPLEX, 0.4, (0, 0, 255), 2)\n",
    "        # Mark the length on the Y axis\n",
    "        cv2.putText(frame, \"100\", (origin_x - 30, origin_y - 100), cv2.FONT_HERSHEY_SIMPLEX, 0.4, (255, 0, 255), 2)\n",
    "        cv2.putText(frame, f'({origin_x}, {origin_y})', (origin_x-200, origin_y-10),\n",
    "        cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2) #about 14cm for 100pixels\n",
    "        # ... (the rest of your code to display the frame)\n",
    "\n",
    "        frame = np.flipud(frame)\n",
    "\n",
    "        # Display the resulting frame\n",
    "        cv2.imshow('Robot Detection', frame)\n",
    "\n",
    "        # Pause to maintain the update frequency\n",
    "        time_to_wait = max(int((start_time + update_rate - time.time()) * 1000), 1)\n",
    "        if cv2.waitKey(time_to_wait) & 0xFF == ord('q'):\n",
    "            break\n",
    "finally:\n",
    "    # Release the capture when everything is finished\n",
    "    video_capture.release()\n",
    "    cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378b385e-6a1e-411d-b32f-3582d5787673",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(center)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec623cd9-75a3-4ae9-b9df-8e11b8a2ac20",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(threshold=np.inf)\n",
    "\n",
    "# Afficher la matrice global_obstacle\n",
    "print(global_obstacle)\n",
    "plt.imshow(global_obstacle, cmap='gray')\n",
    "plt.title('Matrice d\\'Obstacles')\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "x, y, r = yellow_circles[0] #coordinate of the goal*********************************************************"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dffc645-cada-46ec-949e-b630f247bfa3",
   "metadata": {},
   "source": [
    "\n",
    "\"Initially, for the setup, it's advantageous to position the camera above the play area. This placement minimizes the need to account for perspective distortions. To accurately determine the robot's position and orientation, at least two distinct markers should be placed on the robot. These markers enable precise tracking and analysis of the robot's movements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54b1493-4591-41f1-8e1f-c6d43c869c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Detect red and green circles\n",
    "red_circles = detect_color_circle(image, lower_red_bound, upper_red_bound)\n",
    "green_circles = detect_color_circle(image, lower_green_bound, upper_green_bound)\n",
    "yellow_circles = detect_color_circle(image, lower_yellow_bound, upper_yellow_bound)\n",
    "\n",
    "red_circles, green_circles, yellow_circles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da82c685-ef08-4273-b907-37b1a9e68876",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Assuming red_circles and green_circles contain the detected circles for each color\n",
    "# For demonstration, let's create dummy circle data\n",
    "# red_circles = [(x1, y1, r1), (x2, y2, r2)]\n",
    "# green_circles = [(x3, y3, r3)]\n",
    "\n",
    "# TODO: Replace the dummy values with your actual circle centers and radii\n",
    "#red_circles = [(50, 50, 30), (150, 50, 30)]  # Dummy values\n",
    "#green_circles = [(100, 150, 30)]  # Dummy values\n",
    "\n",
    "# Calculate the midpoint between the centers of the red circles\n",
    "midpoint = ((red_circles[0][0] + red_circles[1][0]) // 2,\n",
    "            (red_circles[0][1] + red_circles[1][1]) // 2)\n",
    "\n",
    "# Draw a line (and arrow) from the green circle's center to the midpoint\n",
    "cv2.arrowedLine(image,  green_circles[0][:2],midpoint, (0, 255, 0), 20)\n",
    "\n",
    "# Calculate the angle of orientation with respect to the x-axis\n",
    "dx = green_circles[0][0] - midpoint[0]\n",
    "dy = green_circles[0][1] - midpoint[1]\n",
    "angle = math.atan2(dy, dx)\n",
    "angle_degrees = math.degrees(angle)\n",
    "\n",
    "# Display the image with the drawn arrow\n",
    "plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "plt.title(f'Angle of orientation: {angle_degrees:.2f} degrees')\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "# Return the midpoint and the angle\n",
    "midpoint, angle_degrees\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67438223-d9b1-418a-8930-f751226a0e25",
   "metadata": {},
   "source": [
    "For the obstacle detection: we used black shapes that we randomly distribute around the board. It is "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e368801-d787-4c38-840d-e4f21a95c84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#image_path = 'WIN_20231124_15_09_52_Pro.jpg'\n",
    "image = initial_frame #cv2.imread(image_path)\n",
    "image = frame.copy()  # Créer une copie de l'image\n",
    "# Convert to HSV color space\n",
    "hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "# Define the lower and upper bounds for the black color\n",
    "lower_black = np.array([0, 0, 0])\n",
    "upper_black = np.array([180, 150, 40])\n",
    "\n",
    "# Create a black color mask\n",
    "mask_black = cv2.inRange(hsv, lower_black, upper_black)\n",
    "\n",
    "# Find contours in the mask\n",
    "contours, _ = cv2.findContours(mask_black, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "# Set a realistic threshold for the area of the contours\n",
    "area_threshold = 2000  # Adjust this threshold according to your needs\n",
    "\n",
    "# Filter the original contours that are larger than the threshold\n",
    "filtered_contours = [cnt for cnt in contours if cv2.contourArea(cnt) > area_threshold]\n",
    "\n",
    "# Perform dilation to increase the size of the black regions\n",
    "kernel_size = 70  # Kernel size can be adjusted to control the amount of dilation\n",
    "kernel = np.ones((kernel_size, kernel_size), np.uint8)\n",
    "mask_dilated = cv2.dilate(mask_black, kernel, iterations=1)\n",
    "\n",
    "# Find contours in the dilated mask\n",
    "dilated_contours, _ = cv2.findContours(mask_dilated, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "# Filter the dilated contours that are larger than the threshold\n",
    "filtered_dilated_contours = [cnt for cnt in dilated_contours if cv2.contourArea(cnt) > area_threshold]\n",
    "\n",
    "# Draw the filtered original contours in green\n",
    "for contour in filtered_contours:\n",
    "    cv2.drawContours(image, [contour], -1, (0, 255, 0), 2)\n",
    "\n",
    "# Draw the filtered dilated contours in red\n",
    "for contour in filtered_dilated_contours:\n",
    "    cv2.drawContours(image, [contour], -1, (0, 0, 255), 2)\n",
    "\n",
    "# Display the image with the drawn contours\n",
    "plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "plt.title('Filtered Original and Dilated Contours')\n",
    "plt.axis('off')  # Hide the axis\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "864f1ea7-1588-4b28-8bbc-6a1fd6180ce2",
   "metadata": {},
   "source": [
    "# Tool functions that help with parameter tuning:\n",
    "<a id=\"Vision-subsection-2\"></a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa54b1a6-9448-4b46-a8e2-939ef1afe52b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_detected_circles(image, circles, circle_color):\n",
    "    \"\"\"\n",
    "    Draws the detected circles on the image and plots it.\n",
    "\n",
    "    :param image: The original image.\n",
    "    :param circles: A list of circles with their coordinates and radius.\n",
    "    :param circle_color: The color to use for drawing the circles.\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    if circles is not None and len(circles) > 0:\n",
    "        for circle in circles:\n",
    "            center = (circle[0], circle[1])  # Circle center\n",
    "            radius = circle[2]  # Circle radius\n",
    "            # Draw the circle's perimeter\n",
    "            cv2.circle(image, center, radius, circle_color, 2)\n",
    "            # Draw the circle's center\n",
    "            cv2.circle(image, center, 2, circle_color, 3)\n",
    "\n",
    "    # Plot the image with detected circles\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "    plt.axis('off')  # Hide the axis\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe05eb0-2409-4b08-8e58-e852199f2b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Vérification si la frame a été capturée avec succès\n",
    "if ret:\n",
    "    # Conversion de l'image en RGB pour l'affichage avec Matplotlib\n",
    "    frame_rgb = cv2.cvtColor(initial_frame, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Affichage de l'image\n",
    "    plt.imshow(initial_frame)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Erreur lors de la capture de l'image.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a142009-f980-44a4-85a6-6302ccfc62a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtenir les dimensions de l'image\n",
    "h, w = frame_rgb.shape[:2]\n",
    "\n",
    "# Calculer le nombre total de pixels\n",
    "total_pixels = h * w\n",
    "\n",
    "print(f\"The image is of dimension {w}x{h} (width x hight)\")\n",
    "print(f\"The total number of pixels is : {total_pixels}\")\n",
    "\n",
    "height, width = global_obstacle.shape\n",
    "\n",
    "print(\"Hauteur de la matrice:\", height)\n",
    "print(\"Largeur de la matrice:\", width)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bfd92df-e5c6-432d-ae01-eb5227452df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Load the image\n",
    "image1 = initial_frame #cv2.imread('WIN_20231124_15_09_52_Pro.jpg')\n",
    "image1= image1.copy()\n",
    "# Convert to HSV color space\n",
    "hsv = cv2.cvtColor(image1, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "# Define the lower and upper bounds for the red color\n",
    "# Note: Adjust these values according to your color calibration\n",
    "\n",
    "\n",
    "# Create a red color mask\n",
    "mask_red = cv2.inRange(hsv, lower_blue_bound, upper_blue_bound)\n",
    "\n",
    "# Apply the mask to the image\n",
    "red_only = cv2.bitwise_and(image1, image1, mask=mask_red)\n",
    "\n",
    "# Convert the result to grayscale\n",
    "gray = cv2.cvtColor(red_only, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "\n",
    "# Apply Canny edge detection to help with circle detection\n",
    "edges = cv2.Canny(gray, 100, 200)\n",
    "\n",
    "# Detect circles using the Hough Transform\n",
    "circles = cv2.HoughCircles(edges, cv2.HOUGH_GRADIENT, 1, 20,\n",
    "                           param1=20, param2=15, minRadius=10, maxRadius=50)\n",
    "\n",
    "# If circles are detected, draw them\n",
    "if circles is not None:\n",
    "    circles = np.uint16(np.around(circles))\n",
    "    for i in circles[0, :]:\n",
    "        cv2.circle(image1, (i[0], i[1]), i[2], (0, 255, 0), 2)\n",
    "        cv2.circle(image1, (i[0], i[1]), 2, (0, 0, 255), 3)\n",
    "\n",
    "# Plotting the different stages\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(1, 4, 1)\n",
    "plt.imshow(cv2.cvtColor(image1, cv2.COLOR_BGR2RGB))\n",
    "plt.title('Original Image')\n",
    "\n",
    "plt.subplot(1, 4, 2)\n",
    "plt.imshow(mask_red, \n",
    "           cmap='gray')\n",
    "plt.title('Red Mask')\n",
    "\n",
    "plt.subplot(1, 4, 3)\n",
    "plt.imshow(edges, cmap='gray')\n",
    "plt.title('Canny Edges')\n",
    "\n",
    "plt.subplot(1, 4, 4)\n",
    "plt.imshow(cv2.cvtColor(image1, cv2.COLOR_BGR2RGB))\n",
    "plt.title('Detected Circles')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a355820-2984-4a83-b507-c85b3b551eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from ipywidgets import interactive\n",
    "\n",
    "def interactive_mask(lower_h, lower_s, lower_v, upper_h, upper_s, upper_v):\n",
    "    \n",
    "    lower_color_bound = np.array([lower_h, lower_s, lower_v])\n",
    "    upper_color_bound = np.array([upper_h, upper_s, upper_v])\n",
    "\n",
    "    # Ensure initial_frame is defined here, or pass it as an argument to this function\n",
    "    image = initial_frame.copy()\n",
    "    \n",
    "    # Convert to HSV color space\n",
    "    hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "    \n",
    "    mask = cv2.inRange(hsv, lower_color_bound, upper_color_bound)\n",
    "    plt.imshow(mask, cmap='gray')\n",
    "    plt.show()\n",
    "\n",
    "# Define initial_frame here or load it before this point\n",
    "# initial_frame = cv2.imread('your_image.jpg')\n",
    "\n",
    "# Setup interactive widgets\n",
    "interactive(interactive_mask, \n",
    "            lower_h=(0,255), lower_s=(0,255), lower_v=(0,255),\n",
    "            upper_h=(0,255), upper_s=(0,255), upper_v=(0,255))\n",
    "# Assuming 'hsv' is your converted HSV image\n",
    "#lower_red_bound = np.array([120, 70, 120])\n",
    "#upper_red_bound = np.array([255, 255, 255])\n",
    "#lower_green_bound = np.array([60, 120, 100])\n",
    "#upper_green_bound = np.array([100, 255, 255])\n",
    "#lower_black = np.array([0, 0, 0])\n",
    "#upper_black = np.array([255, 255, 130])\n",
    "#lower_yellow_bound = np.array([0, 60, 140])\n",
    "#upper_yellow_bound = np.array([40, 105, 255])\n",
    "#lower_white_bound = np.array([0, 0, 200], dtype=np.uint8)\n",
    "#upper_white_bound = np.array([180, 55, 255], dtype=np.uint8)\n",
    "#lower_blue_bound = np.array([00, 80, 0], dtype=np.uint8)\n",
    "#upper_blue_bound = np.array([105, 255, 255], dtype=np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b98467-3505-40c8-9417-8236cdbad250",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = frame #cv2.imread('WIN_20231124_15_09_52_Pro.jpg')\n",
    "red_circles = detect_color_circle(image, lower_red_bound, upper_red_bound)\n",
    "green_circles = detect_color_circle(image, lower_green_bound, upper_green_bound)\n",
    "yellow_circles = detect_color_circle(image, lower_yellow_bound, upper_yellow_bound)\n",
    "\n",
    "red_circles, green_circles, yellow_circles\n",
    "\n",
    "# Assuming red_circles and green_circles contain the detected circles for each color\n",
    "image_copy = image.copy()  # Make a copy to draw on\n",
    "plot_detected_circles(image_copy, red_circles, (200, 0, 255))  # Red color for red circles\n",
    "plot_detected_circles(image_copy, green_circles, (0, 255, 0))  # Green color for green circles\n",
    "plot_detected_circles(image_copy, yellow_circles,(255,255,153))  # yellow color for yellow circles\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e89d89a2-0751-4907-9a38-cee54fc239fd",
   "metadata": {},
   "source": [
    "## 3 Global Navigation\n",
    "<a id=\"global-navigation\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b4a11331",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'global_obstacle' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_13896/3473474969.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mglobal_obstacle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlogical_not\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mglobal_obstacle\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mmax_val_x_init\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mglobal_obstacle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mmax_val_y_init\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mglobal_obstacle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mreduction_coeff\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m20\u001b[0m \u001b[1;31m# tune for speed\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'global_obstacle' is not defined"
     ]
    }
   ],
   "source": [
    "global_obstacle = np.logical_not( global_obstacle )\n",
    "max_val_x_init = global_obstacle.shape[0]\n",
    "max_val_y_init = global_obstacle.shape[1]\n",
    "\n",
    "reduction_coeff = 20 # tune for speed\n",
    "max_val_x = int(max_val_x_init / reduction_coeff)\n",
    "max_val_y = int(max_val_y_init / reduction_coeff)\n",
    "\n",
    "occupancy_grid = np.zeros((max_val_x, max_val_y), dtype=int)\n",
    "for i in range (max_val_x):\n",
    "    for j in range (max_val_y):\n",
    "        sum_pixels = 0\n",
    "        for k in range (reduction_coeff): # dans le doute on augmente la distance de sécurité avec obstacle\n",
    "            sum_pixels = sum_pixels + global_obstacle[int(i * reduction_coeff - reduction_coeff/2 + k)][int(j * reduction_coeff - reduction_coeff/2 + k)]\n",
    "        if sum_pixels == 0:\n",
    "            occupancy_grid[i][j] = 0\n",
    "        else:\n",
    "            occupancy_grid[i][j] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca9bb6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_empty_plot(max_val):\n",
    "    \"\"\"\n",
    "    Helper function to create a figure of the desired dimensions & grid\n",
    "    \n",
    "    :param max_val: dimension of the map along the x and y dimensions\n",
    "    :return: the fig and ax objects.\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(7,7))\n",
    "    \n",
    "    major_ticks = np.arange(0, max_val+1, 5)\n",
    "    minor_ticks = np.arange(0, max_val+1, 1)\n",
    "    ax.set_xticks(major_ticks)\n",
    "    ax.set_xticks(minor_ticks, minor=True)\n",
    "    ax.set_yticks(major_ticks)\n",
    "    ax.set_yticks(minor_ticks, minor=True)\n",
    "    ax.grid(which='minor', alpha=0.2)\n",
    "    ax.grid(which='major', alpha=0.5)\n",
    "    #ax.set_ylim([-1,max_val_x])\n",
    "    ax.set_ylim([max_val_x,-1])\n",
    "    ax.set_xlim([-1,max_val_y])\n",
    "    ax.grid(True)\n",
    "    \n",
    "    return fig, ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf223c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing the grid\n",
    "\n",
    "fig, ax = create_empty_plot(max_val_y)\n",
    "\n",
    "cmap = colors.ListedColormap(['white', 'red']) # Select the colors with which to display obstacles and free cells\n",
    "\n",
    "#occupancy_grid = np.logical_not(occupancy_grid)\n",
    "\n",
    "# Displaying the map\n",
    "ax.imshow(occupancy_grid, cmap=cmap)\n",
    "plt.title(\"Map : free cells in white, occupied cells in red\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8716398d",
   "metadata": {},
   "source": [
    "## A* implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c9959a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_movements_8n():\n",
    "    \"\"\"\n",
    "    Get all possible 8-connectivity movements. Equivalent to get_movements_in_radius(1)\n",
    "    (up, down, left, right and the 4 diagonals).\n",
    "    :return: list of movements with cost [(dx, dy, movement_cost)]\n",
    "    \"\"\"\n",
    "    s2 = math.sqrt(2)\n",
    "    return [(1, 0, 1.0),\n",
    "            (0, 1, 1.0),\n",
    "            (-1, 0, 1.0),\n",
    "            (0, -1, 1.0),\n",
    "            (1, 1, s2),\n",
    "            (-1, 1, s2),\n",
    "            (-1, -1, s2),\n",
    "            (1, -1, s2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3865680",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def reconstruct_path(cameFrom, current):\n",
    "    \"\"\"\n",
    "    Recurrently reconstructs the path from start node to the current node\n",
    "    :param cameFrom: map (dictionary) containing for each node n the node immediately \n",
    "                     preceding it on the cheapest path from start to n \n",
    "                     currently known.\n",
    "    :param current: current node (x, y)\n",
    "    :return: list of nodes from start to current node\n",
    "    \"\"\"\n",
    "    total_path = [current]\n",
    "    while current in cameFrom.keys():\n",
    "        # Add where the current node came from to the start of the list\n",
    "        total_path.insert(0, cameFrom[current]) \n",
    "        current=cameFrom[current]\n",
    "    return total_path\n",
    "\n",
    "def A_Star(start, goal, h, coords, occupancy_grid, movement_type=\"4N\", max_val_x=max_val_x, max_val_y=max_val_y):\n",
    "    \"\"\"\n",
    "    A* for 2D occupancy grid. Finds a path from start to goal.\n",
    "    h is the heuristic function. h(n) estimates the cost to reach goal from node n.\n",
    "    :param start: start node (x, y)\n",
    "    :param goal_m: goal node (x, y)\n",
    "    :param occupancy_grid: the grid map\n",
    "    :param movement: select between 4-connectivity ('4N') and 8-connectivity ('8N', default)\n",
    "    :return: a tuple that contains: (the resulting path in meters, the resulting path in data array indices)\n",
    "    \"\"\"\n",
    "    \n",
    "    # -----------------------------------------\n",
    "    # DO NOT EDIT THIS PORTION OF CODE\n",
    "    # -----------------------------------------\n",
    "    \n",
    "    # Check if the start and goal are within the boundaries of the map\n",
    "    for point in [start, goal]:\n",
    "        for coord in point:\n",
    "            assert coord>=0 and coord<max_val_y, \"start or end goal not contained in the map\"\n",
    "    \n",
    "    # check if start and goal nodes correspond to free spaces\n",
    "    if occupancy_grid[start[0], start[1]]:\n",
    "        raise Exception('Start node is not traversable')\n",
    "\n",
    "    if occupancy_grid[goal[0], goal[1]]:\n",
    "        raise Exception('Goal node is not traversable')\n",
    "    \n",
    "    # get the possible movements corresponding to the selected connectivity\n",
    "    if movement_type == '4N':\n",
    "        movements = _get_movements_4n()\n",
    "    elif movement_type == '8N':\n",
    "        movements = _get_movements_8n()\n",
    "    else:\n",
    "        raise ValueError('Unknown movement')\n",
    "    \n",
    "    # --------------------------------------------------------------------------------------------\n",
    "    # A* Algorithm implementation - feel free to change the structure / use another pseudo-code\n",
    "    # --------------------------------------------------------------------------------------------\n",
    "    \n",
    "    # The set of visited nodes that need to be (re-)expanded, i.e. for which the neighbors need to be explored\n",
    "    # Initially, only the start node is known.\n",
    "    openSet = [start]\n",
    "    \n",
    "    # The set of visited nodes that no longer need to be expanded.\n",
    "    closedSet = []\n",
    "\n",
    "    # For node n, cameFrom[n] is the node immediately preceding it on the cheapest path from start to n currently known.\n",
    "    cameFrom = dict()\n",
    "\n",
    "    # For node n, gScore[n] is the cost of the cheapest path from start to n currently known.\n",
    "    gScore = dict(zip(coords, [np.inf for x in range(len(coords))]))\n",
    "    gScore[start] = 0\n",
    "\n",
    "    # For node n, fScore[n] := gScore[n] + h(n). map with default value of Infinity\n",
    "    fScore = dict(zip(coords, [np.inf for x in range(len(coords))]))\n",
    "    fScore[start] = h[start]\n",
    "    \n",
    "    current = ([0,0])\n",
    "\n",
    "    # while there are still elements to investigate\n",
    "    while openSet != []:\n",
    "        \n",
    "        previous = current\n",
    "        \n",
    "        \n",
    "        #the node in openSet having the lowest fScore[] value\n",
    "        fScore_openSet = {key:val for (key,val) in fScore.items() if key in openSet}\n",
    "        current = min(fScore_openSet, key=fScore_openSet.get)\n",
    "        del fScore_openSet\n",
    "        \n",
    "        #If the goal is reached, reconstruct and return the obtained path\n",
    "        if current == goal:\n",
    "            return reconstruct_path(cameFrom, current), closedSet\n",
    "\n",
    "        openSet.remove(current)\n",
    "        closedSet.append(current)\n",
    "        \n",
    "        #for each neighbor of current:\n",
    "        for dx, dy, deltacost in movements:\n",
    "            \n",
    "            neighbor = (current[0]+dx, current[1]+dy)\n",
    "            \n",
    "            # if the node is not in the map, skip\n",
    "            if (neighbor[0] >= occupancy_grid.shape[0]) or (neighbor[1] >= occupancy_grid.shape[1]) or (neighbor[0] < 0) or (neighbor[1] < 0):\n",
    "                continue\n",
    "            \n",
    "            # if the node is occupied or has already been visited, skip\n",
    "            if (occupancy_grid[neighbor[0], neighbor[1]]) or (neighbor in closedSet): \n",
    "                continue\n",
    "                \n",
    "            ROTATION_COST = 1\n",
    "            \n",
    "            if(not (current == start)):\n",
    "                #print(cameFrom)\n",
    "                vector_prev = ([current[0] - (cameFrom[current])[0], current[1] - (cameFrom[current])[1]]) \n",
    "                vector_next = ([neighbor[0] - current[0], neighbor[1] - current[1]]) \n",
    "                angle = np.arccos(np.dot(vector_prev, vector_next) / (np.linalg.norm(vector_prev) * np.linalg.norm(vector_next)))\n",
    "                rotation_cost = angle * ROTATION_COST\n",
    "            else:\n",
    "                rotation_cost = 0\n",
    "                \n",
    "            # d(current,neighbor) is the weight of the edge from current to neighbor\n",
    "            # tentative_gScore is the distance from start to the neighbor through current\n",
    "            tentative_gScore = gScore[current] + deltacost + rotation_cost\n",
    "            \n",
    "            if neighbor not in openSet:\n",
    "                openSet.append(neighbor)\n",
    "                \n",
    "            if tentative_gScore < gScore[neighbor]:\n",
    "                # This path to neighbor is better than any previous one. Record it!\n",
    "                cameFrom[neighbor] = current\n",
    "                gScore[neighbor] = tentative_gScore\n",
    "                fScore[neighbor] = gScore[neighbor] + h[neighbor]\n",
    "\n",
    "    # Open set is empty but goal was never reached\n",
    "    print(\"No path found to goal\")\n",
    "    return [], closedSet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555b8fb5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-08T22:44:38.966544Z",
     "start_time": "2020-05-08T22:44:37.185492Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define the start and end goal\n",
    "start = (int(robot_vector[0]/reduction_coeff),int(robot_vector[1]/reduction_coeff))\n",
    "#goal = (int(630 /reduction_coeff),int(900 /reduction_coeff))\n",
    "goal = (25,39)\n",
    "\n",
    "\n",
    "# -----------------------------------------\n",
    "# DO NOT EDIT THIS PORTION OF CODE - \n",
    "# EXECUTION AND PLOTTING OF THE ALGORITHM\n",
    "# -----------------------------------------\n",
    "    \n",
    "    \n",
    "# List of all coordinates in the grid\n",
    "w,z = np.mgrid[0:max_val_x:1, 0:max_val_y:1]\n",
    "pos = np.empty(w.shape + (2,))\n",
    "pos[:, :, 0] = w; pos[:, :, 1] = z\n",
    "pos = np.reshape(pos, (w.shape[0]*w.shape[1], 2))\n",
    "coords = list([(int(w[0]), int(w[1])) for w in pos])\n",
    "\n",
    "# Define the heuristic, here = distance to goal ignoring obstacles\n",
    "h = np.linalg.norm(pos - goal, axis=-1)\n",
    "h = dict(zip(coords, h))\n",
    "\n",
    "# Run the A* algorithm\n",
    "path, visitedNodes = A_Star(start, goal, h, coords, occupancy_grid, movement_type=\"8N\")\n",
    "path = np.array(path).reshape(-1, 2).transpose()\n",
    "visitedNodes = np.array(visitedNodes).reshape(-1, 2).transpose()\n",
    "\n",
    "path_final = path * reduction_coeff\n",
    "\n",
    "# Displaying the map\n",
    "fig_astar, ax_astar = create_empty_plot(max_val_y)\n",
    "ax_astar.imshow(occupancy_grid, cmap=cmap)\n",
    "print(path_final)\n",
    "# Plot the best path found and the list of visited nodes\n",
    "ax_astar.scatter(visitedNodes[1], visitedNodes[0], marker=\"o\", color = 'orange');\n",
    "ax_astar.plot(path[1], path[0], marker=\"o\", color = 'blue');\n",
    "ax_astar.scatter(start[1], start[0], marker=\"o\", color = 'green', s=200);\n",
    "ax_astar.scatter(goal[1], goal[0], marker=\"o\", color = 'purple', s=200);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "734d9acc-b079-45ba-be37-212bdc8f451c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6476dab3-1207-4529-b878-436ab4b969bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import\n",
    "import time\n",
    "import asyncio\n",
    "from tdmclient import aw, ClientAsync\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "# conversion thymio speed to mm/s\n",
    "Thymio_to_mms = 0.349\n",
    "px_to_mm = 140/100\n",
    "#Thymio_to_pxs = Thymio_to_mms * mm_to_px \n",
    "\n",
    "# Thymio connection\n",
    "async def connect_Thymio():\n",
    "    \"\"\"\n",
    "    Establish a connection with the Thymio if possible\n",
    "    \"\"\"\n",
    "    global node, client\n",
    "    try:\n",
    "        client = ClientAsync()\n",
    "        node = await asyncio.wait_for(client.wait_for_node(), timeout=2.0)\n",
    "        await node.lock()\n",
    "        print(\"Thymio connected\")\n",
    "\n",
    "    except asyncio.TimeoutError:\n",
    "        print(\"Thymio not connected: Timeout while waiting for node.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Thymio not connected: {str(e)}\")\n",
    "        \n",
    "# Thymio disconnection\n",
    "def disconnect_Thymio():\n",
    "    \"\"\"\n",
    "    Enable to disconnect the Thymio\n",
    "    \"\"\"\n",
    "    aw(node.stop())\n",
    "    aw(node.unlock())\n",
    "    print(\"Thymio disconnected\")\n",
    "        \n",
    "# Thymio control motor speeds  \n",
    "async def set_speeds(left_speed, right_speed):\n",
    "    \"\"\"\n",
    "    Enable to set the speed of the Thymio's wheels\n",
    "    \"\"\"\n",
    "    global node\n",
    "    v = {\n",
    "        \"motor.left.target\":  [left_speed],\n",
    "        \"motor.right.target\": [right_speed],\n",
    "    }\n",
    "    await node.set_variables(v)\n",
    "    \n",
    "async def motors_stop():\n",
    "    \"\"\"\n",
    "    Stop the Thymio\n",
    "    \"\"\"\n",
    "    global node\n",
    "    v = {\n",
    "        \"motor.left.target\":  [0],\n",
    "        \"motor.right.target\": [0],\n",
    "    }\n",
    "    await node.set_variables(v)    \n",
    "    \n",
    "# Check\n",
    "#await connect_Thymio()\n",
    "#await set_speeds(40, -40)\n",
    "#time.sleep(2)\n",
    "#await motors_stop()\n",
    "#disconnect_Thymio()\n",
    "\n",
    "# Turn a specified angle \n",
    "\n",
    "# Constants\n",
    "ROTATION_SPEED = 100\n",
    "TIME_FULL_TURN = (8800/1000)\n",
    "\n",
    "async def turn(angle):\n",
    "    # Calculate the time needed to turn through the required angle\n",
    "    rotation_time = (abs(angle) / (2*np.pi)) * TIME_FULL_TURN\n",
    "\n",
    "    # Turn robot on itself\n",
    "    # Check the sign of angle\n",
    "    if np.sign(angle) > 0:\n",
    "        # If angle is positive, turn in one direction\n",
    "        await set_speeds(ROTATION_SPEED, -ROTATION_SPEED)\n",
    "    else:\n",
    "        # If angle is negative, turn in the other direction\n",
    "        await set_speeds(-ROTATION_SPEED, ROTATION_SPEED)\n",
    "\n",
    "    # Wait required time\n",
    "    time.sleep(rotation_time)\n",
    "\n",
    "    # Stop the robot\n",
    "    #await motors_stop()\n",
    "\n",
    "# Check\n",
    "gamma = np.pi\n",
    "await connect_Thymio()\n",
    "await turn(-gamma)\n",
    "disconnect_Thymio()\n",
    "\n",
    "# Constants\n",
    "FORWARD_SPEED = 200  # Base speed\n",
    "TIME_PER_MM = 15.5/1000  # Time it takes for the robot to travel one meter at base speed\n",
    "\n",
    "async def move_forward(distance_px):\n",
    "    # Calculate the time needed to travel the requested distance\n",
    "    \n",
    "    distance_mm = distance_px * px_to_mm\n",
    "    travel_time = (distance_mm) * TIME_PER_MM\n",
    "    \n",
    "    # Robot moves forward\n",
    "    await set_speeds(FORWARD_SPEED, FORWARD_SPEED)\n",
    "\n",
    "    # Wait for the necessary time\n",
    "    time.sleep(travel_time)\n",
    "    #print(\"End of forward after : \", travel_time)\n",
    "\n",
    "    # Stop the robot\n",
    "    #await motors_stop()\n",
    "\n",
    "# Using the function\n",
    "\n",
    "#await connect_Thymio()\n",
    "# distance = 1  # Distance to travel in meters\n",
    "#await move_forward(distance)\n",
    "#disconnect_Thymio()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25af5fc7-58f6-4295-9130-3b3eada1816e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "\n",
    "import math\n",
    "import time\n",
    "import asyncio\n",
    "import numpy as np\n",
    "\n",
    "robot_vector = robot_vector\n",
    "start = path_final[0]\n",
    "goal = path_final[-1]\n",
    "\n",
    "gamma = 0 # angle entre orientation actuelle et orientation pour le point suivant\n",
    "current_node = 1 # indice de l'étape suivante\n",
    "dist_to_node = 0 # distance en pixels séparant le robot de l'étape suivante  \n",
    "norm_vector_node = np.array([0,0]) # vecteur normalisé reliant la position à la prochaine étape\n",
    "\n",
    "ANGLE_THRESHOLD = 0.1 # valeur à partir de laquelle le robot arrête de tourner et commence à avancer vers la prochaine étape\n",
    "FORWARD_THRESHOLD = 1 # valeur à partir de laquelle le robot change d'étape\n",
    "ROTATION_TIME_THRESHOLD = 1.2\n",
    "FORWARD_TIME_THRESHOLD = 100\n",
    "current_angle = np.radians(robot_vector[2])\n",
    "\n",
    "# translation\n",
    "#forward_duration = 1\n",
    "#run_speed=118\n",
    "#run_offset=0\n",
    "\n",
    "# rotation\n",
    "#rot_duration = 1.2\n",
    "#rot_speed = 125\n",
    "\n",
    "await connect_Thymio()\n",
    "\n",
    "while True:\n",
    "    print(current_node)\n",
    "    # Normal vector\n",
    "    norm_vector_node[0] = path_final[0][current_node] - path_final[0][current_node-1]\n",
    "    norm_vector_node[1] = path_final[1][current_node] - path_final[1][current_node-1] \n",
    "    norm_vector_node = norm_vector_node / np.linalg.norm(norm_vector_node)\n",
    "    \n",
    "    # angle gamma\n",
    "    gamma = math.atan2(norm_vector_node[0], norm_vector_node[1]) - current_angle #angle en rad\n",
    "    \n",
    "    # distance d\n",
    "    path_current_node = np.array([path_final[0][current_node], path_final[1][current_node]])\n",
    "    path_previous_node = np.array([path_final[0][current_node-1], path_final[1][current_node-1]])\n",
    "    d = np.linalg.norm(path_current_node - path_previous_node) # distance en px/20\n",
    "    print(\"gamma : \", gamma)\n",
    "    print(\"d : \", d)\n",
    "    \n",
    "    if(abs(gamma) > ANGLE_THRESHOLD):\n",
    "        await turn(gamma)\n",
    "    \n",
    "    current_angle += gamma\n",
    "    \n",
    "    if( d >= FORWARD_THRESHOLD):\n",
    "        await move_forward(d)\n",
    "    \n",
    "    current_node = current_node + 1\n",
    "    \n",
    "\n",
    "    \n",
    "    if current_node == path_final.shape[1]:\n",
    "        break\n",
    "await motors_stop()\n",
    "disconnect_Thymio()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4526dbe-a8ee-4b20-a861-78e317751afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "disconnect_Thymio()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d4a839",
   "metadata": {},
   "source": [
    "- blanc : libre\n",
    "- rouge : obstacle\n",
    "- orange : case explorée\n",
    "- bleu : chemin le plus court\n",
    "- vert : départ\n",
    "- violet : arrivée"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "552d0770",
   "metadata": {},
   "source": [
    "Maintenant que nous avons calculé le chemin le plus court, nous appelons des fonctions pour diriger le robot dans la bonne direction. Les données qui sortent de ce fichier sont dans la matrice path_final qui regroupe les coordonées des étapes recalculées à la bonne échelle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d53c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0 # angle entre orientation actuelle et orientation pour le point suivant\n",
    "current_node = 1 # indice de l'étape suivante\n",
    "dist_to_node = 0 # distance en pixels séparant le robot de l'étape suivante  \n",
    "norm_vector_node = np.array([0,0]) # vecteur normalisé reliant la position à la prochaine étape\n",
    "\n",
    "if(robot_vector[2] >= 0):\n",
    "    orientation = math.radians(180 - robot_vector[2])\n",
    "elif (robot_vector[2] < 0):\n",
    "    orientation = math.radians( -(180 + robot_vector[2]))\n",
    "\n",
    "\n",
    "ANGLE_THRESHOLD = 0.1 # valeur à partir de laquelle le robot arrête de tourner et commence à avancer vers la prochaine étape\n",
    "DISTANCE_TO_NODE_THRESHOLD = 5 # valeur à partir de laquelle le robot change d'étape\n",
    "ROTATION_TIME_THRESHOLD = 400\n",
    "FORWARD_TIME_THRESHOLD = 100\n",
    "\n",
    "while True:\n",
    "    norm_vector_node[0] = path[0][current_node] - position[0]\n",
    "    norm_vector_node[1] = path[1][current_node] - position[1] # calcul du vecteur normalisé\n",
    "    norm_vector_node = norm_vector_node / np.linalg.norm(norm_vector_node)\n",
    "    alpha = np.arctan(norm_vector_node) # calcul de l'angle (A REFAIRE EN FONCTION DE COMMENT ON CALCULE LES ORIENTATIONS)\n",
    "\n",
    "    gamma = np.arccos(np.dot(position,norm_vector_node))  # Calcul de gamma\n",
    "    print(path[:][current_node])\n",
    "    dist_to_node = np.linalg.norm(path[:, current_node] - position) # On calcule la distance \n",
    "\n",
    "\n",
    "    if (abs(gamma) > ANGLE_THRESHOLD): # Tourner le robot jusqu'à être en dessous d'une certaine valeur de threshold\n",
    "\n",
    "        start_time = time.time()\n",
    "        #\n",
    "        #\n",
    "        # fonction qui fait tourner le robot pour être dans la bonne direction\n",
    "        # turn on the spot\n",
    "        while ((time.time() - start_time) < abs(gamma)*ROTATION_TIME_THRESHOLD):\n",
    "            motor_left_target= -100 * math.copysign(1, gamma)\n",
    "            motor_right_target= 100 * math.copysign(1, gamma)\n",
    "        #\n",
    "        #\n",
    "        end_time = time.time()\n",
    "        elapsed_time_rot = end_time - start_time\n",
    "        print(elapsed_time_rot)\n",
    "\n",
    "    if (dist_to_node > DISTANCE_TO_NODE_THRESHOLD): # Avancer le robot jusqu'à  atteindre la prochaine étape\n",
    "\n",
    "        start_time = time.time()\n",
    "        #\n",
    "        #\n",
    "        # fonction pour faire avancer le robot à 60\n",
    "        while ((time.time() - start_time) < abs(gamma)*ROTATION_TIME_THRESHOLD):\n",
    "            motor_left_target = 200\n",
    "            motor_left_target = 200\n",
    "        #\n",
    "        #\n",
    "        end_time = time.time()\n",
    "        elapsed_time_forw = end_time - start_time # return la distance parcourue en multipliant la vitesse par le temps\n",
    "        \n",
    "\n",
    "    current_node = current_node + 1\n",
    "\n",
    "\n",
    "    if current == path.shape[1]:\n",
    "        break\n",
    "print(\"On est arrivé\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ff5e1f-15bb-476c-a740-0f5fc32e9e8a",
   "metadata": {},
   "source": [
    "# Kalman Filterwhile 1:\"\n",
    " on3\",\n",
    "   \"version\": \"3.11.5\"\n",
    "  }\n",
    " },\n",
    " \"nbformat\": 4,\n",
    " \"nbformat_minor\": 5\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a33aa833-15b3-4877-86cb-171284040b07",
   "metadata": {},
   "source": [
    "The localization of the Thymio robot is performed using a Kalman filter. This filtering method is well suited to estimating the position and orientation of a mobile robot from noisy or incomplete measurements. The design of the filter in this project is based on using the position ($x, y$) and orientation ($\\theta$) provided by the camera as measurements. In addition, the speed of the robot, provided by the wheel speed sensors ($v_r, v_l$), is used as a prediction. In short, the Kalman filter merges a prediction of the system's future state with a measurement of that state to estimate position probabilistically."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fbb3fad-4699-42d6-98ba-b3e8a164e731",
   "metadata": {},
   "source": [
    "## State-space model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fdeaa31-8b8b-4b90-a272-c872a56a40d4",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "161c8da7-511a-4963-9766-d39749ef9757",
   "metadata": {},
   "source": [
    "To estimate the robot's future position, a state-space model needs to be developed: \n",
    "\n",
    "$$\\hat{s}_{a\\_priori}^{t+1} = A \\cdot \\hat{s}_{a\\_posteriori}^{t} + B \\cdot u^{t} + q^t$$\n",
    "\n",
    "The prediction of the future state is referred to as $\\hat{s}_{a\\_priori}^{t+1}$, i.e. the a priori estimate at time t+1. Since the state of the system is defined by its position ($x, y$) and orientation ($\\theta$), this gives: \n",
    "\n",
    "$$\\hat{s}_{a\\_priori}^{t+1} = \\begin{pmatrix}\n",
    "\\hat{x}_{a\\_priori}^{t+1} \\\\\\\\\n",
    "\\hat{y}_{a\\_priori}^{t+1} \\\\\\\\\n",
    "\\hat{\\theta}_{a\\_priori}^{t+1}\n",
    "\\end{pmatrix}$$\n",
    "\n",
    "The current state corresponds to the term $\\hat{s}_{a\\_posteriori}^{t}$, which is the a posteriori estimate at time t. In the same way as above, this gives:\n",
    "\n",
    "$$\\hat{s}_{a\\_posteriori}^{t} = \\begin{pmatrix}\n",
    "\\hat{x}_{a\\_posteriori}^{t} \\\\\\\\\n",
    "\\hat{y}_{a\\_posteriori}^{t} \\\\\\\\\n",
    "\\hat{\\theta}_{a\\_posteriori}^{t}\n",
    "\\end{pmatrix}$$\n",
    "\n",
    "The system input at time t is represented by the vector $u^{t}$. This is made up of two terms: translational speed ($v$) and rotational speed ($\\omega$). \n",
    "\n",
    "$$u^t = \\begin{pmatrix}\n",
    "v \\\\\\\\\n",
    "\\omega\n",
    "\\end{pmatrix} $$\n",
    "\n",
    "These are defined on the basis of the speeds measured by the wheel speed sensors, i.e. the right ($v_r$) and left ($v_l$) speeds, and the spacing between the two wheels ($e$).\n",
    "\n",
    "$$ v = \\cfrac{v_r + v_l}{2} \\qquad\\qquad \\omega = \\cfrac{v_r-v_l}{e} $$ \n",
    "\n",
    "Matrix A characterizes the evolution of the system state, while matrix B describes the impact of the input on the future state. An odometry-based approach allows us to determine these two matrices by considering a very short time interval ($\\delta t$). During this time interval, the robot rotates by $\\delta \\theta = \\omega \\cdot \\delta t$. Knowing this, and referring to the diagram below, the following system of equations can be established: \n",
    "\n",
    "$$\\begin{equation}\n",
    "\\begin{cases}\n",
    "\\hat{x}_{a\\_priori}^{t+1} = \\hat{x}_{a\\_posteriori}^{t} + v \\cdot \\cos\\left(\\hat{\\theta}_{a\\_posteriori}^{t} + \\delta \\theta^t \\right) \\cdot \\delta t \\\\\n",
    "\\hat{y}_{a\\_priori}^{t+1} = \\hat{y}_{a\\_posteriori}^{t} + v \\cdot \\sin\\left(\\hat{\\theta}_{a\\_posteriori}^{t} + \\delta \\theta^t \\right) \\cdot \\delta t \\\\\n",
    "\\hat{\\theta}_{a\\_priori}^{t+1} = \\hat{\\theta}_{a\\_posteriori}^{t} + \\omega \\cdot \\delta t\n",
    "\\end{cases}\n",
    "\\end{equation}$$\n",
    "\n",
    "![state-space_model](Images/schematics.png)\n",
    "\n",
    "The matrix form of this system therefore becomes:\n",
    "\n",
    "$$\\begin{equation}\n",
    "A = \\begin{bmatrix} \n",
    "1 & 0 & 0\\\\ \n",
    "0 & 1 & 0 \\\\ \n",
    "0 & 0 & 1\n",
    "\\end{bmatrix}\n",
    "\\qquad\\qquad\n",
    "B = \\begin{bmatrix} \n",
    "\\cos\\left(\\hat{\\theta}_{a\\_posteriori}^{t} + \\delta \\theta^t \\right) \\cdot \\delta t & 0\\\\\n",
    "\\sin\\left(\\hat{\\theta}_{a\\_posteriori}^{t} + \\delta \\theta^t \\right) \\cdot \\delta t & 0 \\\\\n",
    "0 & \\delta t \n",
    "\\end{bmatrix}\n",
    "\\end{equation}$$\n",
    "\n",
    "The final term $q^t$ of this state-space model represents the stochastic perturbation of the state with covariance matrix Q defined as follows:\n",
    "\n",
    "$$\n",
    "Q = \\begin{bmatrix} \n",
    "q_1 & 0 & 0\\\\ \n",
    "0 & q_2 & 0 \\\\ \n",
    "0 & 0 & q_3\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "These diagonal coefficients can be evaluated using an approach similar to that used in Exercise 8 of the MICRO-452 course.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b284d0-0bd3-4e9c-a4c6-7545a786c8d7",
   "metadata": {},
   "source": [
    "### Measurement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa9dbef-5fff-4319-b6ae-b8130a602071",
   "metadata": {},
   "source": [
    "Having explored the prediction phase of the state-space model, attention now turns to the second essential part: updating the measurements. This stage aims to refine the predictions by integrating real information captured by the camera. The formula governing this step is :\n",
    "\n",
    "$$ m^{t+1} = C \\cdot s^{t+1} + r^{t+1}$$ \n",
    "\n",
    "Measurements taken at time t+1 are represented here by the term $m_{t+1}$. The data collected by the camera are therefore:\n",
    "\n",
    "$$m^{t+1} = \\begin{pmatrix}\n",
    "x_{captured}^{t+1} \\\\\\\\\n",
    "y_{captured}^{t+1} \\\\\\\\\n",
    "\\theta_{captured}^{t+1}\n",
    "\\end{pmatrix}$$\n",
    "\n",
    "The robot's position ($x, y$) and orientation ($\\theta$) measured by the camera are used directly as system outputs, without any transformation. The matrix C linking the measurements to the state is therefore defined as follows:\n",
    "\n",
    "$$C = \\begin{bmatrix} \n",
    "1 & 0 & 0\\\\ \n",
    "0 & 1 & 0 \\\\ \n",
    "0 & 0 & 1\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "The term $s^{t+1}$ simply represents the state of the system at time t+1:\n",
    "\n",
    "$$s^{t+1} = \\begin{pmatrix}\n",
    "x^{t+1} \\\\\\\\\n",
    "y^{t+1} \\\\\\\\\n",
    "\\theta^{t+1}\n",
    "\\end{pmatrix}$$\n",
    "\n",
    "Finally, the last term $r^{t+1}$ of this equation represents noise on measurements with a covariance matrix R defined as follows:\n",
    "\n",
    "$$\n",
    "R = \\begin{bmatrix} \n",
    "r_1 & 0 & 0\\\\ \n",
    "0 & r_2 & 0 \\\\ \n",
    "0 & 0 & r_3\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Note: When the camera's view is obstructed, estimation is only possible using the prediction model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ae69e3-47a8-4904-97e7-50ad37fe2fbf",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "186b10f8-a2a2-4687-9734-f2bf1590144e",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f20ee2a-0bf1-4375-94ad-6c2e58ec7e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "import scipy\n",
    "import asyncio\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from random import randrange\n",
    "from tdmclient import aw, ClientAsync"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e969fc8-5c7f-4364-a29a-20ba905b0565",
   "metadata": {},
   "source": [
    "### Basic functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b81742b0-fff5-4cf0-a3e7-419dfbc3483c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thymio connection\n",
    "async def connect_Thymio():\n",
    "    \"\"\"\n",
    "    Establish a connection with the Thymio if possible\n",
    "    \"\"\"\n",
    "    global node, client\n",
    "    try:\n",
    "        client = ClientAsync()\n",
    "        node = await asyncio.wait_for(client.wait_for_node(), timeout=2.0)\n",
    "        await node.lock()\n",
    "        print(\"Thymio connected\")\n",
    "\n",
    "    except asyncio.TimeoutError:\n",
    "        print(\"Thymio not connected: Timeout while waiting for node.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Thymio not connected: {str(e)}\")\n",
    "        \n",
    "# Thymio disconnection\n",
    "def disconnect_Thymio():\n",
    "    \"\"\"\n",
    "    Enable to disconnect the Thymio\n",
    "    \"\"\"\n",
    "    aw(node.stop())\n",
    "    aw(node.unlock())\n",
    "    print(\"Thymio disconnected\")\n",
    "\n",
    "# Thymio control motor speeds  \n",
    "def set_speeds(left_speed, right_speed):\n",
    "    \"\"\"\n",
    "    Enable to set the speed of the Thymio's wheels\n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"motor.left.target\": [left_speed],\n",
    "        \"motor.right.target\": [right_speed],\n",
    "    }\n",
    "    \n",
    "# Check\n",
    "await connect_Thymio()\n",
    "node.send_set_variables(set_speeds(40, 40))\n",
    "time.sleep(2)\n",
    "node.send_set_variables(set_speeds(0, 0))\n",
    "disconnect_Thymio()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "572b3a10-ea9e-4d5e-86ab-d25a87323b05",
   "metadata": {},
   "source": [
    "### Global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467f4252-9629-42d9-a382-caa3178ac010",
   "metadata": {},
   "outputs": [],
   "source": [
    "# General variables:\n",
    "update_time = 0.05               # Time in [s] before the next update\n",
    "thymio_speed_to_mms = 0.4348     # Ratio to convert Thymio speed into mm/s\n",
    "acquire_data = True              # Boolean that determines whether the program should collect data or not\n",
    "samples_acquired = 0             # Number of samples acquired\n",
    "nb_samples = 10                  # Number of samples to reach\n",
    "camera_on = False                # Boolean that determines whether the camera has vision or not\n",
    "\n",
    "#Thymio variables: \n",
    "robot_diameter = 94              # Distance between the two wheels\n",
    "v_r = 0                          # Speed of the right wheel \n",
    "v_l = 0                          # Speed of the left wheel\n",
    "v_r_tot = 0                      # Sum of right speeds over several samples\n",
    "v_l_tot = 0                      # Sum of left speeds over several samples\n",
    "v = 0                            # Average translation speed\n",
    "w = 0                            # Average rotation speed\n",
    "delta_theta = 0                  # Angle variation\n",
    "thymio_data = []\n",
    "\n",
    "#Thymio position and orientation\n",
    "x0_vision = 10\n",
    "y0_vision = 10\n",
    "theta0_vision = np.pi/2\n",
    "x_vision = 20\n",
    "y_vision = 20\n",
    "theta_vision = np.pi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae4e73d9-16a1-430c-82b8-80ca6db29495",
   "metadata": {},
   "source": [
    "### Filter initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ba302c-2c19-453d-969b-3c0b91ae8d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_initialization():\n",
    "    \"\"\"\n",
    "    Initialize the various vectors and matrices requiered for filtering\n",
    "    \n",
    "    param vision_x_position: robot x position deduced from the camera vision\n",
    "    param vision_y_position: robot y position deduced from the camera vision\n",
    "    param vision_theta_orientation: robot theta orientation deduced from the camera vision\n",
    "    \"\"\"\n",
    "    \n",
    "    global s_prev_est_a_posteriori, P_prev_est_a_posteriori, A, B, u, C, Q, R\n",
    "\n",
    "    ## Previous State A Posteriori Estimation Vector\n",
    "    # Vector representing the estimated state of the system at the previous time step\n",
    "    s_prev_est_a_posteriori = np.array([[x0_vision], \n",
    "                                        [y0_vision], \n",
    "                                        [theta0_vision]])\n",
    "    #print(\"\\n\".join([f\"s_prev_est_a_posteriori = [\", \"\\n\".join([\" , \".join([f\"{x: 10.2f}\" for x in row]) for row in s_prev_est_a_posteriori]), \"]\"]))\n",
    "    \n",
    "    ## Previous State A Posteriori Covariance Matrix\n",
    "    # Matrix representing the estimated precision of the previous estimated state\n",
    "    P_prev_est_a_posteriori = np.array([[1000, 0, 0], \n",
    "                                        [0, 1000, 0], \n",
    "                                        [0, 0, 1000]]) \n",
    "    #print(\"\\n\".join([f\"P_prev_est_a_posteriori = [\", \"\\n\".join([\" , \".join([f\"{x: 10.2f}\" for x in row]) for row in P_prev_est_a_posteriori]), \"]\"]))\n",
    "\n",
    "    ## State Matrix\n",
    "    # Matrix defining how the system evolves from one time step to the next\n",
    "    A = np.array([[1, 0, 0], \n",
    "                  [0, 1, 0], \n",
    "                  [0, 0, 1]])\n",
    "    #print(\"\\n\".join([f\"A = [\", \"\\n\".join([\" , \".join([f\"{x: 10.2f}\" for x in row]) for row in A]), \"]\"]))\n",
    "    \n",
    "    ## Input Matrix \n",
    "    # Matrix describing the impact of the input on the state\n",
    "    B = np.array([[1, 0], \n",
    "                  [0, 1], \n",
    "                  [0, 0]]); \n",
    "    #print(\"\\n\".join([f\"B = [\", \"\\n\".join([\" , \".join([f\"{x: 10.2f}\" for x in row]) for row in B]), \"]\"]))\n",
    "    \n",
    "    ## Input Vector\n",
    "    # Vector representing control inputs applied to the system \n",
    "    u = np.array([[0], \n",
    "                  [0]])\n",
    "    #print(\"\\n\".join([f\"u = [\", \"\\n\".join([\" , \".join([f\"{x: 10.2f}\" for x in row]) for row in u]), \"]\"]))\n",
    "\n",
    "    ## Output Matrix\n",
    "    # Matrix linking measurements to state\n",
    "    C = np.array([[1, 0, 0], \n",
    "                  [0, 1, 0], \n",
    "                  [0, 0, 1]])\n",
    "    #print(\"\\n\".join([f\"C = [\", \"\\n\".join([\" , \".join([f\"{x: 10.2f}\" for x in row]) for row in C]), \"]\"]))\n",
    "    \n",
    "    ## Process Noise Covariance Matrix\n",
    "    # Covariance matrix representing uncertainty in system dynamics\n",
    "    Q = np.array([[1, 0, 0], \n",
    "                  [0, 1, 0], \n",
    "                  [0, 0, 0.1]])\n",
    "    #print(\"\\n\".join([f\"Q = [\", \"\\n\".join([\" , \".join([f\"{x: 10.2f}\" for x in row]) for row in Q]), \"]\"]))\n",
    "\n",
    "    ## Measurement Noise Covariance Matrix\n",
    "    # Matrix representing uncertainty of camera measurements\n",
    "    R = np.array([[0.1, 0, 0], \n",
    "                  [0, 0.1, 0], \n",
    "                  [0, 0, 0.01]])\n",
    "    #print(\"\\n\".join([f\"R = [\", \"\\n\".join([\" , \".join([f\"{x: 10.2f}\" for x in row]) for row in R]), \"]\"]))\n",
    "\n",
    "# Check\n",
    "#filter_initialization()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e31997-51f2-4f9e-af3b-f056c4457502",
   "metadata": {},
   "source": [
    "## Update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ddd1e3-fd38-4cf7-9bf0-c29c2e62465e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from threading import Timer\n",
    "\n",
    "# Define a RepeatedTimer class that uses Python's threading module to create \n",
    "# a timer that executes a specific function at regular intervals\n",
    "\n",
    "class RepeatedTimer(object):\n",
    "    def __init__(self, interval, function, *args, **kwargs):\n",
    "        self._timer     = None\n",
    "        self.interval   = interval\n",
    "        self.function   = function\n",
    "        self.args       = args\n",
    "        self.kwargs     = kwargs\n",
    "        self.is_running = False\n",
    "        self.start()\n",
    "\n",
    "    def _run(self):\n",
    "        self.is_running = False\n",
    "        self.start()\n",
    "        self.function(*self.args, **self.kwargs)\n",
    "\n",
    "    def start(self):\n",
    "        if not self.is_running:\n",
    "            self._timer = Timer(self.interval, self._run)\n",
    "            self._timer.start()\n",
    "            self.is_running = True\n",
    "\n",
    "    def stop(self):\n",
    "        self._timer.cancel()\n",
    "        self.is_running = False\n",
    "\n",
    "def get_data():\n",
    "    global v_r, v_l, v_r_tot, v_l_tot, samples_acquired, thymio_data\n",
    "    if samples_acquired<nb_samples:\n",
    "        if samples_acquired == 0:\n",
    "            thymio_data.clear() \n",
    "        thymio_data.append({\"left_speed\":node[\"motor.left.speed\"],\n",
    "                            \"right_speed\":node[\"motor.right.speed\"]})\n",
    "        v_r_tot += node[\"motor.right.speed\"]\n",
    "        v_l_tot += node[\"motor.left.speed\"]\n",
    "        samples_acquired += 1\n",
    "        #print(\"sample number : \", samples_acquired)\n",
    "        #print(\"Right = {}, Left = {}\".format(node[\"motor.right.speed\"], node[\"motor.left.speed\"]))\n",
    "    else:\n",
    "        v_r = round(v_r_tot/nb_samples)\n",
    "        v_l = round(v_l_tot/nb_samples)\n",
    "        \n",
    "async def get_speeds():\n",
    "    global samples_acquired, v_r_tot, v_l_tot\n",
    "    if acquire_data:\n",
    "        await node.wait_for_variables() # wait for Thymio variables values\n",
    "        rt = RepeatedTimer(update_time, get_data) # Auto-start get_data()\n",
    "\n",
    "        try:\n",
    "            await client.sleep(1.5*((nb_samples)*update_time)) # Time needed for the sampling (with a safety constant)\n",
    "        finally:\n",
    "            rt.stop()  \n",
    "            samples_acquired = 0\n",
    "            v_r_tot = 0\n",
    "            v_l_tot = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11527b78-704d-4db4-8cbf-64a5fd0ff9e7",
   "metadata": {},
   "source": [
    "### Updating input vector and matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9137ee10-6c24-4de8-ac54-273a2ebad606",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_input():\n",
    "    \"\"\"\n",
    "    Update the input vector and matrix\n",
    "    \"\"\"\n",
    "    \n",
    "    global B,u\n",
    "    \n",
    "    print(\"v_r :\", v_r)\n",
    "    print(\"v_l :\", v_l)\n",
    "    \n",
    "    # Average translational speed\n",
    "    v = (v_r +v_l)/2 \n",
    "    print(\"v : \", v)\n",
    "    \n",
    "    # Average rotational speed\n",
    "    w = (v_r -v_l)/robot_diameter \n",
    "    \n",
    "    # Input vector\n",
    "    u = np.array([[v], \n",
    "                  [w]]) \n",
    "    #print(\"\\n\".join([f\"u = [\", \"\\n\".join([\" , \".join([f\"{x: 10.2f}\" for x in row]) for row in u]), \"]\"]))\n",
    "    \n",
    "    # Angle variation\n",
    "    delta_theta = w * update_time\n",
    "    \n",
    "    # Input matrix\n",
    "    B = np.array([[np.cos(delta_theta + s_prev_est_a_posteriori[2][0])*update_time, 0],\n",
    "                  [np.sin(delta_theta + s_prev_est_a_posteriori[2][0])*update_time, 0], \n",
    "                  [0, update_time]]); \n",
    "    #print(\"\\n\".join([f\"B = [\", \"\\n\".join([\" , \".join([f\"{x: 10.2f}\" for x in row]) for row in B]), \"]\"]))\n",
    "\n",
    "# Check\n",
    "#update_input()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d65365d-3651-4f97-b443-f965b03c4d57",
   "metadata": {},
   "source": [
    "## Plot the speed graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ab5a66-c444-4306-9eac-1be3b59799b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def speeds_graph():\n",
    "\n",
    "    l_speed = [x[\"left_speed\"] for x in thymio_data]\n",
    "    r_speed = [x[\"right_speed\"] for x in thymio_data]\n",
    "    avg_speed = np.array([(x[\"left_speed\"]+x[\"right_speed\"])/2 for x in thymio_data])\n",
    "    #print(avg_speed)\n",
    "\n",
    "    # Print the speeds\n",
    "    plt.plot(l_speed, label=\"Left motor\")\n",
    "    plt.plot(r_speed, label=\"Right motor\")\n",
    "    plt.plot(avg_speed, label=\"Average\")\n",
    "    plt.xlabel(\"Time step\")\n",
    "    plt.ylabel(\"Measured Velocity\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    ########### Might be useful for the variance calculation ###########\n",
    "    #                                                                  #\n",
    "    # avg_speed = avg_speed[:] / thymio_speed_to_mms                   #\n",
    "    # print(avg_speed)                                                 #\n",
    "    #                                                                  #\n",
    "    # var_speed = np.var(avg_speed)                                    #\n",
    "    # print(\"The speed variance in mm^2/s^2 is {}\".format(var_speed))  #\n",
    "    #                                                                  #\n",
    "    # q_nu = var_speed/2 # variance on speed state                     #\n",
    "    # r_nu = var_speed/2 # variance on speed measurement               #\n",
    "    # print(\"q_nu = {} et r_nu = {}\".format(q_nu, r_nu))               #\n",
    "    #                                                                  #\n",
    "    # qp = 0.04 # variance on position state                           #\n",
    "    # rp = 0.25 # variance on position measurement                     #\n",
    "    #                                                                  #\n",
    "    # qt = 0.04 # variance on orientation state                        #\n",
    "    # rt = 0.25 # variance on orientation measurement                  #\n",
    "    #                                                                  #\n",
    "    ####################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08475c46-3e94-4f8d-bf18-03dda94ed9c1",
   "metadata": {},
   "source": [
    "## Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df849d5b-5733-4c4c-9fef-8cdd3786aaee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kalman_filter(s_prev_est_a_posteriori, P_prev_est_a_posteriori):\n",
    "    \"\"\"\n",
    "    Estimates the current state using the input sensor data and the previous state\n",
    "    \n",
    "    param s_prev_est_a_posteriori: previous state a posteriori estimation\n",
    "    param P_prev_est_a_posteriori: previous state a posteriori covariance\n",
    "    \n",
    "    return s_est_a_posteriori: new a posteriori state estimation\n",
    "    return P_est_a_posteriori: new a posteriori state covariance\n",
    "    \"\"\"\n",
    "    \n",
    "    ## Prediciton through the a priori estimate\n",
    "    # estimated mean of the state\n",
    "    s_est_a_priori = np.dot(A, s_prev_est_a_posteriori)+ np.dot(B, u);\n",
    "    \n",
    "    # Estimated covariance of the state\n",
    "    P_est_a_priori = np.dot(A, np.dot(P_prev_est_a_posteriori, A.T)) + Q\n",
    "    \n",
    "    ## Update         \n",
    "    # m, C, and R for a posteriori estimate, depending on the detection of the camera\n",
    "    if camera_on == True:\n",
    "        m = np.array([[x_vision], \n",
    "                      [y_vision], \n",
    "                      [theta_vision]])\n",
    "        # innovation / measurement residual\n",
    "        i = m - np.dot(C, s_est_a_priori);\n",
    "        # measurement prediction covariance\n",
    "        S = np.dot(C, np.dot(P_est_a_priori, C.T)) + R;     \n",
    "        # Kalman gain (tells how much the predictions should be corrected based on the measurements)\n",
    "        K = np.dot(P_est_a_priori, np.dot(C.T, np.linalg.inv(S)));\n",
    "        # a posteriori estimate\n",
    "        s_est_a_posteriori = s_est_a_priori + np.dot(K,i);\n",
    "        P_est_a_posteriori = P_est_a_priori - np.dot(K,np.dot(C, P_est_a_priori));\n",
    "    else:\n",
    "        K = 0 # Kalman gain is null because the camera can't deliver any data\n",
    "        # a posteriori estimate\n",
    "        s_est_a_posteriori = s_est_a_priori;\n",
    "        P_est_a_posteriori = P_est_a_priori;\n",
    "     \n",
    "    return s_est_a_posteriori, P_est_a_posteriori\n",
    "\n",
    "#Check\n",
    "#kalman_filter(s_prev_est_a_posteriori, P_prev_est_a_posteriori)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77fa8690-7a74-46c7-982a-fafea5e9d2b8",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8860562a-bcc1-4e36-92c5-97ad46b854ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "await connect_Thymio()\n",
    "\n",
    "speed_test_value = randrange(200)      \n",
    "    \n",
    "node.send_set_variables(set_speeds(speed_test_value, speed_test_value))\n",
    "\n",
    "time.sleep(0.5)\n",
    "\n",
    "# Test\n",
    "\n",
    "# Initialization\n",
    "filter_initialization()\n",
    "\n",
    "# Initial state\n",
    "print(\"\\n\".join([f\"Initial state = [\", \"\\n\".join([\" , \".join([f\"{x: 10.2f}\" for x in row]) for row in s_prev_est_a_posteriori]), \"]\"]))\n",
    "\n",
    "for _ in range(3):\n",
    "    await get_speeds()         # Get the speeds\n",
    "    update_input()             # Update inputs\n",
    "    speeds_graph()             # Plot the graph of the speeds\n",
    "    s_prev_est_a_posteriori, P_prev_est_a_posteriori = kalman_filter(s_prev_est_a_posteriori, P_prev_est_a_posteriori)\n",
    "    print(\"\\n\".join([f\"Next state = [\", \"\\n\".join([\" , \".join([f\"{x: 10.2f}\" for x in row]) for row in s_prev_est_a_posteriori]), \"]\"]))\n",
    "\n",
    "node.send_set_variables(set_speeds(0, 0))\n",
    "\n",
    "disconnect_Thymio()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0405d350-7a58-41e8-a076-ddfddd07fe0a",
   "metadata": {},
   "source": [
    "What need to be done with the Thymio used for the final demo:\n",
    "- Evaluate the Q and R matrices\n",
    "- Find the speed in pixels per second : $v_{px/s} = v_{thymio} \\cdot thymio\\_speed\\_to\\_mms \\cdot mm\\_to\\_px$\n",
    "- Time in the B matrix: update_time or 1.5*((nb_samples)*update_time)? Depends on how we decide to get the speed... "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b37673-7537-4eb3-a42a-19581251fe49",
   "metadata": {},
   "source": [
    "## 5 Local Navigation\n",
    "<a id=\"local-navigation\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12420d31-c408-4a79-8bbf-645f9e0c86ac",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6af7c0a-5251-401b-9fdf-be2bca9561db",
   "metadata": {},
   "source": [
    "## 7 Main\n",
    "<a id=\"main\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f818910-0668-4e72-8e89-818877c12d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from ipywidgets import interactive\n",
    "from matplotlib import colors\n",
    "from PIL import Image\n",
    "import math\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f72a4fa8-5d73-47de-a57c-7190521004f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from ipywidgets import interactive\n",
    "from matplotlib import colors\n",
    "from PIL import Image\n",
    "import math\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b83967b-ca11-47f2-bd4e-4084c88d5829",
   "metadata": {},
   "source": [
    "## 6 Conclusion\n",
    "<a id=\"conclusion\"></a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
