{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84588a6d-d18e-4ad6-8d06-e251d942fb7c",
   "metadata": {},
   "source": [
    "# [MICRO-452:] Project Report - Groupe 28\n",
    "**Authors:** Celest Angela Tjong, Adrien Louis Baptiste Dupont, Luca Sidoti Pinto, Didier Henri Neuenschwander\n",
    "**Supervisors:** Prof. Francesco Mondada\n",
    "Date: 17 Novembre 2023\n",
    "\n",
    "[MICRO-452]: **to be changed**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2258c98-b00d-4caf-a0a6-b64e2262d6e0",
   "metadata": {},
   "source": [
    "\n",
    "<span style=\"color:red; font-size:40px;\">use as few personal pronouns as possible (we, our, etc.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "034c8421-4f25-49b6-8bc0-f73cc5369cd5",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "* [1. Introduction](#introduction)\n",
    "* [2. Vision](#vision)\n",
    "    * [2.1. Subsection 1](#vision-subsection-1)\n",
    "    * [2.2. Subsection 2](#vision-subsection-2)\n",
    "* [3. Global Navigation](#global-navigation)\n",
    "* [4. Filtering](#filtering)\n",
    "* [5. Local Navigation](#local-navigation)\n",
    "* [6. Conclusion](#conclusion)\n",
    "lusion)\n",
    "clusion)\n",
    "usion)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd4e09dc-1142-46dd-8485-6bc9b292bf37",
   "metadata": {},
   "source": [
    "## 1 Introduction\n",
    "<a id=\"introduction\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df62e044-49e6-4d91-a7fa-89e01de6e9ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import colors\n",
    "from PIL import Image\n",
    "import math\n",
    "import time\n",
    "from ipywidgets import interactive\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43bfb7cc-aff3-468a-855e-2ef86f760444",
   "metadata": {},
   "source": [
    "## 2 Vision\n",
    "<a id=\"vision\"></a>\n",
    "https://docs.opencv.org/4.x/df/d9d/tutorial_py_colorspaces.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2add8a4f-6198-4070-8728-8f1b016321d7",
   "metadata": {},
   "source": [
    "### Vision Subsection 1\n",
    "<a id=\"Vision-subsection-1\"></a>\n",
    "**Ask to TA if we have to describe every function used in the notebook?**\n",
    "1. **Color Space Conversion**:\n",
    "   -In order to have consistent result for the detection, it is standard to convert     The function begins by converting the input image from the BGR color space (standard in OpenCV) to the HSV color space using `cv2.cvtColor`. HSV (Hue, Saturation, Value) is often more effective for color filtering. Indeed it is particulary usefull for image processing  because it separates color information (hue) from intensity or lighting (value). Thus it allows the recognition to be less dependant of the lighting condition, as it is possible to modify theses parameter. In order to fix the variables of the color used, it is a good practise to calibrate the calibrate the HSV in case of light changes. It has been done with the function...\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "1. **Color Space Conversion**:\n",
    "   -In order to have consistent result for the detection, it is standard to convert     The function begins by converting the input image from the BGR color space (standard in OpenCV) to the HSV color space using `cv2.cvtColor`. HSV (Hue, Saturation, Value) is often more effective for color filtering. Indeed it is particulary usefull for image processing  because it separates color information (hue) from intensity or lighting (value). It allows the recognition to be less dependant of the lighting condition, as it is possible to modify theses parameter. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f86f29-1250-4c61-bda7-d2bb2c9bd996",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define color thresholds in HSV\n",
    "# Note: these thresholds may need to be adjusted for your specific image conditions\n",
    "lower_red_bound = np.array([120, 100, 70])\n",
    "upper_red_bound = np.array([255, 255, 255])\n",
    "lower_green_bound = np.array([60, 50, 100])\n",
    "upper_green_bound = np.array([100, 255, 255])\n",
    "lower_yellow_bound = np.array([0, 50, 120])\n",
    "upper_yellow_bound = np.array([40, 105, 255])\n",
    "lower_black_bound = np.array([0, 0, 0])\n",
    "upper_black_bound = np.array([255, 255, 130])\n",
    "lower_blue_bound = np.array([90, 80, 0])\n",
    "upper_blue_bound = np.array([105, 255, 255])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e07e93e-eef0-40ac-9d5a-eb0bd9dacd52",
   "metadata": {},
   "source": [
    "The first think to do is to preprocess the image for the colour object needed to be located. It is done by first creating a mask. The mask is created by specifying a range of colors (in HSV color space). Pixels within this color range are marked as 1 (or true), while all other pixels are marked as 0 (or false). it is created using `cv2.inRange` which filters out all colors except those within the specified `lower_color_bound` and `upper_color_bound`. This step isolated the specified color. \n",
    "Then the function `cv2.bitwise_and`, extract the area corresponding with the range of colour of the image given as input. It is done by comaparing each pixel of the image with the mask (same size of the image, comaparing with a logical &). \n",
    "Finally The color-filtered image is converted to grayscale using `cv2.cvtColor` because the subsequent edge detection step (Canny) requires a single-channel image.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ef7079-7a86-4716-8507-3cfe19dc34cb",
   "metadata": {},
   "source": [
    "In order to detect the different coloured form, it is common to beginby using a canny filter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa86aa6-a564-4a7d-81b0-1cb0807e6e8a",
   "metadata": {},
   "source": [
    "### Function: `detect_color_circle`\n",
    "\n",
    "#### Purpose:\n",
    "The `detect_color_circle` function is designed to detect circles of a specific color in an image. It employs color filtering, Canny edge detection, and the Hough Circle Transform to achieve this.\n",
    "\n",
    "#### Process:\n",
    "\n",
    "\n",
    "\n",
    "2. **Color Masking**:\n",
    "   - A mask is created using `cv2.inRange` which filters out all colors except those within the specified `lower_color_bound` and `upper_color_bound`. This step isolates the regions of the specified color.\n",
    "\n",
    "3. **Mask Application**:\n",
    "   - The mask is then applied to the original image using `cv2.bitwise_and`. This step ensures that only the parts of the image with the desired color are retained for further processing.\n",
    "\n",
    "4. **Grayscale Conversion**:\n",
    "   - The color-filtered image is converted to grayscale using `cv2.cvtColor` because the subsequent edge detection step (Canny) requires a single-channel image.\n",
    "\n",
    "5. **Canny Edge Detection**:\n",
    "   - `cv2.Canny` is applied to detect edges in the image. It works by identifying areas in the image where sharp changes in intensity occur. The function takes two threshold values (here, 100 and 200) that determine the sensitivity of the edge detection. Edges that are found are used as input for the circle detection.\n",
    "\n",
    "6. **Hough Circle Transform**:\n",
    "   - `cv2.HoughCircles` is used to detect circles in the image. It operates on the principle of the Hough Transform, which is a feature extraction technique used in image analysis. The function detects circles by finding sets of edge points that form a circular shape.\n",
    "   - Parameters like `param1` (higher threshold of the two passed to the Canny edge detector), `param2` (threshold for center detection in the Hough Transform), `minRadius`, and `maxRadius` control the sensitivity and size of the circles to be detected.\n",
    "\n",
    "7. **Output**:\n",
    "   - If circles are detected, the function returns a list of tuples, each containing the `(x, y)` coordinates of the center of a circle and its radius. If no circles are found, it returns an empty list.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b67ae3-5774-4f12-9345-9cc499b15b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import tdmclient Notebook environment:\n",
    "#import tdmclient.notebook\n",
    "#await tdmclient.notebook.start()\n",
    "# forward\n",
    "#motor_left_target= 100\n",
    "#motor_right_target= 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "926418ed-c1c2-4aa3-ac23-8f7630815551",
   "metadata": {},
   "source": [
    "Function that record a video and save it on the folder of the project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa1ae44-55a6-49f6-b96a-0854adea57c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "video_capture = cv2.VideoCapture(0, cv2.CAP_DSHOW)\n",
    "\n",
    "video_capture.set(cv2.CAP_PROP_FRAME_WIDTH, 1080)\n",
    "video_capture.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)\n",
    "# Obtenir les dimensions de la frame\n",
    "frame_width = int(video_capture.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(video_capture.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "# Définir le codec et créer un objet VideoWriter\n",
    "fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "out = cv2.VideoWriter('output.avi', fourcc, 20.0, (frame_width, frame_height))\n",
    "\n",
    "# Enregistrer la vidéo\n",
    "while video_capture.isOpened():\n",
    "    ret, frame = video_capture.read()\n",
    "    if ret:\n",
    "        # Écrire la frame dans le fichier\n",
    "        out.write(frame)\n",
    "\n",
    "        # Afficher la frame (si vous voulez voir le flux en temps réel)\n",
    "        cv2.imshow('Frame', frame)\n",
    "\n",
    "        # Quitter avec la touche 'q'\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "    else:\n",
    "        break\n",
    "\n",
    "# Libérer les ressources\n",
    "video_capture.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32ca27f-3812-4957-ba8b-73ae4c6c68b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Define a function to detect circles of a specific color\n",
    "def detect_color_circle(image, lower_color_bound, upper_color_bound):\n",
    "    # Convert to HSV color space\n",
    "    hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "    # Create a mask for the specified color\n",
    "    mask = cv2.inRange(hsv, lower_color_bound, upper_color_bound)\n",
    "\n",
    "    # Apply the mask to the original image\n",
    "    color_only = cv2.bitwise_and(image, image, mask=mask)\n",
    "\n",
    "    # Convert to grayscale for circle detection\n",
    "    gray = cv2.cvtColor(color_only, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Apply Canny edge detection to help with circle detection\n",
    "    edges = cv2.Canny(gray, 100, 200)\n",
    "\n",
    "    # Use Hough Transform to detect circles\n",
    "    circles = cv2.HoughCircles(edges, cv2.HOUGH_GRADIENT, 1, 20,\n",
    "                           param1=20, param2=15, minRadius=10, maxRadius=50)\n",
    "    \n",
    "    # If circles are detected, return the list of circles with x, y coordinates and radius\n",
    "    if circles is not None:\n",
    "        # Convert the (1, N, 3) array to (N, 3)\n",
    "        circles = np.uint16(np.around(circles[0, :]))          \n",
    "        return [(circle[0], circle[1], circle[2]) for circle in circles]\n",
    "    else:\n",
    "        return []\n",
    "    \n",
    "            \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f4f67d-21ce-4785-a6f9-de19714850bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_obstacle_mask(image, contours, kernel_size):\n",
    "    \"\"\"\n",
    "    Create a mask with zeros in the areas inside the dilated contours.\n",
    "\n",
    "    :param image: Input image.\n",
    "    :param contours: Contours to dilate and fill in the mask.\n",
    "    :param kernel_size: Size of the kernel used for dilation.\n",
    "    :return: Mask with zeros inside the dilated contours and ones elsewhere.\n",
    "    \"\"\"\n",
    "    # Create an empty mask of the same size as the image\n",
    "    h, w = image.shape[:2]\n",
    "    mask = np.ones((h, w), dtype=np.uint8)\n",
    "\n",
    "    # Perform dilation to increase the size of the black regions\n",
    "    kernel = np.ones((kernel_size, kernel_size), np.uint8)\n",
    "    for contour in contours:\n",
    "        # Create an individual mask for each contour\n",
    "        contour_mask = np.zeros((h, w), dtype=np.uint8)\n",
    "        cv2.fillPoly(contour_mask, [contour], 255)\n",
    "        contour_mask = cv2.dilate(contour_mask, kernel, iterations=1)\n",
    "        \n",
    "        # Combine the individual mask with the global mask\n",
    "        mask = cv2.bitwise_and(mask, cv2.bitwise_not(contour_mask))\n",
    "\n",
    "        #also add the contours\n",
    "        # Let's create a border around the image\n",
    "        border_size = 50\n",
    "        border_color = [0, 0, 0]  # Black border\n",
    "        # Use cv2.copyMakeBorder to add a border around the image\n",
    "        mask_with_border = cv2.copyMakeBorder(mask, border_size, border_size, border_size, border_size,\n",
    "                                           cv2.BORDER_CONSTANT, value=border_color)\n",
    "    \n",
    "    return mask_with_border\n",
    "\n",
    "\n",
    "# Now you have a mask with zeros in the obstacle areas and ones elsewhere\n",
    "# You can return this mask from your function or process it further as needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd427fec-563d-47a2-8b57-8ae7e5731492",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "def scale_contour(contour, scale):\n",
    "    M = cv2.moments(contour)\n",
    "    if M['m00'] == 0:\n",
    "        return contour\n",
    "    cx = int(M['m10']/M['m00'])\n",
    "    cy = int(M['m01']/M['m00'])\n",
    "    scaled_contour = np.zeros_like(contour)\n",
    "    for i, point in enumerate(contour):\n",
    "        x, y = point[0]\n",
    "        dx = x - cx\n",
    "        dy = y - cy\n",
    "        scaled_contour[i] = [[int(cx + dx * scale), int(cy + dy * scale)]]\n",
    "    return scaled_contour\n",
    "\n",
    "def detect_obstacle_contours(image, area_threshold, scale_factor):\n",
    "    hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "    mask_black = cv2.inRange(hsv, lower_black_bound, upper_black_bound)\n",
    "    contours, _ = cv2.findContours(mask_black, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    filtered_contours = [cnt for cnt in contours if cv2.contourArea(cnt) > area_threshold]\n",
    "\n",
    "    scaled_contours = [scale_contour(cnt, scale_factor) for cnt in filtered_contours]\n",
    "    \n",
    "    contour_image = image.copy()\n",
    "    cv2.drawContours(contour_image, filtered_contours, -1, (0, 255, 0), 2)\n",
    "    cv2.drawContours(contour_image, scaled_contours, -1, (0, 0, 255), 2)\n",
    "    return contour_image, filtered_contours, scaled_contours\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7151aada-0dc7-4d3d-9514-dbfca0ad35fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def detect_obstacle_contours(image, area_threshold, kernel_size):\n",
    "    \"\"\"\n",
    "    Detects and dilates obstacle contours in the given image.\n",
    "    :param image: Input image.\n",
    "    :param area_threshold: Area threshold for filtering contours.\n",
    "    :param kernel_size: Size of the kernel used for dilation.\n",
    "    :return: Image with obstacle contours drawn.\n",
    "    \"\"\"\n",
    "    hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "    mask_black = cv2.inRange(hsv, lower_black_bound, upper_black_bound)\n",
    "    contours, _ = cv2.findContours(mask_black, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    filtered_contours = [cnt for cnt in contours if cv2.contourArea(cnt) > area_threshold]\n",
    "    kernel = np.ones((kernel_size, kernel_size), np.uint8)\n",
    "    mask_dilated = cv2.dilate(mask_black, kernel, iterations=1)\n",
    "    dilated_contours, _ = cv2.findContours(mask_dilated, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    filtered_dilated_contours = [cnt for cnt in dilated_contours if cv2.contourArea(cnt) > area_threshold+10000]\n",
    "    contour_image = image.copy()\n",
    "    cv2.drawContours(contour_image, filtered_contours, -1, (0, 255, 0), 2)\n",
    "    cv2.drawContours(contour_image, filtered_dilated_contours, -1, (0, 0, 255), 2)\n",
    "    return contour_image, filtered_contours, filtered_dilated_contours\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13047d64-3da8-4f07-b297-a7fd8fc28292",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_obstacle_matrix(image, dilated_contours):\n",
    "    height, width = image.shape[:2]\n",
    "    obstacle_matrix = np.ones((height, width), dtype=np.uint8)\n",
    "\n",
    "    for contour in dilated_contours:\n",
    "        # Remplir chaque contour dilaté avec 0 (obstacle)\n",
    "        cv2.fillPoly(obstacle_matrix, [contour], 0)\n",
    "\n",
    "    return obstacle_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3675f92-3883-46ae-9b15-f1df15e42525",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_roi_from_circles(image, circles):\n",
    "    if circles is not None and len(circles) >= 4:\n",
    "        # Assurez-vous que les points sont dans le format correct\n",
    "        points = np.array([circle[:2] for circle in circles], dtype=np.float32)\n",
    "\n",
    "        # Calcul de la boîte englobante\n",
    "        rect = cv2.boundingRect(points)\n",
    "\n",
    "        # Recadrage de l'image\n",
    "        x, y, w, h = rect\n",
    "        cropped_image = image[y:y+h, x:x+w]\n",
    "        return cropped_image,(x, y, w, h)\n",
    "    else:\n",
    "        print(\"Nombre insuffisant de cercles détectés ou format incorrect.\")\n",
    "        return image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e39f10-cfe2-4ce8-a98c-7aaf8a7ffec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_blue_rectangle_center(image, lower_yellow_bound, upper_yellow_bound):\n",
    "    # Convertir l'image en espace de couleur HSV\n",
    "    hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "    # Créer un masque pour la couleur bleue\n",
    "    mask = cv2.inRange(hsv, lower_yellow_bound, upper_yellow_bound)\n",
    "\n",
    "    # Trouver les contours dans le masque\n",
    "    contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    # Identifier le contour qui correspond au rectangle bleu\n",
    "    for contour in contours:\n",
    "        # Optionnel : vérifier si le contour est suffisamment grand ou a une forme spécifique\n",
    "\n",
    "        # Calculer la boîte englobante pour le contour\n",
    "        x, y, w, h = cv2.boundingRect(contour)\n",
    "\n",
    "        # Calculer le centre du rectangle\n",
    "        center = (x + w // 2, y + h // 2)\n",
    "        return center\n",
    "\n",
    "    return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee2f0da7-ba21-4f7f-8161-3b76a6e77880",
   "metadata": {},
   "source": [
    "In each frame, there are multiple elements that need to be displayed. The first category includes static elements such as the contours of obstacles and the goal point. Since these do not change over time, it is computationally more efficient to identify and locate them in the first frame, and then display them consistently in subsequent frames.\n",
    "\n",
    "On the other hand, elements related to the robot's localization must be determined in each frame. To gather information about its orientation and location, three circles are placed on the top of the robot. A green circle is located at the middle back, and two red circles are positioned on each side, with their centers aligned with the robot's ceeris. This arrangement facilitates the extraction of necessary information. By connecting the centers of the two red circles and creating a midpoint, a vector c thenan be formed by connecting this midpoint to the center of the green circ.It might also be feasible to use only two markers (circles), one at the back and one at the front, distinguished by their color. However, the marker at the front would hide the press button.le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c51b5c-0df7-4528-99d5-a5e367371f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the recorded video file\n",
    "#video_path = 'output.avi'\n",
    "\n",
    "# Open the video file\n",
    "# video_capture = cv2.VideoCapture(video_path)\n",
    "\n",
    "# # Ensure that the video file opens correctly\n",
    "# if not video_capture.isOpened():\n",
    "#     print(\"Error: Unable to open video file.\")\n",
    "#     exit()\n",
    "\n",
    "video_capture = cv2.VideoCapture(0, cv2.CAP_DSHOW)\n",
    "\n",
    "video_capture.set(cv2.CAP_PROP_FRAME_WIDTH, 1080)\n",
    "video_capture.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)\n",
    "window_name = 'Robot Detection'\n",
    "cv2.namedWindow(window_name, cv2.WINDOW_NORMAL)\n",
    "\n",
    "# Resize the window (width, height)\n",
    "cv2.resizeWindow(window_name, 540, 360)  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Initial detection of obstacles and goal\n",
    "ret, initial_frame = video_capture.read()\n",
    "if ret:\n",
    "     #initial_frame, cropping_coords = crop_largest_white_area(initial_frame, 200000)\n",
    "     blue_circles = detect_color_circle(initial_frame, lower_blue_bound, upper_blue_bound)\n",
    "     # for (x, y, r) in blue_circles:\n",
    "     #         cv2.circle(initial_frame, (x, y), r, (255, 0, 0), 2)  # Dessiner en bleu\n",
    "     # #cv2.circle(, (blue_circles[0], blue_circles[1]), 2, (0, 0, 255), 3)\n",
    "     initial_frame ,cropping_coords= crop_roi_from_circles(initial_frame, blue_circles)\n",
    "     contour_image = detect_obstacle_contours(initial_frame, 1000, 50)\n",
    "     global_obstacle = create_obstacle_matrix(initial_frame,contour_image[1])\n",
    "     # center = find_blue_rectangle_center(initial_frame, lower_yellow_bound, upper_yellow_bound)\n",
    "     # if center:\n",
    "     #    # Store the coordinates of the detected yellow circle\n",
    "     #    yellow_circle_coords = center  # center est déjà un tuple (x, y)\n",
    "     #    radius = 10  # Rayon du cercle, vous pouvez ajuster cette valeur\n",
    "     #    color = (0, 255, 255)  # Couleur jaune en BGR\n",
    "     #    cv2.circle(initial_frame, yellow_circle_coords, radius, color, 3)\n",
    "    # Dessiner le cercle\n",
    "     initial_frame = np.flipud(initial_frame)\n",
    "     plt.imshow(cv2.cvtColor(contour_image[0], cv2.COLOR_BGR2RGB))\n",
    "     plt.title('Initial Contours')\n",
    "     plt.axis('off')\n",
    "     plt.show()\n",
    "\n",
    "\n",
    "# Robot update frequency (10 Hz)\n",
    "update_rate = 0.01  # 10 times per second\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Capture frame-by-frame\n",
    "        ret, frame = video_capture.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        if cropping_coords is not None:\n",
    "            x, y, w, h = cropping_coords\n",
    "            frame = frame[y:y+h, x:x+w]\n",
    "        # if cropping_coords:\n",
    "        #     x, y, w, h = cropping_coords\n",
    "        #     frame = frame[y:y+h, x:x+w]\n",
    "        # Update obstacle contours for the current frame\n",
    " \n",
    "        # Detect red and green circles\n",
    "        red_circles = detect_color_circle(frame, lower_red_bound, upper_red_bound)\n",
    "        green_circles = detect_color_circle(frame, lower_green_bound, upper_green_bound)\n",
    "\n",
    "        # if center:\n",
    "        #     cv2.circle(initial_frame, yellow_circle_coords, radius, color, 3)\n",
    "\n",
    "        if red_circles and green_circles:\n",
    "                    if len(red_circles) >= 2:\n",
    "                        # Calculate the midpoint between the centers of the red circles\n",
    "                        midpoint = ((red_circles[0][0] + red_circles[1][0]) // 2,\n",
    "                        (red_circles[0][1] + red_circles[1][1]) // 2)\n",
    "                        # Calculate the directional vector\n",
    "                        direction = np.array([midpoint[0] - green_circles[0][0], midpoint[1] - green_circles[0][1]])\n",
    "                    \n",
    "                        # Normalize and extend the vector\n",
    "                        length = 100  # Additional length\n",
    "                        direction = direction / np.linalg.norm(direction) * length\n",
    "                    \n",
    "                        # Calculate the new endpoint\n",
    "                        new_endpoint = (int(green_circles[0][0] + direction[0]), int(green_circles[0][1] + direction[1]))\n",
    "                    \n",
    "                        # Draw the extended arrow\n",
    "                        cv2.arrowedLine(frame, green_circles[0][:2], new_endpoint, (0, 0, 0), 3)\n",
    "\n",
    "                        # Calculate the angle of orientation with respect to the x-axis\n",
    "                        dx = green_circles[0][0] - midpoint[0]\n",
    "                        dy = green_circles[0][1] - midpoint[1]\n",
    "                        angle = math.atan2(dy, dx)\n",
    "                        angle_degrees = math.degrees(angle)\n",
    "                        if(angle_degrees>= 0):\n",
    "                            angle_degrees =180 - angle_degrees\n",
    "                        elif (angle_degrees < 0):\n",
    "                            angle_degrees = -(180 + angle_degrees)\n",
    "                        robot_vector = (midpoint[0], midpoint[1], angle_degrees) #information of the robot\n",
    "                        # Optionally, display the angle\n",
    "                        cv2.putText(frame, f'Angle: {angle_degrees:.2f} degrees', (10, 30), \n",
    "                                    cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n",
    "                     \n",
    "                        cv2.putText(frame, f'Midpoint: ({midpoint[0]}, {midpoint[1]})', (10, 700),\n",
    "                                    cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n",
    "        cv2.drawContours(frame, contour_image[1], -1, (0, 255, 0), 2)\n",
    "        cv2.drawContours(frame, contour_image[2], -1, (0, 0, 255), 2)\n",
    "       # cv2.circle(frame, (x, y), r, (0, 255, 255), 3)\n",
    "        # Frame dimensions\n",
    "\n",
    "        height, width = frame.shape[:2]\n",
    "\n",
    "        # Coordinates of the starting point of the reference frame (bottom right of the image)\n",
    "        origin_x, origin_y = width - 150, height - 70  # Adjustment for a length of 100\n",
    "        # Draw the X axis\n",
    "        cv2.line(frame, (origin_x, origin_y), (origin_x + 100, origin_y), (0, 0, 255), 2)\n",
    "        # Draw the Y axis\n",
    "        cv2.line(frame, (origin_x, origin_y), (origin_x, origin_y - 100), (0, 255, 0), 2)\n",
    "        # Mark the length on the X axis\n",
    "        cv2.putText(frame, \"100\", (origin_x + 100, origin_y + 15), cv2.FONT_HERSHEY_SIMPLEX, 0.4, (0, 0, 255), 2)\n",
    "        # Mark the length on the Y axis\n",
    "        cv2.putText(frame, \"100\", (origin_x - 30, origin_y - 100), cv2.FONT_HERSHEY_SIMPLEX, 0.4, (255, 0, 255), 2)\n",
    "        cv2.putText(frame, f'({origin_x}, {origin_y})', (origin_x-200, origin_y-10),\n",
    "        cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2) #about 14cm for 100pixels\n",
    "        # ... (the rest of your code to display the frame)\n",
    "\n",
    "        frame = np.flipud(frame)\n",
    "\n",
    "        # Display the resulting frame\n",
    "        cv2.imshow('Robot Detection', frame)\n",
    "\n",
    "        # Pause to maintain the update frequency\n",
    "        time_to_wait = max(int((start_time + update_rate - time.time()) * 1000), 1)\n",
    "        if cv2.waitKey(time_to_wait) & 0xFF == ord('q'):\n",
    "            break\n",
    "finally:\n",
    "    # Release the capture when everything is finished\n",
    "    video_capture.release()\n",
    "    cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378b385e-6a1e-411d-b32f-3582d5787673",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(center)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec623cd9-75a3-4ae9-b9df-8e11b8a2ac20",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(threshold=np.inf)\n",
    "\n",
    "# Afficher la matrice global_obstacle\n",
    "print(global_obstacle)\n",
    "plt.imshow(global_obstacle, cmap='gray')\n",
    "plt.title('Matrice d\\'Obstacles')\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "x, y, r = yellow_circles[0] #coordinate of the goal*********************************************************"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dffc645-cada-46ec-949e-b630f247bfa3",
   "metadata": {},
   "source": [
    "\n",
    "\"Initially, for the setup, it's advantageous to position the camera above the play area. This placement minimizes the need to account for perspective distortions. To accurately determine the robot's position and orientation, at least two distinct markers should be placed on the robot. These markers enable precise tracking and analysis of the robot's movements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54b1493-4591-41f1-8e1f-c6d43c869c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Detect red and green circles\n",
    "red_circles = detect_color_circle(image, lower_red_bound, upper_red_bound)\n",
    "green_circles = detect_color_circle(image, lower_green_bound, upper_green_bound)\n",
    "yellow_circles = detect_color_circle(image, lower_yellow_bound, upper_yellow_bound)\n",
    "\n",
    "red_circles, green_circles, yellow_circles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da82c685-ef08-4273-b907-37b1a9e68876",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Assuming red_circles and green_circles contain the detected circles for each color\n",
    "# For demonstration, let's create dummy circle data\n",
    "# red_circles = [(x1, y1, r1), (x2, y2, r2)]\n",
    "# green_circles = [(x3, y3, r3)]\n",
    "\n",
    "# TODO: Replace the dummy values with your actual circle centers and radii\n",
    "#red_circles = [(50, 50, 30), (150, 50, 30)]  # Dummy values\n",
    "#green_circles = [(100, 150, 30)]  # Dummy values\n",
    "\n",
    "# Calculate the midpoint between the centers of the red circles\n",
    "midpoint = ((red_circles[0][0] + red_circles[1][0]) // 2,\n",
    "            (red_circles[0][1] + red_circles[1][1]) // 2)\n",
    "\n",
    "# Draw a line (and arrow) from the green circle's center to the midpoint\n",
    "cv2.arrowedLine(image,  green_circles[0][:2],midpoint, (0, 255, 0), 20)\n",
    "\n",
    "# Calculate the angle of orientation with respect to the x-axis\n",
    "dx = green_circles[0][0] - midpoint[0]\n",
    "dy = green_circles[0][1] - midpoint[1]\n",
    "angle = math.atan2(dy, dx)\n",
    "angle_degrees = math.degrees(angle)\n",
    "\n",
    "# Display the image with the drawn arrow\n",
    "plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "plt.title(f'Angle of orientation: {angle_degrees:.2f} degrees')\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "# Return the midpoint and the angle\n",
    "midpoint, angle_degrees\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67438223-d9b1-418a-8930-f751226a0e25",
   "metadata": {},
   "source": [
    "For the obstacle detection: we used black shapes that we randomly distribute around the board. It is "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e368801-d787-4c38-840d-e4f21a95c84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#image_path = 'WIN_20231124_15_09_52_Pro.jpg'\n",
    "image = initial_frame #cv2.imread(image_path)\n",
    "image = frame.copy()  # Créer une copie de l'image\n",
    "# Convert to HSV color space\n",
    "hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "# Define the lower and upper bounds for the black color\n",
    "lower_black = np.array([0, 0, 0])\n",
    "upper_black = np.array([180, 150, 40])\n",
    "\n",
    "# Create a black color mask\n",
    "mask_black = cv2.inRange(hsv, lower_black, upper_black)\n",
    "\n",
    "# Find contours in the mask\n",
    "contours, _ = cv2.findContours(mask_black, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "# Set a realistic threshold for the area of the contours\n",
    "area_threshold = 2000  # Adjust this threshold according to your needs\n",
    "\n",
    "# Filter the original contours that are larger than the threshold\n",
    "filtered_contours = [cnt for cnt in contours if cv2.contourArea(cnt) > area_threshold]\n",
    "\n",
    "# Perform dilation to increase the size of the black regions\n",
    "kernel_size = 70  # Kernel size can be adjusted to control the amount of dilation\n",
    "kernel = np.ones((kernel_size, kernel_size), np.uint8)\n",
    "mask_dilated = cv2.dilate(mask_black, kernel, iterations=1)\n",
    "\n",
    "# Find contours in the dilated mask\n",
    "dilated_contours, _ = cv2.findContours(mask_dilated, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "# Filter the dilated contours that are larger than the threshold\n",
    "filtered_dilated_contours = [cnt for cnt in dilated_contours if cv2.contourArea(cnt) > area_threshold]\n",
    "\n",
    "# Draw the filtered original contours in green\n",
    "for contour in filtered_contours:\n",
    "    cv2.drawContours(image, [contour], -1, (0, 255, 0), 2)\n",
    "\n",
    "# Draw the filtered dilated contours in red\n",
    "for contour in filtered_dilated_contours:\n",
    "    cv2.drawContours(image, [contour], -1, (0, 0, 255), 2)\n",
    "\n",
    "# Display the image with the drawn contours\n",
    "plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "plt.title('Filtered Original and Dilated Contours')\n",
    "plt.axis('off')  # Hide the axis\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "864f1ea7-1588-4b28-8bbc-6a1fd6180ce2",
   "metadata": {},
   "source": [
    "# Tool functions that help with parameter tuning:\n",
    "<a id=\"Vision-subsection-2\"></a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa54b1a6-9448-4b46-a8e2-939ef1afe52b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_detected_circles(image, circles, circle_color):\n",
    "    \"\"\"\n",
    "    Draws the detected circles on the image and plots it.\n",
    "\n",
    "    :param image: The original image.\n",
    "    :param circles: A list of circles with their coordinates and radius.\n",
    "    :param circle_color: The color to use for drawing the circles.\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    if circles is not None and len(circles) > 0:\n",
    "        for circle in circles:\n",
    "            center = (circle[0], circle[1])  # Circle center\n",
    "            radius = circle[2]  # Circle radius\n",
    "            # Draw the circle's perimeter\n",
    "            cv2.circle(image, center, radius, circle_color, 2)\n",
    "            # Draw the circle's center\n",
    "            cv2.circle(image, center, 2, circle_color, 3)\n",
    "\n",
    "    # Plot the image with detected circles\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "    plt.axis('off')  # Hide the axis\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe05eb0-2409-4b08-8e58-e852199f2b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Vérification si la frame a été capturée avec succès\n",
    "if ret:\n",
    "    # Conversion de l'image en RGB pour l'affichage avec Matplotlib\n",
    "    frame_rgb = cv2.cvtColor(initial_frame, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Affichage de l'image\n",
    "    plt.imshow(initial_frame)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Erreur lors de la capture de l'image.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a142009-f980-44a4-85a6-6302ccfc62a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtenir les dimensions de l'image\n",
    "h, w = frame_rgb.shape[:2]\n",
    "\n",
    "# Calculer le nombre total de pixels\n",
    "total_pixels = h * w\n",
    "\n",
    "print(f\"The image is of dimension {w}x{h} (width x hight)\")\n",
    "print(f\"The total number of pixels is : {total_pixels}\")\n",
    "\n",
    "height, width = global_obstacle.shape\n",
    "\n",
    "print(\"Hauteur de la matrice:\", height)\n",
    "print(\"Largeur de la matrice:\", width)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bfd92df-e5c6-432d-ae01-eb5227452df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Load the image\n",
    "image1 = initial_frame #cv2.imread('WIN_20231124_15_09_52_Pro.jpg')\n",
    "image1= image1.copy()\n",
    "# Convert to HSV color space\n",
    "hsv = cv2.cvtColor(image1, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "# Define the lower and upper bounds for the red color\n",
    "# Note: Adjust these values according to your color calibration\n",
    "\n",
    "\n",
    "# Create a red color mask\n",
    "mask_red = cv2.inRange(hsv, lower_blue_bound, upper_blue_bound)\n",
    "\n",
    "# Apply the mask to the image\n",
    "red_only = cv2.bitwise_and(image1, image1, mask=mask_red)\n",
    "\n",
    "# Convert the result to grayscale\n",
    "gray = cv2.cvtColor(red_only, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "\n",
    "# Apply Canny edge detection to help with circle detection\n",
    "edges = cv2.Canny(gray, 100, 200)\n",
    "\n",
    "# Detect circles using the Hough Transform\n",
    "circles = cv2.HoughCircles(edges, cv2.HOUGH_GRADIENT, 1, 20,\n",
    "                           param1=20, param2=15, minRadius=10, maxRadius=50)\n",
    "\n",
    "# If circles are detected, draw them\n",
    "if circles is not None:\n",
    "    circles = np.uint16(np.around(circles))\n",
    "    for i in circles[0, :]:\n",
    "        cv2.circle(image1, (i[0], i[1]), i[2], (0, 255, 0), 2)\n",
    "        cv2.circle(image1, (i[0], i[1]), 2, (0, 0, 255), 3)\n",
    "\n",
    "# Plotting the different stages\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(1, 4, 1)\n",
    "plt.imshow(cv2.cvtColor(image1, cv2.COLOR_BGR2RGB))\n",
    "plt.title('Original Image')\n",
    "\n",
    "plt.subplot(1, 4, 2)\n",
    "plt.imshow(mask_red, \n",
    "           cmap='gray')\n",
    "plt.title('Red Mask')\n",
    "\n",
    "plt.subplot(1, 4, 3)\n",
    "plt.imshow(edges, cmap='gray')\n",
    "plt.title('Canny Edges')\n",
    "\n",
    "plt.subplot(1, 4, 4)\n",
    "plt.imshow(cv2.cvtColor(image1, cv2.COLOR_BGR2RGB))\n",
    "plt.title('Detected Circles')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a355820-2984-4a83-b507-c85b3b551eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from ipywidgets import interactive\n",
    "\n",
    "def interactive_mask(lower_h, lower_s, lower_v, upper_h, upper_s, upper_v):\n",
    "    \n",
    "    lower_color_bound = np.array([lower_h, lower_s, lower_v])\n",
    "    upper_color_bound = np.array([upper_h, upper_s, upper_v])\n",
    "\n",
    "    # Ensure initial_frame is defined here, or pass it as an argument to this function\n",
    "    image = initial_frame.copy()\n",
    "    \n",
    "    # Convert to HSV color space\n",
    "    hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "    \n",
    "    mask = cv2.inRange(hsv, lower_color_bound, upper_color_bound)\n",
    "    plt.imshow(mask, cmap='gray')\n",
    "    plt.show()\n",
    "\n",
    "# Define initial_frame here or load it before this point\n",
    "# initial_frame = cv2.imread('your_image.jpg')\n",
    "\n",
    "# Setup interactive widgets\n",
    "interactive(interactive_mask, \n",
    "            lower_h=(0,255), lower_s=(0,255), lower_v=(0,255),\n",
    "            upper_h=(0,255), upper_s=(0,255), upper_v=(0,255))\n",
    "# Assuming 'hsv' is your converted HSV image\n",
    "#lower_red_bound = np.array([120, 70, 120])\n",
    "#upper_red_bound = np.array([255, 255, 255])\n",
    "#lower_green_bound = np.array([60, 120, 100])\n",
    "#upper_green_bound = np.array([100, 255, 255])\n",
    "#lower_black = np.array([0, 0, 0])\n",
    "#upper_black = np.array([255, 255, 130])\n",
    "#lower_yellow_bound = np.array([0, 60, 140])\n",
    "#upper_yellow_bound = np.array([40, 105, 255])\n",
    "#lower_white_bound = np.array([0, 0, 200], dtype=np.uint8)\n",
    "#upper_white_bound = np.array([180, 55, 255], dtype=np.uint8)\n",
    "#lower_blue_bound = np.array([00, 80, 0], dtype=np.uint8)\n",
    "#upper_blue_bound = np.array([105, 255, 255], dtype=np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b98467-3505-40c8-9417-8236cdbad250",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = frame #cv2.imread('WIN_20231124_15_09_52_Pro.jpg')\n",
    "red_circles = detect_color_circle(image, lower_red_bound, upper_red_bound)\n",
    "green_circles = detect_color_circle(image, lower_green_bound, upper_green_bound)\n",
    "yellow_circles = detect_color_circle(image, lower_yellow_bound, upper_yellow_bound)\n",
    "\n",
    "red_circles, green_circles, yellow_circles\n",
    "\n",
    "# Assuming red_circles and green_circles contain the detected circles for each color\n",
    "image_copy = image.copy()  # Make a copy to draw on\n",
    "plot_detected_circles(image_copy, red_circles, (200, 0, 255))  # Red color for red circles\n",
    "plot_detected_circles(image_copy, green_circles, (0, 255, 0))  # Green color for green circles\n",
    "plot_detected_circles(image_copy, yellow_circles,(255,255,153))  # yellow color for yellow circles\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e89d89a2-0751-4907-9a38-cee54fc239fd",
   "metadata": {},
   "source": [
    "## 3 Global Navigation\n",
    "<a id=\"global-navigation\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a11331",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_obstacle = np.logical_not( global_obstacle )\n",
    "max_val_x_init = global_obstacle.shape[0]\n",
    "max_val_y_init = global_obstacle.shape[1]\n",
    "\n",
    "reduction_coeff = 20 # tune for speed\n",
    "max_val_x = int(max_val_x_init / reduction_coeff)\n",
    "max_val_y = int(max_val_y_init / reduction_coeff)\n",
    "\n",
    "occupancy_grid = np.zeros((max_val_x, max_val_y), dtype=int)\n",
    "for i in range (max_val_x):\n",
    "    for j in range (max_val_y):\n",
    "        sum_pixels = 0\n",
    "        for k in range (reduction_coeff): # dans le doute on augmente la distance de sécurité avec obstacle\n",
    "            sum_pixels = sum_pixels + global_obstacle[int(i * reduction_coeff - reduction_coeff/2 + k)][int(j * reduction_coeff - reduction_coeff/2 + k)]\n",
    "        if sum_pixels == 0:\n",
    "            occupancy_grid[i][j] = 0\n",
    "        else:\n",
    "            occupancy_grid[i][j] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca9bb6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_empty_plot(max_val):\n",
    "    \"\"\"\n",
    "    Helper function to create a figure of the desired dimensions & grid\n",
    "    \n",
    "    :param max_val: dimension of the map along the x and y dimensions\n",
    "    :return: the fig and ax objects.\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(7,7))\n",
    "    \n",
    "    major_ticks = np.arange(0, max_val+1, 5)\n",
    "    minor_ticks = np.arange(0, max_val+1, 1)\n",
    "    ax.set_xticks(major_ticks)\n",
    "    ax.set_xticks(minor_ticks, minor=True)\n",
    "    ax.set_yticks(major_ticks)\n",
    "    ax.set_yticks(minor_ticks, minor=True)\n",
    "    ax.grid(which='minor', alpha=0.2)\n",
    "    ax.grid(which='major', alpha=0.5)\n",
    "    #ax.set_ylim([-1,max_val_x])\n",
    "    ax.set_ylim([max_val_x,-1])\n",
    "    ax.set_xlim([-1,max_val_y])\n",
    "    ax.grid(True)\n",
    "    \n",
    "    return fig, ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf223c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing the grid\n",
    "\n",
    "fig, ax = create_empty_plot(max_val_y)\n",
    "\n",
    "cmap = colors.ListedColormap(['white', 'red']) # Select the colors with which to display obstacles and free cells\n",
    "\n",
    "#occupancy_grid = np.logical_not(occupancy_grid)\n",
    "\n",
    "# Displaying the map\n",
    "ax.imshow(occupancy_grid, cmap=cmap)\n",
    "plt.title(\"Map : free cells in white, occupied cells in red\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8716398d",
   "metadata": {},
   "source": [
    "## A* implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9959a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_movements_8n():\n",
    "    \"\"\"\n",
    "    Get all possible 8-connectivity movements. Equivalent to get_movements_in_radius(1)\n",
    "    (up, down, left, right and the 4 diagonals).\n",
    "    :return: list of movements with cost [(dx, dy, movement_cost)]\n",
    "    \"\"\"\n",
    "    s2 = math.sqrt(2)\n",
    "    return [(1, 0, 1.0),\n",
    "            (0, 1, 1.0),\n",
    "            (-1, 0, 1.0),\n",
    "            (0, -1, 1.0),\n",
    "            (1, 1, s2),\n",
    "            (-1, 1, s2),\n",
    "            (-1, -1, s2),\n",
    "            (1, -1, s2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3865680",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def reconstruct_path(cameFrom, current):\n",
    "    \"\"\"\n",
    "    Recurrently reconstructs the path from start node to the current node\n",
    "    :param cameFrom: map (dictionary) containing for each node n the node immediately \n",
    "                     preceding it on the cheapest path from start to n \n",
    "                     currently known.\n",
    "    :param current: current node (x, y)\n",
    "    :return: list of nodes from start to current node\n",
    "    \"\"\"\n",
    "    total_path = [current]\n",
    "    while current in cameFrom.keys():\n",
    "        # Add where the current node came from to the start of the list\n",
    "        total_path.insert(0, cameFrom[current]) \n",
    "        current=cameFrom[current]\n",
    "    return total_path\n",
    "\n",
    "def A_Star(start, goal, h, coords, occupancy_grid, movement_type=\"8N\", max_val_x=max_val_x, max_val_y=max_val_y):\n",
    "    \"\"\"\n",
    "    A* for 2D occupancy grid. Finds a path from start to goal.\n",
    "    h is the heuristic function. h(n) estimates the cost to reach goal from node n.\n",
    "    :param start: start node (x, y)\n",
    "    :param goal_m: goal node (x, y)\n",
    "    :param occupancy_grid: the grid map\n",
    "    :param movement: select between 4-connectivity ('4N') and 8-connectivity ('8N', default)\n",
    "    :return: a tuple that contains: (the resulting path in meters, the resulting path in data array indices)\n",
    "    \"\"\"\n",
    "    \n",
    "    # -----------------------------------------\n",
    "    # DO NOT EDIT THIS PORTION OF CODE\n",
    "    # -----------------------------------------\n",
    "    \n",
    "    # Check if the start and goal are within the boundaries of the map\n",
    "    for point in [start, goal]:\n",
    "        for coord in point:\n",
    "            assert coord>=0 and coord<max_val_y, \"start or end goal not contained in the map\"\n",
    "    \n",
    "    # check if start and goal nodes correspond to free spaces\n",
    "    if occupancy_grid[start[0], start[1]]:\n",
    "        raise Exception('Start node is not traversable')\n",
    "\n",
    "    if occupancy_grid[goal[0], goal[1]]:\n",
    "        raise Exception('Goal node is not traversable')\n",
    "    \n",
    "    # get the possible movements corresponding to the selected connectivity\n",
    "    if movement_type == '4N':\n",
    "        movements = _get_movements_4n()\n",
    "    elif movement_type == '8N':\n",
    "        movements = _get_movements_8n()\n",
    "    else:\n",
    "        raise ValueError('Unknown movement')\n",
    "    \n",
    "    # --------------------------------------------------------------------------------------------\n",
    "    # A* Algorithm implementation - feel free to change the structure / use another pseudo-code\n",
    "    # --------------------------------------------------------------------------------------------\n",
    "    \n",
    "    # The set of visited nodes that need to be (re-)expanded, i.e. for which the neighbors need to be explored\n",
    "    # Initially, only the start node is known.\n",
    "    openSet = [start]\n",
    "    \n",
    "    # The set of visited nodes that no longer need to be expanded.\n",
    "    closedSet = []\n",
    "\n",
    "    # For node n, cameFrom[n] is the node immediately preceding it on the cheapest path from start to n currently known.\n",
    "    cameFrom = dict()\n",
    "\n",
    "    # For node n, gScore[n] is the cost of the cheapest path from start to n currently known.\n",
    "    gScore = dict(zip(coords, [np.inf for x in range(len(coords))]))\n",
    "    gScore[start] = 0\n",
    "\n",
    "    # For node n, fScore[n] := gScore[n] + h(n). map with default value of Infinity\n",
    "    fScore = dict(zip(coords, [np.inf for x in range(len(coords))]))\n",
    "    fScore[start] = h[start]\n",
    "\n",
    "    # while there are still elements to investigate\n",
    "    while openSet != []:\n",
    "        \n",
    "        #the node in openSet having the lowest fScore[] value\n",
    "        fScore_openSet = {key:val for (key,val) in fScore.items() if key in openSet}\n",
    "        current = min(fScore_openSet, key=fScore_openSet.get)\n",
    "        del fScore_openSet\n",
    "        \n",
    "        #If the goal is reached, reconstruct and return the obtained path\n",
    "        if current == goal:\n",
    "            return reconstruct_path(cameFrom, current), closedSet\n",
    "\n",
    "        openSet.remove(current)\n",
    "        closedSet.append(current)\n",
    "        \n",
    "        #for each neighbor of current:\n",
    "        for dx, dy, deltacost in movements:\n",
    "            \n",
    "            neighbor = (current[0]+dx, current[1]+dy)\n",
    "            \n",
    "            # if the node is not in the map, skip\n",
    "            if (neighbor[0] >= occupancy_grid.shape[0]) or (neighbor[1] >= occupancy_grid.shape[1]) or (neighbor[0] < 0) or (neighbor[1] < 0):\n",
    "                continue\n",
    "            \n",
    "            # if the node is occupied or has already been visited, skip\n",
    "            if (occupancy_grid[neighbor[0], neighbor[1]]) or (neighbor in closedSet): \n",
    "                continue\n",
    "                \n",
    "            # d(current,neighbor) is the weight of the edge from current to neighbor\n",
    "            # tentative_gScore is the distance from start to the neighbor through current\n",
    "            tentative_gScore = gScore[current] + deltacost\n",
    "            \n",
    "            if neighbor not in openSet:\n",
    "                openSet.append(neighbor)\n",
    "                \n",
    "            if tentative_gScore < gScore[neighbor]:\n",
    "                # This path to neighbor is better than any previous one. Record it!\n",
    "                cameFrom[neighbor] = current\n",
    "                gScore[neighbor] = tentative_gScore\n",
    "                fScore[neighbor] = gScore[neighbor] + h[neighbor]\n",
    "\n",
    "    # Open set is empty but goal was never reached\n",
    "    print(\"No path found to goal\")\n",
    "    return [], closedSet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555b8fb5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-08T22:44:38.966544Z",
     "start_time": "2020-05-08T22:44:37.185492Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define the start and end goal\n",
    "start = (int(robot_vector[0]/reduction_coeff),int(robot_vector[1]/reduction_coeff))\n",
    "#goal = (int(630 /reduction_coeff),int(900 /reduction_coeff))\n",
    "goal = (25,39)\n",
    "\n",
    "\n",
    "# -----------------------------------------\n",
    "# DO NOT EDIT THIS PORTION OF CODE - \n",
    "# EXECUTION AND PLOTTING OF THE ALGORITHM\n",
    "# -----------------------------------------\n",
    "    \n",
    "    \n",
    "# List of all coordinates in the grid\n",
    "w,z = np.mgrid[0:max_val_x:1, 0:max_val_y:1]\n",
    "pos = np.empty(w.shape + (2,))\n",
    "pos[:, :, 0] = w; pos[:, :, 1] = z\n",
    "pos = np.reshape(pos, (w.shape[0]*w.shape[1], 2))\n",
    "coords = list([(int(w[0]), int(w[1])) for w in pos])\n",
    "\n",
    "# Define the heuristic, here = distance to goal ignoring obstacles\n",
    "h = np.linalg.norm(pos - goal, axis=-1)\n",
    "h = dict(zip(coords, h))\n",
    "\n",
    "# Run the A* algorithm\n",
    "path, visitedNodes = A_Star(start, goal, h, coords, occupancy_grid, movement_type=\"8N\")\n",
    "path = np.array(path).reshape(-1, 2).transpose()\n",
    "visitedNodes = np.array(visitedNodes).reshape(-1, 2).transpose()\n",
    "\n",
    "path_final = path * reduction_coeff\n",
    "\n",
    "# Displaying the map\n",
    "fig_astar, ax_astar = create_empty_plot(max_val_y)\n",
    "ax_astar.imshow(occupancy_grid, cmap=cmap)\n",
    "print(path_final)\n",
    "# Plot the best path found and the list of visited nodes\n",
    "ax_astar.scatter(visitedNodes[1], visitedNodes[0], marker=\"o\", color = 'orange');\n",
    "ax_astar.plot(path[1], path[0], marker=\"o\", color = 'blue');\n",
    "ax_astar.scatter(start[1], start[0], marker=\"o\", color = 'green', s=200);\n",
    "ax_astar.scatter(goal[1], goal[0], marker=\"o\", color = 'purple', s=200);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c71b292d",
   "metadata": {},
   "source": [
    "- blanc : libre\n",
    "- rouge : obstacle\n",
    "- orange : case explorée\n",
    "- bleu : chemin le plus court\n",
    "- vert : départ\n",
    "- violet : arrivée"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88357db4",
   "metadata": {},
   "source": [
    "Maintenant que nous avons calculé le chemin le plus court, nous appelons des fonctions pour diriger le robot dans la bonne direction. Les données qui sortent de ce fichier sont dans la matrice path_final qui regroupe les coordonées des étapes recalculées à la bonne échelle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6476dab3-1207-4529-b878-436ab4b969bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import\n",
    "import time\n",
    "import asyncio\n",
    "from tdmclient import aw, ClientAsync\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "# conversion thymio speed to mm/s\n",
    "Thymio_to_mms = 0.349\n",
    "px_to_mm = 140/100\n",
    "#Thymio_to_pxs = Thymio_to_mms * mm_to_px \n",
    "\n",
    "# Thymio connection\n",
    "async def connect_Thymio():\n",
    "    \"\"\"\n",
    "    Establish a connection with the Thymio if possible\n",
    "    \"\"\"\n",
    "    global node, client\n",
    "    try:\n",
    "        client = ClientAsync()\n",
    "        node = await asyncio.wait_for(client.wait_for_node(), timeout=2.0)\n",
    "        await node.lock()\n",
    "        print(\"Thymio connected\")\n",
    "\n",
    "    except asyncio.TimeoutError:\n",
    "        print(\"Thymio not connected: Timeout while waiting for node.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Thymio not connected: {str(e)}\")\n",
    "        \n",
    "# Thymio disconnection\n",
    "def disconnect_Thymio():\n",
    "    \"\"\"\n",
    "    Enable to disconnect the Thymio\n",
    "    \"\"\"\n",
    "    aw(node.stop())\n",
    "    aw(node.unlock())\n",
    "    print(\"Thymio disconnected\")\n",
    "        \n",
    "# Thymio control motor speeds  \n",
    "async def set_speeds(left_speed, right_speed):\n",
    "    \"\"\"\n",
    "    Enable to set the speed of the Thymio's wheels\n",
    "    \"\"\"\n",
    "    global node\n",
    "    v = {\n",
    "        \"motor.left.target\":  [left_speed],\n",
    "        \"motor.right.target\": [right_speed],\n",
    "    }\n",
    "    await node.set_variables(v)\n",
    "    \n",
    "async def motors_stop():\n",
    "    \"\"\"\n",
    "    Stop the Thymio\n",
    "    \"\"\"\n",
    "    global node\n",
    "    v = {\n",
    "        \"motor.left.target\":  [0],\n",
    "        \"motor.right.target\": [0],\n",
    "    }\n",
    "    await node.set_variables(v)    \n",
    "    \n",
    "# Check\n",
    "#await connect_Thymio()\n",
    "#await set_speeds(40, -40)\n",
    "#time.sleep(2)\n",
    "#await motors_stop()\n",
    "#disconnect_Thymio()\n",
    "\n",
    "# Turn a specified angle \n",
    "\n",
    "# Constants\n",
    "ROTATION_SPEED = 100\n",
    "TIME_FULL_TURN = (8800/1000)\n",
    "\n",
    "async def turn(angle):\n",
    "    # Calculate the time needed to turn through the required angle\n",
    "    rotation_time = (abs(angle) / (2*np.pi)) * TIME_FULL_TURN\n",
    "\n",
    "    # Turn robot on itself\n",
    "    # Check the sign of angle\n",
    "    if np.sign(angle) > 0:\n",
    "        # If angle is positive, turn in one direction\n",
    "        await set_speeds(-ROTATION_SPEED, ROTATION_SPEED)\n",
    "    else:\n",
    "        # If angle is negative, turn in the other direction\n",
    "        await set_speeds(ROTATION_SPEED, -ROTATION_SPEED)\n",
    "\n",
    "    # Wait required time\n",
    "    time.sleep(rotation_time)\n",
    "\n",
    "    # Stop the robot\n",
    "    #await motors_stop()\n",
    "\n",
    "# Check\n",
    "gamma = np.pi\n",
    "await connect_Thymio()\n",
    "await turn(-gamma)\n",
    "disconnect_Thymio()\n",
    "\n",
    "# Constants\n",
    "FORWARD_SPEED = 200  # Base speed\n",
    "TIME_PER_MM = 15.5/1000  # Time it takes for the robot to travel one meter at base speed\n",
    "\n",
    "async def move_forward(distance_px):\n",
    "    # Calculate the time needed to travel the requested distance\n",
    "    \n",
    "    distance_mm = distance_px * px_to_mm\n",
    "    travel_time = (distance_mm) * TIME_PER_MM\n",
    "    \n",
    "    # Robot moves forward\n",
    "    await set_speeds(FORWARD_SPEED, FORWARD_SPEED)\n",
    "\n",
    "    # Wait for the necessary time\n",
    "    time.sleep(travel_time)\n",
    "    #print(\"End of forward after : \", travel_time)\n",
    "\n",
    "    # Stop the robot\n",
    "    #await motors_stop()\n",
    "\n",
    "# Using the function\n",
    "\n",
    "#await connect_Thymio()\n",
    "# distance = 1  # Distance to travel in meters\n",
    "#await move_forward(distance)\n",
    "#disconnect_Thymio()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25af5fc7-58f6-4295-9130-3b3eada1816e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Test\n",
    "\n",
    "import math\n",
    "import time\n",
    "import asyncio\n",
    "import numpy as np\n",
    "\n",
    "robot_vector = robot_vector\n",
    "start = start = path_final[:, 0:1]\n",
    "goal = path_final[:, -1:]\n",
    "\n",
    "gamma = 0 \n",
    "current_node = 0\n",
    "next_node = 1\n",
    "dist_to_node = 0\n",
    "norm_vector_node = np.array([0,0]) \n",
    "\n",
    "ANGLE_THRESHOLD = 0.1 \n",
    "FORWARD_THRESHOLD = 1 \n",
    "ROTATION_TIME_THRESHOLD = 1.2\n",
    "FORWARD_TIME_THRESHOLD = 100\n",
    "current_angle = np.radians(robot_vector[2])\n",
    "\n",
    "await connect_Thymio()\n",
    "\n",
    "while True:\n",
    "    print(current_node)\n",
    "    # Normal vector\n",
    "    norm_vector_node[0] = path_final[0][next_node] - path_final[0][current_node]\n",
    "    norm_vector_node[1] = path_final[1][next_node] - path_final[1][current_node ] \n",
    "    norm_vector_node = norm_vector_node / np.linalg.norm(norm_vector_node)\n",
    "    \n",
    "    # angle gamma\n",
    "    gamma = math.atan2(norm_vector_node[0], norm_vector_node[1]) - current_angle #angle en rad\n",
    "    \n",
    "    # distance d\n",
    "    path_current_node = np.array([path_final[0][next_node], path_final[1][next_node]])\n",
    "    path_previous_node = np.array([path_final[0][current_node], path_final[1][current_node]])\n",
    "    d = np.linalg.norm(path_current_node - path_previous_node) # distance en px/20\n",
    "    print(\"gamma : \", gamma)\n",
    "    print(\"d : \", d)\n",
    "    \n",
    "    if(abs(gamma) > ANGLE_THRESHOLD):\n",
    "        await turn(gamma)\n",
    "    \n",
    "    current_angle += gamma\n",
    "    \n",
    "    if( d >= FORWARD_THRESHOLD):\n",
    "        await move_forward(d)\n",
    "    \n",
    "    current_node += 1\n",
    "    next_node += 1\n",
    "    \n",
    "    if current_node == path_final.shape[1]:\n",
    "        break\n",
    "await motors_stop()\n",
    "disconnect_Thymio()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba16bf1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def reach_next_node(next_node, mode, estimated_pos):\n",
    "\n",
    "    # Normal vector\n",
    "    norm_vector_node = np.array([0,0])\n",
    "    # Normal vector   \n",
    "    norm_vector_node[0] = path_final[0][next_node] - estimated_pos[0]\n",
    "    norm_vector_node[1] = path_final[1][next_node] - estimated_pos[1] \n",
    "    norm_vector_node = norm_vector_node / np.linalg.norm(norm_vector_node)\n",
    "    \n",
    "    # angle gamma\n",
    "    gamma = - math.atan2(norm_vector_node[0], norm_vector_node[1]) - estimated_pos[2]\n",
    "    \n",
    "    # distance d\n",
    "    path_next_node = np.array(path[next_node][:])\n",
    "    path_current_node = np.array([estimated_pos[0], estimated_pos[1]])\n",
    "    d = np.linalg.norm(path_next_node - path_current_node) \n",
    "\n",
    "    print(d)\n",
    "    if(not mode):\n",
    "        if(abs(gamma) > ANGLE_THRESHOLD):\n",
    "            await turn(gamma)\n",
    "    \n",
    "        #robot_vector[2] = gamma + estimated_pos[2]\n",
    "        \n",
    "    if (mode):\n",
    "        if( d > FORWARD_THRESHOLD):\n",
    "            await move_forward(d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c19adfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "import asyncio\n",
    "import numpy as np\n",
    "\n",
    "path_final = np.array([[140, 160, 180, 200, 220, 240, 260, 280, 300, 300, 300, 320, 320, 340, 340, 360, 360, 360, 360, 360, 360, 380, 400, 420, 440, 440, 460, 460, 480, 480, 480, 500],\n",
    "                       [160, 180, 200, 220, 240, 260, 280, 300, 320, 340, 360, 380, 400, 420, 440, 460, 480, 500, 520, 540, 560, 580, 600, 620, 640, 660, 680, 700, 720, 740, 760, 780]])\n",
    "\n",
    "current_node = 0\n",
    "next_node = 1\n",
    "estimated_pos = np.array([140, 160, 0])\n",
    "norm_vector_node = np.array([0,0])\n",
    "current_angle = estimated_pos[2]\n",
    "current_pos = np.array([estimated_pos[0], estimated_pos[1]])\n",
    "\n",
    "ANGLE_THRESHOLD = 0.1 \n",
    "FORWARD_THRESHOLD = 1 \n",
    "ROTATION_TIME_THRESHOLD = 1.2\n",
    "FORWARD_TIME_THRESHOLD = 100\n",
    "\n",
    "await connect_Thymio()\n",
    "                \n",
    "while True:\n",
    "    # Normal vector   \n",
    "    norm_vector_node[0] = path_final[0][next_node] - current_pos[0]\n",
    "    #print(norm_vector_node[0])\n",
    "    norm_vector_node[1] = path_final[1][next_node] - current_pos[1] \n",
    "    #print(norm_vector_node[1])\n",
    "\n",
    "    # angle gamma\n",
    "    gamma = - math.atan2(norm_vector_node[0], norm_vector_node[1]) - current_angle\n",
    "    #print(\"current angle\", current_node, \": \", current_angle*180/np.pi)\n",
    "    print(\"gamma\", current_node, \": \", gamma)\n",
    "\n",
    "    # distance d\n",
    "    path_next_node = np.array([path_final[0][next_node], path_final[1][next_node]])\n",
    "    current_pos = path_next_node \n",
    "    #print(path_next_node)\n",
    "    path_current_node = np.array([path_final[0][current_node], path_final[1][current_node]])\n",
    "    #print(path_current_node)\n",
    "    d = np.linalg.norm(path_next_node - path_current_node)\n",
    "    print(\"d\", current_node ,\": \", d)\n",
    "\n",
    "    if(abs(gamma) > ANGLE_THRESHOLD):\n",
    "            await turn(gamma)\n",
    "\n",
    "    current_angle += gamma\n",
    "\n",
    "    if( d >= FORWARD_THRESHOLD):\n",
    "            await move_forward(d)\n",
    "\n",
    "    current_node += 1\n",
    "    next_node += 1\n",
    "\n",
    "    if next_node == path_final.shape[1]:\n",
    "        break\n",
    "\n",
    "await motors_stop()\n",
    "disconnect_Thymio()\n",
    "print(\"fin\")\n",
    "\n",
    "# start position\n",
    "#start = path_final[:, 0:1]\n",
    "#print(\"start position: \", start)\n",
    "# goal position\n",
    "#goal = path_final[:, -1:]\n",
    "#print(\"goal position: \",goal)\n",
    "\n",
    "\n",
    "# current position\n",
    "#position = path_final[:, current_node:next_node]\n",
    "#print(\"current position: \", position)\n",
    "\n",
    "#current_node += 1\n",
    "#next_node += 1\n",
    "\n",
    "# next position\n",
    "#position = path_final[:, current_node:next_node]\n",
    "#print(\"next position: \", position)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ff5e1f-15bb-476c-a740-0f5fc32e9e8a",
   "metadata": {},
   "source": [
    "# Kalman Filter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a33aa833-15b3-4877-86cb-171284040b07",
   "metadata": {},
   "source": [
    "The localization of the Thymio robot is performed using a Kalman filter. This filtering method is well suited to estimating the position and orientation of a mobile robot from noisy or incomplete measurements. The design of the filter in this project is based on using the position ($x, y$) and orientation ($\\theta$) provided by the camera as measurements. In addition, the speed of the robot, provided by the wheel speed sensors ($v_r, v_l$), is used as a prediction. In short, the Kalman filter merges a prediction of the system's future state with a measurement of that state to estimate position probabilistically."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fbb3fad-4699-42d6-98ba-b3e8a164e731",
   "metadata": {},
   "source": [
    "## State-space model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fdeaa31-8b8b-4b90-a272-c872a56a40d4",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "161c8da7-511a-4963-9766-d39749ef9757",
   "metadata": {},
   "source": [
    "To estimate the robot's future position, a state-space model needs to be developed: \n",
    "\n",
    "$$\\hat{s}_{a\\_priori}^{t+1} = A \\cdot \\hat{s}_{a\\_posteriori}^{t} + B \\cdot u^{t} + q^t$$\n",
    "\n",
    "The prediction of the future state is referred to as $\\hat{s}_{a\\_priori}^{t+1}$, i.e. the a priori estimate at time t+1. Since the state of the system is defined by its position ($x, y$) and orientation ($\\theta$), this gives: \n",
    "\n",
    "$$\\hat{s}_{a\\_priori}^{t+1} = \\begin{pmatrix}\n",
    "\\hat{x}_{a\\_priori}^{t+1} \\\\\\\\\n",
    "\\hat{y}_{a\\_priori}^{t+1} \\\\\\\\\n",
    "\\hat{\\theta}_{a\\_priori}^{t+1}\n",
    "\\end{pmatrix}$$\n",
    "\n",
    "The current state corresponds to the term $\\hat{s}_{a\\_posteriori}^{t}$, which is the a posteriori estimate at time t. In the same way as above, this gives:\n",
    "\n",
    "$$\\hat{s}_{a\\_posteriori}^{t} = \\begin{pmatrix}\n",
    "\\hat{x}_{a\\_posteriori}^{t} \\\\\\\\\n",
    "\\hat{y}_{a\\_posteriori}^{t} \\\\\\\\\n",
    "\\hat{\\theta}_{a\\_posteriori}^{t}\n",
    "\\end{pmatrix}$$\n",
    "\n",
    "The system input at time t is represented by the vector $u^{t}$. This is made up of two terms: translational speed ($v$) and rotational speed ($\\omega$). \n",
    "\n",
    "$$u^t = \\begin{pmatrix}\n",
    "v \\\\\\\\\n",
    "\\omega\n",
    "\\end{pmatrix} $$\n",
    "\n",
    "These are defined on the basis of the speeds measured by the wheel speed sensors, i.e. the right ($v_r$) and left ($v_l$) speeds, and the spacing between the two wheels ($e$).\n",
    "\n",
    "$$ v = \\cfrac{v_r + v_l}{2} \\qquad\\qquad \\omega = \\cfrac{v_r-v_l}{e} $$ \n",
    "\n",
    "Matrix A characterizes the evolution of the system state, while matrix B describes the impact of the input on the future state. An odometry-based approach allows us to determine these two matrices by considering a very short time interval ($\\delta t$). During this time interval, the robot rotates by $\\delta \\theta = \\omega \\cdot \\delta t$. Knowing this, and referring to the diagram below, the following system of equations can be established: \n",
    "\n",
    "$$\\begin{equation}\n",
    "\\begin{cases}\n",
    "\\hat{x}_{a\\_priori}^{t+1} = \\hat{x}_{a\\_posteriori}^{t} + v \\cdot \\cos\\left(\\hat{\\theta}_{a\\_posteriori}^{t} + \\delta \\theta^t \\right) \\cdot \\delta t \\\\\n",
    "\\hat{y}_{a\\_priori}^{t+1} = \\hat{y}_{a\\_posteriori}^{t} + v \\cdot \\sin\\left(\\hat{\\theta}_{a\\_posteriori}^{t} + \\delta \\theta^t \\right) \\cdot \\delta t \\\\\n",
    "\\hat{\\theta}_{a\\_priori}^{t+1} = \\hat{\\theta}_{a\\_posteriori}^{t} + \\omega \\cdot \\delta t\n",
    "\\end{cases}\n",
    "\\end{equation}$$\n",
    "\n",
    "![state-space_model](Images/schematics.png)\n",
    "\n",
    "The matrix form of this system therefore becomes:\n",
    "\n",
    "$$\\begin{equation}\n",
    "A = \\begin{bmatrix} \n",
    "1 & 0 & 0\\\\ \n",
    "0 & 1 & 0 \\\\ \n",
    "0 & 0 & 1\n",
    "\\end{bmatrix}\n",
    "\\qquad\\qquad\n",
    "B = \\begin{bmatrix} \n",
    "\\cos\\left(\\hat{\\theta}_{a\\_posteriori}^{t} + \\delta \\theta^t \\right) \\cdot \\delta t & 0\\\\\n",
    "\\sin\\left(\\hat{\\theta}_{a\\_posteriori}^{t} + \\delta \\theta^t \\right) \\cdot \\delta t & 0 \\\\\n",
    "0 & \\delta t \n",
    "\\end{bmatrix}\n",
    "\\end{equation}$$\n",
    "\n",
    "The final term $q^t$ of this state-space model represents the stochastic perturbation of the state with covariance matrix Q defined as follows:\n",
    "\n",
    "$$\n",
    "Q = \\begin{bmatrix} \n",
    "q_1 & 0 & 0\\\\ \n",
    "0 & q_2 & 0 \\\\ \n",
    "0 & 0 & q_3\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "These diagonal coefficients can be evaluated using an approach similar to that used in Exercise 8 of the MICRO-452 course.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b284d0-0bd3-4e9c-a4c6-7545a786c8d7",
   "metadata": {},
   "source": [
    "### Measurement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa9dbef-5fff-4319-b6ae-b8130a602071",
   "metadata": {},
   "source": [
    "Having explored the prediction phase of the state-space model, attention now turns to the second essential part: updating the measurements. This stage aims to refine the predictions by integrating real information captured by the camera. The formula governing this step is :\n",
    "\n",
    "$$ m^{t+1} = C \\cdot s^{t+1} + r^{t+1}$$ \n",
    "\n",
    "Measurements taken at time t+1 are represented here by the term $m_{t+1}$. The data collected by the camera are therefore:\n",
    "\n",
    "$$m^{t+1} = \\begin{pmatrix}\n",
    "x_{captured}^{t+1} \\\\\\\\\n",
    "y_{captured}^{t+1} \\\\\\\\\n",
    "\\theta_{captured}^{t+1}\n",
    "\\end{pmatrix}$$\n",
    "\n",
    "The robot's position ($x, y$) and orientation ($\\theta$) measured by the camera are used directly as system outputs, without any transformation. The matrix C linking the measurements to the state is therefore defined as follows:\n",
    "\n",
    "$$C = \\begin{bmatrix} \n",
    "1 & 0 & 0\\\\ \n",
    "0 & 1 & 0 \\\\ \n",
    "0 & 0 & 1\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "The term $s^{t+1}$ simply represents the state of the system at time t+1:\n",
    "\n",
    "$$s^{t+1} = \\begin{pmatrix}\n",
    "x^{t+1} \\\\\\\\\n",
    "y^{t+1} \\\\\\\\\n",
    "\\theta^{t+1}\n",
    "\\end{pmatrix}$$\n",
    "\n",
    "Finally, the last term $r^{t+1}$ of this equation represents noise on measurements with a covariance matrix R defined as follows:\n",
    "\n",
    "$$\n",
    "R = \\begin{bmatrix} \n",
    "r_1 & 0 & 0\\\\ \n",
    "0 & r_2 & 0 \\\\ \n",
    "0 & 0 & r_3\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Note: When the camera's view is obstructed, estimation is only possible using the prediction model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ae69e3-47a8-4904-97e7-50ad37fe2fbf",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "186b10f8-a2a2-4687-9734-f2bf1590144e",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f20ee2a-0bf1-4375-94ad-6c2e58ec7e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "import scipy\n",
    "import asyncio\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from random import randrange\n",
    "from tdmclient import aw, ClientAsync"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "572b3a10-ea9e-4d5e-86ab-d25a87323b05",
   "metadata": {},
   "source": [
    "### Global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467f4252-9629-42d9-a382-caa3178ac010",
   "metadata": {},
   "outputs": [],
   "source": [
    "# General variables:\n",
    "update_time = 0.05               # Time in [s] before the next update\n",
    "thymio_speed_to_mms = 0.4348     # Ratio to convert Thymio speed into mm/s\n",
    "acquire_data = True              # Boolean that determines whether the program should collect data or not\n",
    "samples_acquired = 0             # Number of samples acquired\n",
    "nb_samples = 10                  # Number of samples to reach\n",
    "camera_on = False                # Boolean that determines whether the camera has vision or not\n",
    "\n",
    "#Thymio variables: \n",
    "robot_diameter = 94              # Distance between the two wheels\n",
    "v_r = 0                          # Speed of the right wheel \n",
    "v_l = 0                          # Speed of the left wheel\n",
    "v = 0                            # Average translation speed\n",
    "w = 0                            # Average rotation speed\n",
    "delta_theta = 0                  # Angle variation\n",
    "thymio_data = []\n",
    "\n",
    "#Thymio position and orientation\n",
    "x0_vision = 10\n",
    "y0_vision = 10\n",
    "theta0_vision = np.pi/2\n",
    "x_vision = 20\n",
    "y_vision = 20\n",
    "theta_vision = np.pi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae4e73d9-16a1-430c-82b8-80ca6db29495",
   "metadata": {},
   "source": [
    "### Filter initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ba302c-2c19-453d-969b-3c0b91ae8d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_initialization():\n",
    "    \"\"\"\n",
    "    Initialize the various vectors and matrices requiered for filtering\n",
    "    \n",
    "    robot_vector: position (x and y) and orientation taken from the camera vision\n",
    "    \"\"\"\n",
    "    \n",
    "    global s_prev_est_a_posteriori, P_prev_est_a_posteriori, A, B, u, C, Q, R\n",
    "\n",
    "    ## Previous State A Posteriori Estimation Vector\n",
    "    # Vector representing the estimated state of the system at the previous time step\n",
    "    s_prev_est_a_posteriori = robot_vector\n",
    "    #print(\"\\n\".join([f\"s_prev_est_a_posteriori = [\", \"\\n\".join([\" , \".join([f\"{x: 10.2f}\" for x in row]) for row in s_prev_est_a_posteriori]), \"]\"]))\n",
    "    \n",
    "    ## Previous State A Posteriori Covariance Matrix\n",
    "    # Matrix representing the estimated precision of the previous estimated state\n",
    "    P_prev_est_a_posteriori = np.array([[1000, 0, 0], \n",
    "                                        [0, 1000, 0], \n",
    "                                        [0, 0, 1000]]) \n",
    "    #print(\"\\n\".join([f\"P_prev_est_a_posteriori = [\", \"\\n\".join([\" , \".join([f\"{x: 10.2f}\" for x in row]) for row in P_prev_est_a_posteriori]), \"]\"]))\n",
    "\n",
    "    ## State Matrix\n",
    "    # Matrix defining how the system evolves from one time step to the next\n",
    "    A = np.array([[1, 0, 0], \n",
    "                  [0, 1, 0], \n",
    "                  [0, 0, 1]])\n",
    "    #print(\"\\n\".join([f\"A = [\", \"\\n\".join([\" , \".join([f\"{x: 10.2f}\" for x in row]) for row in A]), \"]\"]))\n",
    "    \n",
    "    ## Input Matrix \n",
    "    # Matrix describing the impact of the input on the state\n",
    "    B = np.array([[1, 0], \n",
    "                  [0, 1], \n",
    "                  [0, 0]]); \n",
    "    #print(\"\\n\".join([f\"B = [\", \"\\n\".join([\" , \".join([f\"{x: 10.2f}\" for x in row]) for row in B]), \"]\"]))\n",
    "    \n",
    "    ## Input Vector\n",
    "    # Vector representing control inputs applied to the system \n",
    "    u = np.array([0, 0])\n",
    "    #print(\"\\n\".join([f\"u = [\", \"\\n\".join([\" , \".join([f\"{x: 10.2f}\" for x in row]) for row in u]), \"]\"]))\n",
    "\n",
    "    ## Output Matrix\n",
    "    # Matrix linking measurements to state\n",
    "    C = np.array([[1, 0, 0], \n",
    "                  [0, 1, 0], \n",
    "                  [0, 0, 1]])\n",
    "    #print(\"\\n\".join([f\"C = [\", \"\\n\".join([\" , \".join([f\"{x: 10.2f}\" for x in row]) for row in C]), \"]\"]))\n",
    "    \n",
    "    ## Process Noise Covariance Matrix\n",
    "    # Covariance matrix representing uncertainty in system dynamics\n",
    "    Q = np.array([[1, 0, 0], \n",
    "                  [0, 1, 0], \n",
    "                  [0, 0, 0.1]])\n",
    "    #print(\"\\n\".join([f\"Q = [\", \"\\n\".join([\" , \".join([f\"{x: 10.2f}\" for x in row]) for row in Q]), \"]\"]))\n",
    "\n",
    "    ## Measurement Noise Covariance Matrix\n",
    "    # Matrix representing uncertainty of camera measurements\n",
    "    R = np.array([[0.1, 0, 0], \n",
    "                  [0, 0.1, 0], \n",
    "                  [0, 0, 0.01]])\n",
    "    #print(\"\\n\".join([f\"R = [\", \"\\n\".join([\" , \".join([f\"{x: 10.2f}\" for x in row]) for row in R]), \"]\"]))\n",
    "\n",
    "# Check\n",
    "#filter_initialization()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e31997-51f2-4f9e-af3b-f056c4457502",
   "metadata": {},
   "source": [
    "## Update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9137ee10-6c24-4de8-ac54-273a2ebad606",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_input(v_l,v_r,update_time):\n",
    "    \"\"\"\n",
    "    Update the input vector and matrix\n",
    "    \n",
    "    v_l: robot x position deduced from the camera vision\n",
    "    v_r: robot y position deduced from the camera vision\n",
    "    update_time: robot theta orientation deduced from the camera vision\n",
    "    \"\"\"\n",
    "    \n",
    "    global B,u\n",
    "    \n",
    "    Thymio_to_mms = 0.349\n",
    "    mm_to_px = 100/140\n",
    "    \n",
    "    # Average translational speed\n",
    "    v = (v_r +v_l)/2 # Thymio speed (T)\n",
    "    v = v * Thymio_to_mms * mm_to_px # Speed in px/s (T -> mm/s -> px/s)\n",
    "\n",
    "    # Average rotational speed\n",
    "    w = (v_r -v_l)/robot_diameter # Thymio speed (T)\n",
    "    \n",
    "    # Input vector\n",
    "    u = np.array([v, w]) \n",
    "    #print(\"\\n\".join([f\"u = [\", \"\\n\".join([\" , \".join([f\"{x: 10.2f}\" for x in row]) for row in u]), \"]\"]))\n",
    "    \n",
    "    # Angle variation\n",
    "    delta_theta = w * update_time\n",
    "    \n",
    "    # Input matrix\n",
    "    B = np.array([[np.cos(delta_theta + s_prev_est_a_posteriori[2])*update_time, 0],\n",
    "                  [np.sin(delta_theta + s_prev_est_a_posteriori[2])*update_time, 0], \n",
    "                  [0, update_time]]); \n",
    "    #print(\"\\n\".join([f\"B = [\", \"\\n\".join([\" , \".join([f\"{x: 10.2f}\" for x in row]) for row in B]), \"]\"]))\n",
    "\n",
    "# Check\n",
    "#update_input()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08475c46-3e94-4f8d-bf18-03dda94ed9c1",
   "metadata": {},
   "source": [
    "## Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df849d5b-5733-4c4c-9fef-8cdd3786aaee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kalman_filter(s_prev_est_a_posteriori, P_prev_est_a_posteriori):\n",
    "    \"\"\"\n",
    "    Estimates the current state using the input sensor data and the previous state\n",
    "    \n",
    "    param s_prev_est_a_posteriori: previous state a posteriori estimation\n",
    "    param P_prev_est_a_posteriori: previous state a posteriori covariance\n",
    "    \n",
    "    return s_est_a_posteriori: new a posteriori state estimation\n",
    "    return P_est_a_posteriori: new a posteriori state covariance\n",
    "    \"\"\"\n",
    "    \n",
    "    ## Prediciton through the a priori estimate\n",
    "    # estimated mean of the state\n",
    "    s_est_a_priori = np.dot(A, s_prev_est_a_posteriori)+ np.dot(B, u);\n",
    "    \n",
    "    # Estimated covariance of the state\n",
    "    P_est_a_priori = np.dot(A, np.dot(P_prev_est_a_posteriori, A.T)) + Q\n",
    "    \n",
    "    ## Update         \n",
    "    # m, C, and R for a posteriori estimate, depending on the detection of the camera\n",
    "    if camera_on == True:\n",
    "        m = get_position()\n",
    "        # innovation / measurement residual\n",
    "        i = m - np.dot(C, s_est_a_priori);\n",
    "        # measurement prediction covariance\n",
    "        S = np.dot(C, np.dot(P_est_a_priori, C.T)) + R;     \n",
    "        # Kalman gain (tells how much the predictions should be corrected based on the measurements)\n",
    "        K = np.dot(P_est_a_priori, np.dot(C.T, np.linalg.inv(S)));\n",
    "        # a posteriori estimate\n",
    "        s_est_a_posteriori = s_est_a_priori + np.dot(K,i);\n",
    "        P_est_a_posteriori = P_est_a_priori - np.dot(K,np.dot(C, P_est_a_priori));\n",
    "    else:\n",
    "        K = 0 # Kalman gain is null because the camera can't deliver any data\n",
    "        # a posteriori estimate\n",
    "        s_est_a_posteriori = s_est_a_priori;\n",
    "        P_est_a_posteriori = P_est_a_priori;\n",
    "     \n",
    "    return s_est_a_posteriori, P_est_a_posteriori\n",
    "\n",
    "#Check\n",
    "#kalman_filter(s_prev_est_a_posteriori, P_prev_est_a_posteriori)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77fa8690-7a74-46c7-982a-fafea5e9d2b8",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8860562a-bcc1-4e36-92c5-97ad46b854ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "await connect_Thymio()\n",
    "\n",
    "speed_test_value = randrange(200)      \n",
    "    \n",
    "node.send_set_variables(set_speeds(speed_test_value, speed_test_value))\n",
    "\n",
    "time.sleep(0.5)\n",
    "\n",
    "# Test\n",
    "\n",
    "# Initialization\n",
    "filter_initialization()\n",
    "\n",
    "# Initial state\n",
    "print(\"\\n\".join([f\"Initial state = [\", \"\\n\".join([\" , \".join([f\"{x: 10.2f}\" for x in row]) for row in s_prev_est_a_posteriori]), \"]\"]))\n",
    "\n",
    "for _ in range(3):\n",
    "    await get_speeds()         # Get the speeds\n",
    "    update_input()             # Update inputs\n",
    "    speeds_graph()             # Plot the graph of the speeds\n",
    "    s_prev_est_a_posteriori, P_prev_est_a_posteriori = kalman_filter(s_prev_est_a_posteriori, P_prev_est_a_posteriori)\n",
    "    print(\"\\n\".join([f\"Next state = [\", \"\\n\".join([\" , \".join([f\"{x: 10.2f}\" for x in row]) for row in s_prev_est_a_posteriori]), \"]\"]))\n",
    "\n",
    "node.send_set_variables(set_speeds(0, 0))\n",
    "\n",
    "disconnect_Thymio()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0405d350-7a58-41e8-a076-ddfddd07fe0a",
   "metadata": {},
   "source": [
    "What need to be done with the Thymio used for the final demo:\n",
    "- Evaluate the Q and R matrices\n",
    "- Find the speed in pixels per second : $v_{px/s} = v_{thymio} \\cdot thymio\\_speed\\_to\\_mms \\cdot mm\\_to\\_px$\n",
    "- Time in the B matrix: update_time or 1.5*((nb_samples)*update_time)? Depends on how we decide to get the speed... "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b37673-7537-4eb3-a42a-19581251fe49",
   "metadata": {},
   "source": [
    "## 5 Local Navigation\n",
    "<a id=\"local-navigation\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12420d31-c408-4a79-8bbf-645f9e0c86ac",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6af7c0a-5251-401b-9fdf-be2bca9561db",
   "metadata": {},
   "source": [
    "# Main\n",
    "<a id=\"main\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d991d3",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f818910-0668-4e72-8e89-818877c12d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "import asyncio\n",
    "import numpy as np\n",
    "from tdmclient import aw, ClientAsync"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c0fb00",
   "metadata": {},
   "source": [
    "## Basic functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "206346dc",
   "metadata": {},
   "source": [
    "### Kalman Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a844d2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_initialization():\n",
    "    \"\"\"\n",
    "    Initialize the various vectors and matrices requiered for filtering\n",
    "    \n",
    "    robot_vector: position (x and y) and orientation taken from the camera vision\n",
    "    \"\"\"\n",
    "    \n",
    "    global s_prev_est_a_posteriori, P_prev_est_a_posteriori, A, B, u, C, Q, R\n",
    "\n",
    "    ## Previous State A Posteriori Estimation Vector\n",
    "    # Vector representing the estimated state of the system at the previous time step\n",
    "    s_prev_est_a_posteriori = robot_vector\n",
    "    #print(\"\\n\".join([f\"s_prev_est_a_posteriori = [\", \"\\n\".join([\" , \".join([f\"{x: 10.2f}\" for x in row]) for row in s_prev_est_a_posteriori]), \"]\"]))\n",
    "    \n",
    "    ## Previous State A Posteriori Covariance Matrix\n",
    "    # Matrix representing the estimated precision of the previous estimated state\n",
    "    P_prev_est_a_posteriori = np.array([[1000, 0, 0], \n",
    "                                        [0, 1000, 0], \n",
    "                                        [0, 0, 1000]]) \n",
    "    #print(\"\\n\".join([f\"P_prev_est_a_posteriori = [\", \"\\n\".join([\" , \".join([f\"{x: 10.2f}\" for x in row]) for row in P_prev_est_a_posteriori]), \"]\"]))\n",
    "\n",
    "    ## State Matrix\n",
    "    # Matrix defining how the system evolves from one time step to the next\n",
    "    A = np.array([[1, 0, 0], \n",
    "                  [0, 1, 0], \n",
    "                  [0, 0, 1]])\n",
    "    #print(\"\\n\".join([f\"A = [\", \"\\n\".join([\" , \".join([f\"{x: 10.2f}\" for x in row]) for row in A]), \"]\"]))\n",
    "    \n",
    "    ## Input Matrix \n",
    "    # Matrix describing the impact of the input on the state\n",
    "    B = np.array([[1, 0], \n",
    "                  [0, 1], \n",
    "                  [0, 0]]); \n",
    "    #print(\"\\n\".join([f\"B = [\", \"\\n\".join([\" , \".join([f\"{x: 10.2f}\" for x in row]) for row in B]), \"]\"]))\n",
    "    \n",
    "    ## Input Vector\n",
    "    # Vector representing control inputs applied to the system \n",
    "    u = np.array([0, 0])\n",
    "    #print(\"\\n\".join([f\"u = [\", \"\\n\".join([\" , \".join([f\"{x: 10.2f}\" for x in row]) for row in u]), \"]\"]))\n",
    "\n",
    "    ## Output Matrix\n",
    "    # Matrix linking measurements to state\n",
    "    C = np.array([[1, 0, 0], \n",
    "                  [0, 1, 0], \n",
    "                  [0, 0, 1]])\n",
    "    #print(\"\\n\".join([f\"C = [\", \"\\n\".join([\" , \".join([f\"{x: 10.2f}\" for x in row]) for row in C]), \"]\"]))\n",
    "    \n",
    "    ## Process Noise Covariance Matrix\n",
    "    # Covariance matrix representing uncertainty in system dynamics\n",
    "    Q = np.array([[1, 0, 0], \n",
    "                  [0, 1, 0], \n",
    "                  [0, 0, 0.1]])\n",
    "    #print(\"\\n\".join([f\"Q = [\", \"\\n\".join([\" , \".join([f\"{x: 10.2f}\" for x in row]) for row in Q]), \"]\"]))\n",
    "\n",
    "    ## Measurement Noise Covariance Matrix\n",
    "    # Matrix representing uncertainty of camera measurements\n",
    "    R = np.array([[0.1, 0, 0], \n",
    "                  [0, 0.1, 0], \n",
    "                  [0, 0, 0.01]])\n",
    "    #print(\"\\n\".join([f\"R = [\", \"\\n\".join([\" , \".join([f\"{x: 10.2f}\" for x in row]) for row in R]), \"]\"]))\n",
    "\n",
    "# Check\n",
    "#filter_initialization()\n",
    "\n",
    "def update_input(v_l,v_r,update_time, direction_rotation):\n",
    "    \"\"\"\n",
    "    Update the input vector and matrix\n",
    "    \n",
    "    v_l: robot x position deduced from the camera vision\n",
    "    v_r: robot y position deduced from the camera vision\n",
    "    update_time: robot theta orientation deduced from the camera vision\n",
    "    \"\"\"\n",
    "    \n",
    "    global B,u\n",
    "    \n",
    "    Thymio_to_mms = 0.349\n",
    "    mm_to_px = 100/140\n",
    "    \n",
    "    # Average translational speed\n",
    "    v = (v_r +v_l)/2 # Thymio speed (T)\n",
    "    v = v * Thymio_to_mms * mm_to_px # Speed in px/s (T -> mm/s -> px/s)\n",
    "\n",
    "    # Average rotational speed\n",
    "    w = (v_r -v_l)*Thymio_to_mms/robot_diameter # Angular speed in rad/s\n",
    "    #print(\"angular speed :\",w)\n",
    "    \n",
    "    if (direction_rotation == TURN_RIGHT):\n",
    "        w = -w\n",
    "    \n",
    "    # Input vector\n",
    "    u = np.array([v, w]) \n",
    "    #print(\"\\n\".join([f\"u = [\", \"\\n\".join([\" , \".join([f\"{x: 10.2f}\" for x in row]) for row in u]), \"]\"]))\n",
    "    \n",
    "    # Angle variation\n",
    "    delta_theta = w * update_time\n",
    "    \n",
    "    # Input matrix\n",
    "    B = np.array([[np.cos(delta_theta + s_prev_est_a_posteriori[2])*update_time, 0],\n",
    "                  [-np.sin(delta_theta + s_prev_est_a_posteriori[2])*update_time, 0], \n",
    "                  [0, update_time]]); \n",
    "    #print(\"\\n\".join([f\"B = [\", \"\\n\".join([\" , \".join([f\"{x: 10.2f}\" for x in row]) for row in B]), \"]\"]))\n",
    "\n",
    "# Check\n",
    "#update_input()\n",
    "\n",
    "def kalman_filter(s_prev_est_a_posteriori, P_prev_est_a_posteriori):\n",
    "    \"\"\"\n",
    "    Estimates the current state using the input sensor data and the previous state\n",
    "    \n",
    "    param s_prev_est_a_posteriori: previous state a posteriori estimation\n",
    "    param P_prev_est_a_posteriori: previous state a posteriori covariance\n",
    "    \n",
    "    return s_est_a_posteriori: new a posteriori state estimation\n",
    "    return P_est_a_posteriori: new a posteriori state covariance\n",
    "    \"\"\"\n",
    "    \n",
    "    ## Prediciton through the a priori estimate\n",
    "    # estimated mean of the state\n",
    "    s_est_a_priori = np.dot(A, s_prev_est_a_posteriori)+ np.dot(B, u);\n",
    "    \n",
    "    # Estimated covariance of the state\n",
    "    P_est_a_priori = np.dot(A, np.dot(P_prev_est_a_posteriori, A.T)) + Q\n",
    "    \n",
    "    ## Update         \n",
    "    # m, C, and R for a posteriori estimate, depending on the detection of the camera\n",
    "    if camera_on == True:\n",
    "        m = [300,320,0]\n",
    "        # innovation / measurement residual\n",
    "        i = m - np.dot(C, s_est_a_priori);\n",
    "        # measurement prediction covariance\n",
    "        S = np.dot(C, np.dot(P_est_a_priori, C.T)) + R;     \n",
    "        # Kalman gain (tells how much the predictions should be corrected based on the measurements)\n",
    "        K = np.dot(P_est_a_priori, np.dot(C.T, np.linalg.inv(S)));\n",
    "        # a posteriori estimate\n",
    "        s_est_a_posteriori = s_est_a_priori + np.dot(K,i);\n",
    "        P_est_a_posteriori = P_est_a_priori - np.dot(K,np.dot(C, P_est_a_priori));\n",
    "    else:\n",
    "        K = 0 # Kalman gain is null because the camera can't deliver any data\n",
    "        # a posteriori estimate\n",
    "        s_est_a_posteriori = s_est_a_priori;\n",
    "        P_est_a_posteriori = P_est_a_priori;\n",
    "     \n",
    "    return s_est_a_posteriori, P_est_a_posteriori\n",
    "\n",
    "#Check\n",
    "#kalman_filter(s_prev_est_a_posteriori, P_prev_est_a_posteriori)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53160f25",
   "metadata": {},
   "source": [
    "### Connection/ disconnection functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44516198",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thymio connection\n",
    "async def connect_Thymio():\n",
    "    \"\"\"\n",
    "    Establish a connection with the Thymio if possible\n",
    "    \"\"\"\n",
    "    global node, client\n",
    "    try:\n",
    "        client = ClientAsync()\n",
    "        node = await asyncio.wait_for(client.wait_for_node(), timeout=2.0)\n",
    "        await node.lock()\n",
    "        print(\"Thymio connected\")\n",
    "\n",
    "    except asyncio.TimeoutError:\n",
    "        print(\"Thymio not connected: Timeout while waiting for node.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Thymio not connected: {str(e)}\")\n",
    "        \n",
    "# Thymio disconnection\n",
    "def disconnect_Thymio():\n",
    "    \"\"\"\n",
    "    Enable to disconnect the Thymio\n",
    "    \"\"\"\n",
    "    aw(node.stop())\n",
    "    aw(node.unlock())\n",
    "    print(\"Thymio disconnected\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba72462",
   "metadata": {},
   "source": [
    "### Motion functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "784c7d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thymio set motors speeds  \n",
    "async def set_speeds(left_speed, right_speed):\n",
    "    \"\"\"\n",
    "    Enable to set the speed of the Thymio's wheels\n",
    "    \"\"\"\n",
    "    global node\n",
    "    v = {\n",
    "        \"motor.left.target\":  [left_speed],\n",
    "        \"motor.right.target\": [right_speed],\n",
    "    }\n",
    "    await node.set_variables(v)\n",
    "    \n",
    "# Thymio motors stop     \n",
    "async def motors_stop():\n",
    "    \"\"\"\n",
    "    Stop the Thymio\n",
    "    \"\"\"\n",
    "    global node\n",
    "    v = {\n",
    "        \"motor.left.target\":  [0],\n",
    "        \"motor.right.target\": [0],\n",
    "    }\n",
    "    await node.set_variables(v)    \n",
    "    \n",
    "# Check\n",
    "#await connect_Thymio()\n",
    "#await set_speeds(40, -40)\n",
    "#time.sleep(2)\n",
    "#await motors_stop()\n",
    "#disconnect_Thymio()\n",
    "\n",
    "# conversion thymio speed to mm/s\n",
    "Thymio_to_mms = 0.349\n",
    "px_to_mm = 140/100\n",
    "#Thymio_to_pxs = Thymio_to_mms * mm_to_px \n",
    "\n",
    "ROTATION_SPEED = 100\n",
    "TIME_FULL_TURN = (8800/1000)\n",
    "\n",
    "# Thyimo turns a specied angle\n",
    "async def turn(angle):\n",
    "    # Calculate the time needed to turn through the required angle\n",
    "    rotation_time = (abs(angle) / (2*np.pi)) * TIME_FULL_TURN\n",
    "\n",
    "    # Turn robot on itself\n",
    "    # Check the sign of angle\n",
    "    if np.sign(angle) > 0:\n",
    "        # If angle is positive, turn left\n",
    "        await set_speeds(-ROTATION_SPEED, ROTATION_SPEED)\n",
    "        left_or_right = TURN_LEFT\n",
    "    else:\n",
    "        # If angle is negative, turn right\n",
    "        await set_speeds(ROTATION_SPEED, -ROTATION_SPEED)\n",
    "        left_or_right = TURN_RIGHT\n",
    "\n",
    "    # Wait required time\n",
    "    time.sleep(rotation_time)\n",
    "    return rotation_time, left_or_right\n",
    "\n",
    "# Check\n",
    "#gamma = np.pi/2\n",
    "#await connect_Thymio()\n",
    "#time_r = await turn(-gamma)\n",
    "#await motors_stop()\n",
    "#print(time_r)\n",
    "#disconnect_Thymio()\n",
    "\n",
    "# Constants\n",
    "FORWARD_SPEED = 200  # Base speed\n",
    "TIME_PER_MM = 15.5/1000  # Time it takes for the robot to travel one meter at base speed\n",
    "\n",
    "async def move_forward(distance_px):\n",
    "    # Calculate the time needed to travel the requested distance\n",
    "    \n",
    "    distance_mm = distance_px * px_to_mm\n",
    "    travel_time = (distance_mm) * TIME_PER_MM\n",
    "    \n",
    "    # Robot moves forward\n",
    "    await set_speeds(FORWARD_SPEED, FORWARD_SPEED)\n",
    "\n",
    "    # Wait for the necessary time\n",
    "    time.sleep(travel_time)\n",
    "    return travel_time\n",
    "\n",
    "# Check\n",
    "#await connect_Thymio()\n",
    "#distance = 28.28  # Distance to travel in pixels\n",
    "#time_f = await move_forward(distance)\n",
    "#await motors_stop()\n",
    "#print(time_f)\n",
    "#disconnect_Thymio()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d892e2be",
   "metadata": {},
   "source": [
    "### Motion control function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d120a22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def reach_next_node(next_node, mode, estimated_pos):\n",
    "\n",
    "    # Normal vector\n",
    "    vector_next_node = np.array([0,0])  \n",
    "    vector_next_node[0] = path_final[0][next_node] - estimated_pos[0]\n",
    "    vector_next_node[1] = path_final[1][next_node] - estimated_pos[1] \n",
    "    #print(\"vector next node: \", vector_next_node)\n",
    "    \n",
    "    # angle gamma\n",
    "    gamma = - math.atan2(vector_next_node[0], vector_next_node[1]) - estimated_pos[2]\n",
    "    #print(\"gamma\", next_node, \": \", gamma)\n",
    "    \n",
    "    # distance d\n",
    "    path_next_node = np.array([path_final[0][next_node], path_final[1][next_node]])\n",
    "    path_current_node = np.array([estimated_pos[0], estimated_pos[1]])\n",
    "    d = np.linalg.norm(path_next_node - path_current_node)\n",
    "    #print(\"d\", next_node ,\": \", d)\n",
    "\n",
    "    if(not mode):\n",
    "        if(abs(gamma) > ANGLE_THRESHOLD):\n",
    "            time_r, left_or_right = await turn(gamma)\n",
    "        else: \n",
    "            time_r = 0 \n",
    "            left_or_right = 1  \n",
    "        print(\"turn\", next_node)\n",
    "        return time_r, left_or_right \n",
    "        \n",
    "    if (mode):\n",
    "        if( d > FORWARD_THRESHOLD):\n",
    "            time_f = await move_forward(d)\n",
    "            print(\"forward\", next_node)\n",
    "            if(next_node==9):\n",
    "                print(\"d \",d)\n",
    "            return time_f"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a7480d",
   "metadata": {},
   "source": [
    "## Global variables and constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fbd5dd8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global variables\n",
    "\n",
    "#from camera\n",
    "robot_vector = np.array([140, 160, 0])\n",
    "\n",
    "#from global navigation\n",
    "path_final = np.array([[140, 160, 180, 200, 220, 240, 260, 280, 300, 300, 300, 320, 320, 340, 340, 360, 360, 360, 360, 360, 360, 380, 400, 420, 440, 440, 460, 460, 480, 480, 480, 500],\n",
    "                       [160, 180, 200, 220, 240, 260, 280, 300, 320, 340, 360, 380, 400, 420, 440, 460, 480, 500, 520, 540, 560, 580, 600, 620, 640, 660, 680, 700, 720, 740, 760, 780]])\n",
    "\n",
    "# general\n",
    "next_node = 1\n",
    "turn_left_or_right = 0\n",
    "\n",
    "#Filtering\n",
    "camera_on = False                # Boolean that determines whether the camera has vision or not\n",
    "robot_diameter = 94              # Distance between the two wheels\n",
    "\n",
    "# Constants\n",
    "ROTATION_MODE = 0\n",
    "FORWARD_MODE = 1\n",
    "ANGLE_THRESHOLD = 0.5 \n",
    "FORWARD_THRESHOLD = 1\n",
    "ROTATION_TIME_THRESHOLD = 1.2\n",
    "FORWARD_TIME_THRESHOLD = 100\n",
    "TURN_RIGHT=0\n",
    "TURN_LEFT=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d91aa720",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thymio connected\n",
      "[140 160   0]\n",
      "turn 1\n",
      "[140.         160.          -0.81680851]\n",
      "forward 1\n",
      "[160.94778141 182.30687208  -0.81680851]\n",
      "turn 2\n",
      "[160.94778141 182.30687208  -0.81680851]\n",
      "forward 2\n",
      "[180.20427669 202.81272752  -0.81680851]\n",
      "turn 3\n",
      "[180.20427669 202.81272752  -0.81680851]\n",
      "forward 3\n",
      "[199.62018356 223.48833714  -0.81680851]\n",
      "turn 4\n",
      "[199.62018356 223.48833714  -0.81680851]\n",
      "forward 4\n",
      "[219.04596055 244.17445727  -0.81680851]\n",
      "turn 5\n",
      "[219.04596055 244.17445727  -0.81680851]\n",
      "forward 5\n",
      "[238.49357513 264.8838318   -0.81680851]\n",
      "turn 6\n",
      "[238.49357513 264.8838318   -0.81680851]\n",
      "forward 6\n",
      "[257.96240603 285.61579917  -0.81680851]\n",
      "turn 7\n",
      "[257.96240603 285.61579917  -0.81680851]\n",
      "forward 7\n",
      "[277.45286145 306.37079406  -0.81680851]\n",
      "turn 8\n",
      "[277.45286145 306.37079406  -0.81680851]\n",
      "forward 8\n",
      "[296.9653672  327.1492699   -0.81680851]\n",
      "turn 9\n",
      "[ 2.99999702e+02  3.20000703e+02 -2.21730372e-06]\n",
      "forward 9\n",
      "d  19.99929709495004\n",
      "[ 3.21636941e+02  3.20000751e+02 -2.21730372e-06]\n",
      "turn 10\n",
      "[ 3.21636941e+02  3.20000751e+02 -2.21730372e-06]\n",
      "forward 10\n",
      "[ 3.70837789e+02  3.20000860e+02 -2.21730372e-06]\n",
      "turn 11\n",
      "[370.83778928 320.00085998   0.73113198]\n",
      "forward 11\n",
      "[434.17407569 263.19086178   0.73113198]\n",
      "Thymio disconnected\n",
      "Thymio ready for the game!\n"
     ]
    }
   ],
   "source": [
    "await connect_Thymio()\n",
    "\n",
    "filter_initialization()\n",
    "print(s_prev_est_a_posteriori)\n",
    "\n",
    "\n",
    "# 1\n",
    "\n",
    "# Rotation\n",
    "time_rotation, turn_left_or_right = await reach_next_node(next_node, ROTATION_MODE, s_prev_est_a_posteriori)\n",
    "update_input(-ROTATION_SPEED, ROTATION_SPEED, time_rotation, turn_left_or_right)\n",
    "s_prev_est_a_posteriori, P_prev_est_a_posteriori = kalman_filter(s_prev_est_a_posteriori, P_prev_est_a_posteriori)\n",
    "\n",
    "print(s_prev_est_a_posteriori)\n",
    "\n",
    "# Linear displacement\n",
    "forward_time = await reach_next_node(next_node, FORWARD_MODE , s_prev_est_a_posteriori)\n",
    "update_input(FORWARD_SPEED, FORWARD_SPEED, forward_time, 0)\n",
    "s_prev_est_a_posteriori, P_prev_est_a_posteriori = kalman_filter(s_prev_est_a_posteriori, P_prev_est_a_posteriori)\n",
    "print(s_prev_est_a_posteriori)\n",
    "\n",
    "next_node += 1\n",
    "\n",
    "# 2\n",
    "\n",
    "# Rotation\n",
    "time_rotation, turn_left_or_right = await reach_next_node(next_node, ROTATION_MODE, s_prev_est_a_posteriori)\n",
    "update_input(-ROTATION_SPEED, ROTATION_SPEED, time_rotation, turn_left_or_right)\n",
    "s_prev_est_a_posteriori, P_prev_est_a_posteriori = kalman_filter(s_prev_est_a_posteriori, P_prev_est_a_posteriori)\n",
    "\n",
    "print(s_prev_est_a_posteriori)\n",
    "\n",
    "# Linear displacement\n",
    "forward_time = await reach_next_node(next_node, FORWARD_MODE , s_prev_est_a_posteriori)\n",
    "update_input(FORWARD_SPEED, FORWARD_SPEED, forward_time, 0)\n",
    "s_prev_est_a_posteriori, P_prev_est_a_posteriori = kalman_filter(s_prev_est_a_posteriori, P_prev_est_a_posteriori)\n",
    "print(s_prev_est_a_posteriori)\n",
    "\n",
    "next_node += 1\n",
    "\n",
    "# 3\n",
    "\n",
    "# Rotation\n",
    "time_rotation, turn_left_or_right = await reach_next_node(next_node, ROTATION_MODE, s_prev_est_a_posteriori)\n",
    "update_input(-ROTATION_SPEED, ROTATION_SPEED, time_rotation, turn_left_or_right)\n",
    "s_prev_est_a_posteriori, P_prev_est_a_posteriori = kalman_filter(s_prev_est_a_posteriori, P_prev_est_a_posteriori)\n",
    "\n",
    "print(s_prev_est_a_posteriori)\n",
    "\n",
    "# Linear displacement\n",
    "forward_time = await reach_next_node(next_node, FORWARD_MODE , s_prev_est_a_posteriori)\n",
    "update_input(FORWARD_SPEED, FORWARD_SPEED, forward_time, 0)\n",
    "s_prev_est_a_posteriori, P_prev_est_a_posteriori = kalman_filter(s_prev_est_a_posteriori, P_prev_est_a_posteriori)\n",
    "print(s_prev_est_a_posteriori)\n",
    "\n",
    "next_node += 1\n",
    "\n",
    "# 4\n",
    "\n",
    "# Rotation\n",
    "time_rotation, turn_left_or_right = await reach_next_node(next_node, ROTATION_MODE, s_prev_est_a_posteriori)\n",
    "update_input(-ROTATION_SPEED, ROTATION_SPEED, time_rotation, turn_left_or_right)\n",
    "s_prev_est_a_posteriori, P_prev_est_a_posteriori = kalman_filter(s_prev_est_a_posteriori, P_prev_est_a_posteriori)\n",
    "\n",
    "print(s_prev_est_a_posteriori)\n",
    "\n",
    "# Linear displacement\n",
    "forward_time = await reach_next_node(next_node, FORWARD_MODE , s_prev_est_a_posteriori)\n",
    "update_input(FORWARD_SPEED, FORWARD_SPEED, forward_time, 0)\n",
    "s_prev_est_a_posteriori, P_prev_est_a_posteriori = kalman_filter(s_prev_est_a_posteriori, P_prev_est_a_posteriori)\n",
    "print(s_prev_est_a_posteriori)\n",
    "\n",
    "next_node += 1\n",
    "\n",
    "# 5\n",
    "\n",
    "# Rotation\n",
    "time_rotation, turn_left_or_right = await reach_next_node(next_node, ROTATION_MODE, s_prev_est_a_posteriori)\n",
    "update_input(-ROTATION_SPEED, ROTATION_SPEED, time_rotation, turn_left_or_right)\n",
    "s_prev_est_a_posteriori, P_prev_est_a_posteriori = kalman_filter(s_prev_est_a_posteriori, P_prev_est_a_posteriori)\n",
    "\n",
    "print(s_prev_est_a_posteriori)\n",
    "\n",
    "# Linear displacement\n",
    "forward_time = await reach_next_node(next_node, FORWARD_MODE , s_prev_est_a_posteriori)\n",
    "update_input(FORWARD_SPEED, FORWARD_SPEED, forward_time, 0)\n",
    "s_prev_est_a_posteriori, P_prev_est_a_posteriori = kalman_filter(s_prev_est_a_posteriori, P_prev_est_a_posteriori)\n",
    "print(s_prev_est_a_posteriori)\n",
    "\n",
    "next_node += 1\n",
    "\n",
    "# 6\n",
    "\n",
    "# Rotation\n",
    "time_rotation, turn_left_or_right = await reach_next_node(next_node, ROTATION_MODE, s_prev_est_a_posteriori)\n",
    "update_input(-ROTATION_SPEED, ROTATION_SPEED, time_rotation, turn_left_or_right)\n",
    "s_prev_est_a_posteriori, P_prev_est_a_posteriori = kalman_filter(s_prev_est_a_posteriori, P_prev_est_a_posteriori)\n",
    "\n",
    "print(s_prev_est_a_posteriori)\n",
    "\n",
    "# Linear displacement\n",
    "forward_time = await reach_next_node(next_node, FORWARD_MODE , s_prev_est_a_posteriori)\n",
    "update_input(FORWARD_SPEED, FORWARD_SPEED, forward_time, 0)\n",
    "s_prev_est_a_posteriori, P_prev_est_a_posteriori = kalman_filter(s_prev_est_a_posteriori, P_prev_est_a_posteriori)\n",
    "print(s_prev_est_a_posteriori)\n",
    "\n",
    "next_node += 1\n",
    "\n",
    "# 7\n",
    "\n",
    "# Rotation\n",
    "time_rotation, turn_left_or_right = await reach_next_node(next_node, ROTATION_MODE, s_prev_est_a_posteriori)\n",
    "update_input(-ROTATION_SPEED, ROTATION_SPEED, time_rotation, turn_left_or_right)\n",
    "s_prev_est_a_posteriori, P_prev_est_a_posteriori = kalman_filter(s_prev_est_a_posteriori, P_prev_est_a_posteriori)\n",
    "\n",
    "print(s_prev_est_a_posteriori)\n",
    "\n",
    "# Linear displacement\n",
    "forward_time = await reach_next_node(next_node, FORWARD_MODE , s_prev_est_a_posteriori)\n",
    "update_input(FORWARD_SPEED, FORWARD_SPEED, forward_time, 0)\n",
    "s_prev_est_a_posteriori, P_prev_est_a_posteriori = kalman_filter(s_prev_est_a_posteriori, P_prev_est_a_posteriori)\n",
    "print(s_prev_est_a_posteriori)\n",
    "\n",
    "next_node += 1\n",
    "\n",
    "# 8\n",
    "\n",
    "# Rotation\n",
    "time_rotation, turn_left_or_right = await reach_next_node(next_node, ROTATION_MODE, s_prev_est_a_posteriori)\n",
    "update_input(-ROTATION_SPEED, ROTATION_SPEED, time_rotation, turn_left_or_right)\n",
    "s_prev_est_a_posteriori, P_prev_est_a_posteriori = kalman_filter(s_prev_est_a_posteriori, P_prev_est_a_posteriori)\n",
    "print(s_prev_est_a_posteriori)\n",
    "\n",
    "# Linear displacement\n",
    "forward_time = await reach_next_node(next_node, FORWARD_MODE , s_prev_est_a_posteriori)\n",
    "update_input(FORWARD_SPEED, FORWARD_SPEED, forward_time, 0)\n",
    "s_prev_est_a_posteriori, P_prev_est_a_posteriori = kalman_filter(s_prev_est_a_posteriori, P_prev_est_a_posteriori)\n",
    "print(s_prev_est_a_posteriori)\n",
    "\n",
    "next_node += 1\n",
    "\n",
    "# 9\n",
    "\n",
    "camera_on = True\n",
    "\n",
    "# Rotation\n",
    "time_rotation, turn_left_or_right = await reach_next_node(next_node, ROTATION_MODE, s_prev_est_a_posteriori)\n",
    "update_input(-ROTATION_SPEED, ROTATION_SPEED, time_rotation, turn_left_or_right)\n",
    "s_prev_est_a_posteriori, P_prev_est_a_posteriori = kalman_filter(s_prev_est_a_posteriori, P_prev_est_a_posteriori)\n",
    "print(s_prev_est_a_posteriori)\n",
    "\n",
    "camera_on = False \n",
    "\n",
    "# Linear displacement\n",
    "forward_time = await reach_next_node(next_node, FORWARD_MODE , s_prev_est_a_posteriori)\n",
    "update_input(FORWARD_SPEED, FORWARD_SPEED, forward_time, 0)\n",
    "s_prev_est_a_posteriori, P_prev_est_a_posteriori = kalman_filter(s_prev_est_a_posteriori, P_prev_est_a_posteriori)\n",
    "print(s_prev_est_a_posteriori)\n",
    "\n",
    "next_node += 1\n",
    "\n",
    "# 10\n",
    "\n",
    "# Rotation\n",
    "time_rotation, turn_left_or_right = await reach_next_node(next_node, ROTATION_MODE, s_prev_est_a_posteriori)\n",
    "update_input(-ROTATION_SPEED, ROTATION_SPEED, time_rotation, turn_left_or_right)\n",
    "s_prev_est_a_posteriori, P_prev_est_a_posteriori = kalman_filter(s_prev_est_a_posteriori, P_prev_est_a_posteriori)\n",
    "print(s_prev_est_a_posteriori)\n",
    "\n",
    "# Linear displacement\n",
    "forward_time = await reach_next_node(next_node, FORWARD_MODE , s_prev_est_a_posteriori)\n",
    "update_input(FORWARD_SPEED, FORWARD_SPEED, forward_time, 0)\n",
    "s_prev_est_a_posteriori, P_prev_est_a_posteriori = kalman_filter(s_prev_est_a_posteriori, P_prev_est_a_posteriori)\n",
    "print(s_prev_est_a_posteriori)\n",
    "\n",
    "next_node += 1\n",
    "\n",
    "# 11\n",
    "\n",
    "# Rotation\n",
    "time_rotation, turn_left_or_right = await reach_next_node(next_node, ROTATION_MODE, s_prev_est_a_posteriori)\n",
    "update_input(-ROTATION_SPEED, ROTATION_SPEED, time_rotation, turn_left_or_right)\n",
    "s_prev_est_a_posteriori, P_prev_est_a_posteriori = kalman_filter(s_prev_est_a_posteriori, P_prev_est_a_posteriori)\n",
    "print(s_prev_est_a_posteriori)\n",
    "\n",
    "# Linear displacement\n",
    "forward_time = await reach_next_node(next_node, FORWARD_MODE , s_prev_est_a_posteriori)\n",
    "update_input(FORWARD_SPEED, FORWARD_SPEED, forward_time, 0)\n",
    "s_prev_est_a_posteriori, P_prev_est_a_posteriori = kalman_filter(s_prev_est_a_posteriori, P_prev_est_a_posteriori)\n",
    "print(s_prev_est_a_posteriori)\n",
    "\n",
    "next_node += 1\n",
    "\n",
    "await motors_stop()\n",
    "disconnect_Thymio()\n",
    "print(\"Thymio ready for the game!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b1ad3a0",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1894d574",
   "metadata": {},
   "outputs": [],
   "source": [
    "await connect_Thymio()\n",
    "\n",
    "filter_initialization()\n",
    "print(s_prev_est_a_posteriori)\n",
    "\n",
    "while True:\n",
    "    \n",
    "    # Rotation\n",
    "    time_rotation, turn_left_or_right = await reach_next_node(next_node, ROTATION_MODE, s_prev_est_a_posteriori)\n",
    "    update_input(-ROTATION_SPEED, ROTATION_SPEED, time_rotation, turn_left_or_right)\n",
    "    s_prev_est_a_posteriori, P_prev_est_a_posteriori = kalman_filter(s_prev_est_a_posteriori, P_prev_est_a_posteriori)\n",
    "    print(s_prev_est_a_posteriori)\n",
    "\n",
    "    # Linear displacement\n",
    "    forward_time = await reach_next_node(next_node, FORWARD_MODE , s_prev_est_a_posteriori)\n",
    "    update_input(FORWARD_SPEED, FORWARD_SPEED, forward_time, 1)\n",
    "    s_prev_est_a_posteriori, P_prev_est_a_posteriori = kalman_filter(s_prev_est_a_posteriori, P_prev_est_a_posteriori)\n",
    "    print(s_prev_est_a_posteriori)\n",
    "    \n",
    "    next_node += 1\n",
    "    \n",
    "    if (next_node == path_final.shape[1]):\n",
    "        break\n",
    "        \n",
    "await motors_stop()\n",
    "disconnect_Thymio()\n",
    "print(\"Thymio ready for the game!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a5caa1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5b83967b-ca11-47f2-bd4e-4084c88d5829",
   "metadata": {},
   "source": [
    "## 6 Conclusion\n",
    "<a id=\"conclusion\"></a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
