{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84588a6d-d18e-4ad6-8d06-e251d942fb7c",
   "metadata": {},
   "source": [
    "# Project Report - Groupe 28\n",
    "**Authors :** Celest Angela Tjong, Adrien Louis Baptiste Dupont, Luca Sidoti Pinto, Didier Henri Neuenschwander\n",
    "\n",
    "**Supervisors :** Prof. Francesco Mondada\n",
    "\n",
    "**Date** : 7/12/2023\n",
    "\n",
    "**[MICRO-452] : Basics of mobile robotics**\n",
    "\n",
    "---\n",
    "\n",
    "# Table of Contents\n",
    "* [1. Introduction](#introduction)\n",
    "* [2. Initialisation](#initialisation)\n",
    "* [3. Vision](#vision)\n",
    "    * [3.1 Preprocessing for Color Object Detection](#prepro)\n",
    "    * [3.2 Display Elements on the video](#display)\n",
    "\n",
    "* [4. Global Navigation](#global-navigation)\n",
    "    * [4.1 Path planning](#path-planning)\n",
    "    * [4.2 A* implementation](#A*)\n",
    "* [5. Filtering](#filtering)\n",
    "    * [5.1 State-space model](#state-space-model)\n",
    "    * [5.2 Kalman filter](#kalman-filter)\n",
    "* [6. Local Navigation](#local-navigation)\n",
    "* [7. Motion control](#motion-control)\n",
    "    * [7.1 Connection functions](#connection-functions)\n",
    "    * [7.2 Motion functions](#motion-functions)\n",
    "    * [7.3 Motion control function](#motion-control-function)\n",
    "* [8. Projet structure](#project-structure)   \n",
    "* [9. Runnable cells](#runnable-cells)\n",
    "    * [9.1 Functions declaration ](#functions-declaration)\n",
    "    * [9.2 Capture image](#capture-image)\n",
    "    * [9.3 Main](#main)\n",
    "* [10. Conclusion](#conclusion)\n",
    "\n",
    "---\n",
    "<a id=\"introduction\"></a>\n",
    "\n",
    "# Introduction\n",
    "\n",
    "This project, integrated into the MICRO-452 Basics of Mobile Robotics course, aims to combine various essential elements of mobile robotics: vision, global and local navigation, motion control, and filtering.\n",
    "\n",
    "Here is how the project was implemented. Initially, a webcam captures the image of the environment. The necessary information for operation, such as the robot’s position, the map, static obstacles, and the target’s position, are extracted in real-time using conventional image processing techniques. Subsequently, the A* algorithm is used to determine the optimal path. The robot follows this path, while the Kalman filter estimates the position during movement. Knowledge of the path and the robot’s position allows for control of its movement. If the Thymio detects an obstacle using its horizontal proximity sensors, local navigation takes over to avoid a collision. In addition, the robot can find its way back to the target in case of kidnapping. Finally, the Thymio is designed to reach the target even if the camera’s vision is obstructed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e8866c1",
   "metadata": {},
   "source": [
    "---\n",
    "<a id=\"initialisation\"></a>\n",
    "\n",
    "# Initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda90134",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import colors\n",
    "from PIL import Image\n",
    "import math\n",
    "import time\n",
    "from ipywidgets import interactive\n",
    "%matplotlib inline\n",
    "import asyncio\n",
    "from tdmclient import aw, ClientAsync\n",
    "import cv2\n",
    "\n",
    "\n",
    "\n",
    "reduction_coeff = 25\n",
    "ROTATION_COST = 1\n",
    "ROTATION_MODE = 0\n",
    "FORWARD_MODE = 1\n",
    "ANGLE_THRESHOLD = 0.0\n",
    "FORWARD_THRESHOLD = 1\n",
    "ROTATION_TIME_THRESHOLD = 1.2\n",
    "FORWARD_TIME_THRESHOLD = 100\n",
    "TURN_RIGHT=0\n",
    "TURN_LEFT=1\n",
    "ROTATION_SPEED = 100\n",
    "TIME_FULL_TURN = (8800/1000)\n",
    "GROUND_THRESHOLD = 500\n",
    "reduction_coeff = 25\n",
    "Thymio_to_mms = 0.349\n",
    "px_to_mm = 137/100\n",
    "pi = math.pi\n",
    "angle_turned = 0\n",
    "distance_forward = 1\n",
    "obstThrh = 20\n",
    "camera_on = False  \n",
    "next_node = 1\n",
    "calculate_global_path = False\n",
    "\n",
    "\n",
    "# Define color thresholds in HSV\n",
    "lower_red_bound = np.array([120, 100, 70])\n",
    "upper_red_bound = np.array([255, 255, 255])\n",
    "lower_green_bound = np.array([60, 50, 100])\n",
    "upper_green_bound = np.array([100, 255, 255])\n",
    "lower_yellow_bound = np.array([0, 50, 120])\n",
    "upper_yellow_bound = np.array([40, 105, 255])\n",
    "lower_black_bound = np.array([0, 0, 0])\n",
    "upper_black_bound = np.array([255, 255, 130])\n",
    "lower_blue_bound = np.array([90, 80, 0])\n",
    "upper_blue_bound = np.array([105, 255, 255])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f08bea99",
   "metadata": {},
   "source": [
    "---\n",
    "<a id=\"vision\"></a>\n",
    "# Vision\n",
    "\n",
    "\n",
    "This part of the project is primarily carried out using the tutorials available on `OpenCv` platform:\n",
    "\n",
    "https://opencv.org/\n",
    "\n",
    "Vision is highly dependent on the setup, so we have opted to maximize the contrast of the markers and colors used. The background was selected as white, featuring black obstacles, with markers in blue, green, and red.\n",
    "1. **Color Space Conversion**:\n",
    "   - In order to have consistent result for the detection, it is standard to convert     The function begins by converting the input image from the BGR color space (standard in OpenCV) to the HSV color space using `cv2.cvtColor`. HSV (Hue, Saturation, Value) is often more effective for color filtering. Indeed it is particulary usefull for image processing  because it separates color information (hue) from intensity or lighting (value). Thus it allows the recognition to be less dependant of the lighting condition, as it is possible to modify theses parameter. In order to fix the variables of the color used, it is a good practise to calibrate the calibrate the HSV in case of light changes. It has been done for all the colours used just by tuning all the parameters with sliders on a frame captured. <br>\n",
    "   \n",
    "The color calibration has been performed for all the colors used by adjusting the parameters with a sliders function on a captured frame. This approach ensures precise tuning of the HSV values to accommodate varying lighting conditions. The variables found are in the Initialisation section.\n",
    "\n",
    "<a id=\"prepro\"></a>\n",
    "## Preprocessing for Color Object Detection\n",
    "\n",
    "The first step in processing the image for locating the required colored object is to create a mask. This mask is created by specifying a range of colors in the HSV color space. Pixels within this color range are marked as 1 (or true), while all other pixels are marked as 0 (or false). It is created using `cv2.inRange`, which filters out all colors except those within the specified `lower_color_bound` and `upper_color_bound`. This step isolates the specified color.\n",
    "\n",
    "Then, the function `cv2.bitwise_and` extracts the area corresponding to the range of color of the image given as input. This is done by comparing each pixel of the image with the mask (which is the same size as the image, comparing with a logical \"&\").\n",
    "\n",
    "Finally, the color-filtered image is converted to grayscale using `cv2.cvtColor`, because the subsequent edge detection step (Canny) requires a single-channel image.\n",
    "\n",
    "### Canny Edge Detection Thresholds\n",
    "\n",
    "In order to detect the different colored forms, it is common to begin by using the Canny filter. The Canny filter, implemented as `cv2.Canny()` in OpenCV, is an edge detection operator that uses a multi-stage algorithm to detect a wide range of edges in images. It works by first applying Gaussian blurring to smooth the image and reduce noise. Then, it finds the intensity gradients of the image to highlight regions with strong gradient changes, which often indicate edges. The algorithm then tracks along these regions and suppresses any pixel that is not at the maximum of its neighborhood, which thins the edges. Finally, it applies hysteresis thresholding to distinguish between real edges and noise.\n",
    "\n",
    "- `threshold1` (Minimum Threshold): \n",
    "    - This is the lower bound for edge detection. \n",
    "    - Edges with intensity gradients below this value are not considered as true edges.\n",
    "    - Lowering this threshold will make the algorithm more sensitive, detecting more edges but potentially including noise or weaker edges.\n",
    "\n",
    "- `threshold2` (Maximum Threshold): \n",
    "    - This is the upper bound for edge detection.\n",
    "    - Edges with intensity gradients above this value are considered as strong edges.\n",
    "    - Increasing this threshold will result in fewer edges being detected, focusing only on the most pronounced edges.\n",
    "### Hough Transform \n",
    "\n",
    "After edge detection using the Canny filter, the next step often involves identifying shapes, such as circl (commonly used in this project)es, in the processed image. This is where the Hough Transform comes into play. The Hough Transform is a technique used to isolate features of a particular shape within an imageIt maps points in the image space to curves in a parameter space, accumulating votes in \"bins\" for potential shapes. Peaks in this space correspond to detected shapes like lines or circles in the original image.\n",
    ". In the context of circle detection, OpenCV provides the function `cv2.HoughCircles()`.\n",
    "\n",
    "**Working of `cv2.HoughCircles()`:**\n",
    "\n",
    "`cv2.HoughCircles()` is designed to identify circles in an image. It takes the following parameters as input:\n",
    "\n",
    "- `image`: The inputlly theof theput from Canny edge detection).\n",
    "- `method`: The method for detecting circles; typically, `cv2.HOUGH_GRADIENT` is used.\n",
    "- `dp`: The inverse ratio of the accumulator resolution to the image resolution.\n",
    "- `minDist`: The minimum distance between the centers of detected Additional parameters like `param1`, `param2`, `minRadius`, and `maxRadius` help in fine-tuning the detection process. `param1` epresents the higher threshold of the two passed to the Canny edge function. `param2` is the threshold for the center detection stage of the Hough Circle Transform.ion process.\n",
    "\n",
    "**Outputs of `cv2.HoughCircles()`:**\n",
    "\n",
    "- The function returns a set of vectors, each repres nting a circle. \n",
    "- Each vector contains the x and y coordinates of the center of the circle and its radius.\n",
    "\n",
    "\n",
    "In this project, circles have been primarily used, especially for the robot's information and for outlining the play area. They were chosen for their ease in identifying their central point, which allows for easy precise point detection. Furthermore, they are very useful for mapping the contours of the area, as they can be identified even when not completely visible, as demonstrated in the following images:\n",
    "\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "    <td> <img src=\"Images/yellow_raw.png\" alt=\"Image 1\" style=\"width: 300px;\"/> <br/> <center><b>Raw image</b></center> </td>\n",
    "    <td> <img src=\"Images/yellow_mask.png\" alt=\"Image 2\" style=\"width: 300px;\"/> <br/> <center><b>With yellow mask</b></center> </td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "The following function allows for the detection of circles in an image based on the requested color values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32ca27f-3812-4957-ba8b-73ae4c6c68b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_color_circle(image, lower_color_bound, upper_color_bound):\n",
    "    # Convert the input image from BGR to HSV color space for easier color detection\n",
    "    hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "    # Create a mask that isolates the pixels within the specified color range\n",
    "    mask = cv2.inRange(hsv, lower_color_bound, upper_color_bound)\n",
    "\n",
    "    # Apply the mask to the original image to keep only the colors of interest\n",
    "    color_only = cv2.bitwise_and(image, image, mask=mask)\n",
    "\n",
    "    # Convert the masked image to grayscale to prepare for circle detection\n",
    "    gray = cv2.cvtColor(color_only, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Apply Canny edge detection to the grayscale image to highlight edges\n",
    "    edges = cv2.Canny(gray, 100, 200)\n",
    "\n",
    "    # Use Hough Circle Transform to detect circles in the edged image\n",
    "    circles = cv2.HoughCircles(edges, cv2.HOUGH_GRADIENT, 1, 20,\n",
    "                               param1=20, param2=15, minRadius=10, maxRadius=50)\n",
    "    \n",
    "    # Check if any circles were detected\n",
    "    if circles is not None:\n",
    "        # Convert circles' dimensions from float to integers\n",
    "        circles = np.uint16(np.around(circles[0, :]))\n",
    "\n",
    "        # Return a list of tuples, each representing a circle with (x, y) center coordinates and radius\n",
    "        return [(circle[0], circle[1], circle[2]) for circle in circles]\n",
    "    else:\n",
    "        # Return an empty list if no circles are detected\n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d09721cb",
   "metadata": {},
   "source": [
    "The function that crops the image is as follows: it detects the four edge circles and connects them, remembering the coordinates to subsequently crop all following images (video stream)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3675f92-3883-46ae-9b15-f1df15e42525",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_roi_from_circles(image, circles):\n",
    "    if circles is not None and len(circles) >= 4:\n",
    "\n",
    "        points = np.array([circle[:2] for circle in circles], dtype=np.float32)\n",
    "\n",
    "        rect = cv2.boundingRect(points)\n",
    "\n",
    "         # Cropping the image within the defined boundaries\n",
    "        x, y, w, h = rect\n",
    "        cropped_image = image[y:y+h, x:x+w]\n",
    "        return cropped_image,(x, y, w, h)\n",
    "    else:\n",
    "        print(\"Edge circles not detected correctly\")\n",
    "        return image,None "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abed0c71",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <img src=\"Images/blue_circle.png\" alt=\"Image 1\" width=\"300\"/>\n",
    "    <br>\n",
    "    <b>Blue circle placed on each edge</b>\n",
    "</center>\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "    <td> <img src=\"Images/initial_blue_without_obstacle.png\" alt=\"Image 1\" style=\"width: 300px;\"/> <br/> <center><b>with blues circles placed on each edge</b></center> </td>\n",
    "    <td> <img src=\"Images/avec_bords_sans_bleu.png\" alt=\"Image 2\" style=\"width: 300px;\"/> <br/> <center><b>without blues circles placed on each edge</b></center> </td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "<a id=\"display\"></a>\n",
    "## Display Elements on the video\n",
    "\n",
    "In this project, it's important to distinguish between static and dynamic elements that need to be displayed in the video stream.\n",
    "\n",
    "**Static Elements:**\n",
    "These are elements that do not require reprocessing on each frame of the video. They are essential to the global path finding, it includes:\n",
    "- **Map Contours:** The outlines of the navigable area.\n",
    "- **Initial Robot Location:** The starting position and orientation (initial location and angles) of the robot.\n",
    "- **Obstacles:** Fixed objects that the robot must navigate around or avoid.\n",
    "\n",
    "**Dynamic Elements:**\n",
    "These relate to the robot's motion. It includes:\n",
    "- **Robot Variables in Motion:** The current position and orientation (angle) of the robot during its movement.\n",
    "\n",
    "By keeping track of these static and dynamic elements, we can effectively manage the robot's navigation and motion within its environment.\n",
    "\n",
    "For static objects, obstacles are calculated using the same process explained in the introduction (for circles) but this time with the function cv2.findContours (take for input the `image`, `cv2.RETR_TREE`that Retrieves all contours and constructs a full hierarchy, useful for understanding the relationships between contours in an image. And `cv2.CHAIN_APPROX_SIMPLE` that Compresses contours by keeping only essential points, reducing data and simplifying the contours). \n",
    "\n",
    " Moreover, it is important to set a threshold for obstacle detection because even the slightest shadow or dust can otherwise be interpreted as an obstacle. Additionally, it is crucial to dilate the contours since the robot vector is a point without area. These two empirical thresholds (dilation and detection threshold) have been manually tuned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f2f8008",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_obstacle_contours(image, area_threshold, kernel_size):\n",
    "    \"\"\"\n",
    "    Detects and dilates obstacle contours in the given image.\n",
    "    :param image: Input image.\n",
    "    :param area_threshold: Area threshold for filtering contours.\n",
    "    :param kernel_size: Size of the kernel used for dilation.\n",
    "    :return: Image with obstacle contours drawn.\n",
    "    \"\"\"\n",
    "    hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "    mask_black = cv2.inRange(hsv, lower_black_bound, upper_black_bound)\n",
    "    contours, _ = cv2.findContours(mask_black, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "     # Filter out contours that are smaller than the specified area threshold\n",
    "    filtered_contours = [cnt for cnt in contours if cv2.contourArea(cnt) > area_threshold]\n",
    "    # dilation kernel of the specified size (how the size of the Thymio is taken into account)\n",
    "    kernel = np.ones((kernel_size, kernel_size), np.uint8)\n",
    "    # Dilate the mask to make the contours more pronounced\n",
    "    mask_dilated = cv2.dilate(mask_black, kernel, iterations=1)\n",
    "    # Find contours \n",
    "    dilated_contours, _ = cv2.findContours(mask_dilated, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    # Filter out dilated contours that are smaller than the increased area threshold\n",
    "    filtered_dilated_contours = [cnt for cnt in dilated_contours if cv2.contourArea(cnt) > area_threshold+10000]\n",
    "    contour_image = image.copy()\n",
    "    # Draw the original contours in green and the dilated contours in red:\n",
    "    cv2.drawContours(contour_image, filtered_contours, -1, (0, 255, 0), 2)\n",
    "    cv2.drawContours(contour_image, filtered_dilated_contours, -1, (0, 0, 255), 2)\n",
    "    return contour_image, filtered_contours, filtered_dilated_contours\n",
    "    \n",
    "#--------------------------------------------------------------------------------#\n",
    "    \n",
    "def create_obstacle_matrix(image, dilated_contours):\n",
    "    # Get the height and width of the input image\n",
    "    height, width = image.shape[:2]\n",
    "    # Create a matrix (of the same size as the image) filled with ones\n",
    "    # Ones indicate non-obstacle areas initially\n",
    "    obstacle_matrix = np.ones((height, width), dtype=np.uint8)\n",
    "    for contour in dilated_contours:\n",
    "        cv2.fillPoly(obstacle_matrix, [contour], 0)\n",
    "\n",
    "    # Return the obstacle matrix where obstacles are marked as zeros (for the global path planning)\n",
    "    return obstacle_matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd0df3fc",
   "metadata": {},
   "source": [
    "The image below shows the margin given by the dilation of the obstacles:\n",
    "\n",
    "<center>\n",
    "    <img src=\"Images/edge_ limit_initial.png\" alt=\"Image 1\" width=\"500\"/>\n",
    "    <br>\n",
    "    <b>Original contours in green and the dilated contours in red for safety</b>\n",
    "</center>\n",
    "\n",
    "In a similar manner, the goal, identified as a blue rectangle, is detected using the function below. The target assigned to the path planning is the rectangle's central point. To ensure that this rectangle is not misinterpreted as an obstacle, it is advisable to create a white mask and overlay it during the obstacle processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f660693",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_rectangle_center(image, lower_colour_bound, upper_colour_bound, area_threshold=1000):\n",
    "    # convert HSV\n",
    "    hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "    # colour mask \n",
    "    mask = cv2.inRange(hsv, lower_colour_bound, upper_colour_bound)\n",
    "\n",
    "    ## Loop through each contour to identify the one matching the rectangle\n",
    "    contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    # Check if the contour area is larger than the specified threshold\n",
    "    for contour in contours:\n",
    "        if cv2.contourArea(contour) > area_threshold:\n",
    "            # Calculate the bounding box for the contour\n",
    "            x, y, w, h = cv2.boundingRect(contour)\n",
    "\n",
    "            # Erase the rectangle by drawing it in white\n",
    "            cv2.rectangle(image, (x, y), (x + w, y + h), (255, 255, 255), -1)\n",
    "\n",
    "            # Calculate the center of the rectangle\n",
    "            center = (x + w // 2, y + h // 2)\n",
    "            return center\n",
    "    # Return None if no suitable rectangle is found\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa69349c",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <img src=\"Images/goal.png\" alt=\"Image 1\" width=\"500\"/>\n",
    "    <br>\n",
    "    <b>Goal target marked in yellow with a white mask applied</b>\n",
    "</center>\n",
    "\n",
    "The following function combines all the processes that need to be done before the robot moves forward. A white circle is applied to the image at the robot's location so that it is not considered an obstacle (similar to the arrival rectangle)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "798bf4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(video_capture, lower_goal_colour_bound, upper_goal_colour_bound):\n",
    "    ret, initial_frame = video_capture.read()\n",
    "    if ret:\n",
    "        #initial_frame, cropping_coords = crop_largest_white_area(initial_frame, 200000)\n",
    "        blue_circles = detect_color_circle(initial_frame, lower_goal_colour_bound, upper_goal_colour_bound)\n",
    "        # Crop the region of interest (ROI) from the circles detected\n",
    "        initial_frame ,cropping_coords= crop_roi_from_circles(initial_frame, blue_circles)\n",
    "        # Find the center of the blue rectangle (the goal area)\n",
    "        goal_center = find_rectangle_center(initial_frame, lower_blue_bound, upper_blue_bound,2000)\n",
    "        if goal_center:\n",
    "            # Store the coordinates of the detected yellow circle\n",
    "            yellow_circle_coords = goal_center \n",
    "            radius = 10  \n",
    "            color = (0, 255, 255)  \n",
    "            cv2.circle(initial_frame, yellow_circle_coords, radius, color, 3)\n",
    "        # Get the robot's current position and orientation vector\n",
    "        robot_vector = robot_info(initial_frame)\n",
    "        if robot_vector[0] and robot_vector[1]:\n",
    "            cv2.circle(initial_frame, (robot_vector[0], robot_vector[1]),80, (255, 255, 255), -1)\n",
    "        # Detect obstacle contours in the frame\n",
    "        contour_image = detect_obstacle_contours(initial_frame, 2000, 80)\n",
    "        # Create an obstacle matrix based on the detected contours\n",
    "        global_obstacle = create_obstacle_matrix(initial_frame,contour_image[2])\n",
    "         # Flip and display the initial frame with contours for visual inspection\n",
    "        initial_frame = np.flipud(initial_frame)\n",
    "        plt.imshow(cv2.cvtColor(contour_image[0], cv2.COLOR_BGR2RGB))\n",
    "        plt.title('Initial Contours')\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "        # Return the processed frame, cropping coordinates, contour image, global obstacle matrix, goal center, and robot vector\n",
    "        return initial_frame, cropping_coords, contour_image, global_obstacle,goal_center, robot_vector\n",
    "    else:\n",
    "        # Return None for all outputs if the frame wasn't captured\n",
    "        return None, None, None, None, None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "284cca8d",
   "metadata": {},
   "source": [
    "Now, it is possible to see all the preprocessing done before (which is the input for the global path planning). This includes a yellow circle representing the goal, the robot and the contours of the obstacles (white masks are left for visual purposes) .\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "    <td> <img src=\"Images/output_initial.png\" alt=\"Image 1\" style=\"width: 400px;\"/> <br/> <center><b>Preprocess frame (the white circle is the robot initial location)</b></center> </td>\n",
    "    <td> <img src=\"Images/initial2.png\" alt=\"Image 2\" style=\"width: 400px;\"/> <br/> <center><b>Preprocess frame for another initial condition</b></center> </td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "On the other hand, elements related to the robot's localization must be determined in each frame. To gather information about its orientation and location, three circles are placed on the top of the robot. A green circle is located at the middle front, and two red circles are positioned on each side, with their centers aligned with the robot's center of rotation (between the two motors). This arrangement facilitates the extraction of necessary information. By connecting the centers of the two red circles and creating a midpoint, a vector can then be formed by connecting the center of the green circle to this midpoint computed. \n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "    <td> <img src=\"Images/thymio_vector.png\" alt=\"Image 1\" style=\"width: 400px;\"/> <br/> <center><b>Thymio, marked with its midpoint and vector</b></center> </td>\n",
    "    <td> <img src=\"Images/thymio_vector2.png\" alt=\"Image 2\" style=\"width: 300px;\"/> <br/> <center><b>Circles on the thymio</b></center> </td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "The information related to the robot is generated and displayed using the following function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa231db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def robot_info(frame):\n",
    "    global camera_on\n",
    "    red_circles = detect_color_circle(frame, lower_red_bound, upper_red_bound)\n",
    "    green_circles = detect_color_circle(frame, lower_green_bound, upper_green_bound)\n",
    "    \n",
    "    if red_circles and green_circles and len(red_circles) >= 2:\n",
    "        # Calculate robot's position and orientation vector\n",
    "        # Calculate the midpoint between the centers of the red circles\n",
    "        midpoint = ((red_circles[0][0] + red_circles[1][0]) // 2,\n",
    "        (red_circles[0][1] + red_circles[1][1]) // 2)\n",
    "        # Calculate the directional vector\n",
    "        direction = np.array([midpoint[0] - green_circles[0][0], midpoint[1] - green_circles[0][1]])\n",
    "\n",
    "        # Normalize and extend the vector\n",
    "        length = 30  # Additional length\n",
    "        direction = direction / np.linalg.norm(direction) * length\n",
    "\n",
    "        # Calculate the new endpoint\n",
    "        new_endpoint = (int(green_circles[0][0] + direction[0]), int(green_circles[0][1] + direction[1]))\n",
    "\n",
    "        # Draw the extended arrow\n",
    "        cv2.arrowedLine(frame, new_endpoint, green_circles[0][:2], (0, 0, 0), 3)\n",
    "\n",
    "        # Calculate the angle of orientation with respect to the x-axis\n",
    "        dx =   midpoint[0] - green_circles[0][0]\n",
    "        dy =   midpoint[1] - green_circles[0][1]\n",
    "        angle = math.atan2(dy, dx)\n",
    "        angle_degrees = math.degrees(angle)\n",
    "        if(angle_degrees>= 0):\n",
    "            angle_degrees =180 - angle_degrees\n",
    "        elif (angle_degrees < 0):\n",
    "            angle_degrees = -(180 + angle_degrees)\n",
    "        # Store the robot's orientation vector\n",
    "        robot_vector = (midpoint[0], midpoint[1], np.radians(angle_degrees)) \n",
    "        # Display the information on the frame\n",
    "        cv2.putText(frame, f'Angle: {angle_degrees:.2f} degrees', (10, 30), \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n",
    "\n",
    "        cv2.putText(frame, f'Midpoint: ({midpoint[0]}, {midpoint[1]})', (10, 70),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n",
    "\n",
    "        cv2.circle(frame, midpoint,10, (255, 0, 0), -1)\n",
    "        camera_on = True\n",
    "        return midpoint[0], midpoint[1], angle_degrees\n",
    "\n",
    "    else:\n",
    "        camera_on = False\n",
    "        return None,None,None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb971a90",
   "metadata": {},
   "source": [
    "The function below is called for each frame of the video, it invokes the function that retrieves the position and angles of the robot and displays the static obstacles.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e1c7a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_image(frame, cropping_coords, contour_image, update_rate):\n",
    "    if cropping_coords is not None:\n",
    "        x, y, w, h = cropping_coords\n",
    "        frame = frame[y:y+h, x:x+w]\n",
    "    # Detect red and green circles\n",
    "    robot_vector = robot_info(frame)\n",
    "    if  robot_vector is not None:\n",
    "        cv2.drawContours(frame, contour_image[1], -1, (0, 255, 0), 2)\n",
    "        cv2.drawContours(frame, contour_image[2], -1, (0, 0, 255), 2)\n",
    "\n",
    "        # Display the resulting frame\n",
    "        cv2.imshow('Robot Detection', frame)\n",
    "    return robot_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d16de34e",
   "metadata": {},
   "source": [
    "\n",
    "In the context of our robotics project, we encountered a specific challenge related to detecting our robot's position. When the robot was perfectly positioned on a specific target, a sheet placed on the ground, it did not report the expected coordinates. We identified that this problem was due to parallax caused by the thickness of the robot. Due to its viewing angle, the aerial camera distorted the real position of the robot. To solve this problem, we projected the robot's position onto a 2D plane and used correction functions to adjust the coordinates. This allowed us to read the robot's position more accurately : \n",
    "\n",
    "For the x axis, we computated an equation fitting our measures : \n",
    "\n",
    "$$ f_{corr,x}(x) = 0.96x + 16.67$$\n",
    "\n",
    "![interactions](Images/graphe_1.jpg)\n",
    "\n",
    "For the y axis : \n",
    "\n",
    "$$ f_{corr,y}(x) = 0.95x + 14.29$$\n",
    "\n",
    "![interactions](Images/graphe_2.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df600486",
   "metadata": {},
   "outputs": [],
   "source": [
    "def x_robot_projection_to_ground(x_pos_robot):\n",
    "    return round(0.96*x_pos_robot+16.67)\n",
    "    \n",
    "def y_robot_projection_to_ground(y_pos_robot):\n",
    "    return round(0.95*y_pos_robot+14.29)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7805995a-c458-4480-903c-cd51b8dc35ea",
   "metadata": {},
   "source": [
    "Here is the code that operates the vision component independently: it first displays an image with all static elements, then streams video at 10Hz from the camera. This approach is effective when the robot is programmed to move in a straight line, as it allows observation of the moving vector and all static elements being displayed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be888b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize video capture from the first camera device\n",
    "video_capture = cv2.VideoCapture(0, cv2.CAP_DSHOW)\n",
    "# Initialize video capture from the first camera device\n",
    "# Set video frame width and height\n",
    "video_capture.set(cv2.CAP_PROP_FRAME_WIDTH, 1080)\n",
    "video_capture.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)\n",
    "\n",
    "# Create a window for display\n",
    "window_name = 'Robot Detection'\n",
    "cv2.namedWindow(window_name, cv2.WINDOW_NORMAL)\n",
    "\n",
    "# Resize the window (width, height)\n",
    "cv2.resizeWindow(window_name, 540, 360)  \n",
    "\n",
    "# Perform initial detection of obstacles and goal\n",
    "# This function will need the video_capture object and color bounds as inputs\n",
    "initial_frame, cropping_coords, contour_image, global_obstacle,goal_center, robot_pose = preprocess_image(video_capture, lower_blue_bound, upper_blue_bound)\n",
    "# Robot update frequency (10 Hz)\n",
    "update_rate = 0.01  # 10 times per second\n",
    "# Check if initial frame is not None\n",
    "if initial_frame is not None:\n",
    "    \n",
    "    try:\n",
    "        while True:\n",
    "            # Record start time of the loop\n",
    "            start_time = time.time()\n",
    "            # Read a new frame from the video capture\n",
    "            ret, frame = video_capture.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            # Process each frame for robot position and other tasks\n",
    "            robot_pose = process_image(frame, cropping_coords, contour_image, update_rate)\n",
    "            \n",
    "            # The coordinates of the robot are then projected to have the true coordinates\n",
    "            robot_vector = np.zeros(3)\n",
    "            robot_vector[0] = int(x_robot_projection_to_ground(robot_pose[0]))\n",
    "            robot_vector[1] = int(y_robot_projection_to_ground(robot_pose[1]))\n",
    "            robot_vector[2] = np.radians(robot_pose[2])\n",
    "            # Wait for a short period to maintain the update frequency and check for 'q' key press to quit\n",
    "            time_to_wait = max(int((start_time + update_rate - time.time()) * 1000), 1)\n",
    "            if cv2.waitKey(time_to_wait) & 0xFF == ord('q'):\n",
    "                break\n",
    "\n",
    "    except Exception as e:\n",
    "        # Print any errors that occur\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "    finally:\n",
    "        # Release the video capture and close all OpenCV windows when done\n",
    "        video_capture.release()\n",
    "        cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdad22f3",
   "metadata": {},
   "source": [
    "---\n",
    "<a id=\"global-navigation\"></a>\n",
    "# Global navigation\n",
    "\n",
    "<a id=\"path-planning\"></a>\n",
    "## Path planning\n",
    "\n",
    "The path search is transformed into a graph search. A problem encountered is the computational time needed for a grid the size of the image. To reduce this duration a size reduction coefficient is used to downsize the original grid. The value of this coefficient is chosen to have the best trade-off between computational time and precision.\n",
    "\n",
    "| `reduction_coeff` | Computation time |\n",
    "|----------|----------|\n",
    "| 10 | 9.599 |\n",
    "| 15 | 2.298 |\n",
    "| 20 | 0.513 |\n",
    "| 25 | 0.193 |\n",
    "| 30 | 0.099 |\n",
    "\n",
    "The solution with the best trade-off is a reduction factor of 25, allowing an average computational time of 0.193s while keeping acceptable precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a37cf6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def  create_downsized_grid(global_obstacle):\n",
    "    \"\"\" \n",
    "    Creation of the downsized grid occupation_grid with a scale of reduction_coeff\n",
    "    \n",
    "    param : \n",
    "        - global_obstacle : grid containing the obstacle positions with original size\n",
    "    return : \n",
    "        - occupancy_grid: Downsized grid with averaged obstacle positions\n",
    "    \"\"\"\n",
    "\n",
    "    # Invert the data to have a value of 0 for free cells and 1 for occupied cells.\n",
    "    global_obstacle = np.logical_not( global_obstacle )  \n",
    "    max_val_x_init = global_obstacle.shape[0]\n",
    "    max_val_y_init = global_obstacle.shape[1]\n",
    "\n",
    "    # The size of the new grid is calculated from the size of the original grid global_obstacle\n",
    "    max_val_x = int(max_val_x_init / reduction_coeff)\n",
    "    max_val_y = int(max_val_y_init / reduction_coeff)\n",
    "\n",
    "    # Creation of the downsized grid with\n",
    "    occupancy_grid = np.zeros((max_val_x, max_val_y), dtype=int)\n",
    "\n",
    "    # Inspection of squares (of size reduction_coeff*reduction_coeff) in the original grid\n",
    "    for i in range (max_val_x):\n",
    "        for j in range (max_val_y):\n",
    "            sum_pixels = 0\n",
    "\n",
    "            for k in range (reduction_coeff):\n",
    "\n",
    "                # Verification that the indexs indice_x and indice_y are in the grid range\n",
    "                indice_x = int(i * reduction_coeff - reduction_coeff/2 + k)\n",
    "                if (indice_x < 0):\n",
    "                    indice_x = 0\n",
    "                elif (indice_x > (max_val_x_init - 1)):\n",
    "                    indice_x = max_val_x_init -1\n",
    "\n",
    "                indice_y = int(j * reduction_coeff - reduction_coeff/2 + k)\n",
    "                if (indice_y < 0):\n",
    "\n",
    "                    indice_y = 0\n",
    "                elif (indice_y > (max_val_y_init - 1)):\n",
    "                    indice_y = max_val_y_init -1\n",
    "\n",
    "                # The sum of all values present in the inspected square is stored in sum_pixels\n",
    "                sum_pixels = sum_pixels + global_obstacle[indice_x][indice_y]\n",
    "\n",
    "\n",
    "            # If an obstacle is found in the square, a 1 is placed in the corresponding position in the new grid\n",
    "            if sum_pixels == 0:\n",
    "                occupancy_grid[i][j] = 0\n",
    "            else:\n",
    "                occupancy_grid[i][j] = 1\n",
    "\n",
    "    return occupancy_grid\n",
    "\n",
    "#--------------------------------------------------------------------------------#\n",
    "\n",
    "def create_empty_plot(max_val_x, max_val_y):\n",
    "    \"\"\"\n",
    "    Creation of a figure to plot a matrix of max size max_val \n",
    "    \n",
    "    param : \n",
    "        - max_val_x : maximum dimension of the matrix on x axis\n",
    "        - max_val_y : maximum dimension of the matrix on y axis\n",
    "    return:\n",
    "        -  fig : general figure\n",
    "        -  ax : axis information\n",
    "    \"\"\"\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(7,7))\n",
    "    \n",
    "    major_ticks = np.arange(0, max_val_y+1, 5)\n",
    "    minor_ticks = np.arange(0, max_val_y+1, 1)\n",
    "    ax.set_xticks(major_ticks)\n",
    "    ax.set_xticks(minor_ticks, minor=True)\n",
    "    ax.set_yticks(major_ticks)\n",
    "    ax.set_yticks(minor_ticks, minor=True)\n",
    "    ax.grid(which='minor', alpha=0.2)\n",
    "    ax.grid(which='major', alpha=0.5)\n",
    "    \n",
    "    # Definition of axis limits with the information of the matrix size\n",
    "    ax.set_ylim([max_val_x,-1])\n",
    "    ax.set_xlim([-1,max_val_y])\n",
    "    ax.grid(True)\n",
    "    \n",
    "    return fig, ax\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed6a06a2",
   "metadata": {},
   "source": [
    "<a id=\"A*\"></a>\n",
    "## A* implementation\n",
    "\n",
    "The path planning for the Thymio is made using a hybrid A* algorithm. This ensures an optimised search by using the distance to goal to choose the nodes to explore. The formula used is the following : \n",
    "\n",
    "$$ f(n) = g(n) + h(n) $$\n",
    "\n",
    "Here f(n) is the function we try to optimise, with g(n) being the motion cost and h(n) the heuristic function. The motion cost has here been tailored to the Thymio's mechanism, as it takes into account on one side the cost of forward motion with `deltacost` and on the other side the rotational cost with `rotational_cost`. Adding the rotational cost is key to avoid repetitive direction changes.\n",
    "\n",
    "The `path_final` matrix is structured such that the y-coordinates are on the first row and the x-coordinates on the second. Consequently, an inversion of these two rows is required prior to executing the robot's movement.\n",
    "\n",
    "After having computated the final path, a similar grid is displayed :\n",
    "\n",
    "![path](Images/graph_path.jpg)\n",
    "\n",
    "\n",
    "Moreover, the downsizing operation of the original grid introduces inaccuracies regarding the initial and final position of the robot. To mitigate this limitation, the exact coordinates of these two specific positions are preferred over the values returned by the A* algorithm when constructing the movement array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23828a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_movements_8n():\n",
    "    \"\"\"\n",
    "    Get all possible 8-connectivity movements : up, down, left, right and the 4 diagonals.\n",
    "    \n",
    "    return : \n",
    "        - list of movements with cost [(dx, dy, movement_cost)]\n",
    "    \"\"\"\n",
    "    s2 = math.sqrt(2)\n",
    "    return [(1, 0, 1.0),\n",
    "            (0, 1, 1.0),\n",
    "            (-1, 0, 1.0),\n",
    "            (0, -1, 1.0),\n",
    "            (1, 1, s2),\n",
    "            (-1, 1, s2),\n",
    "            (-1, -1, s2),\n",
    "            (1, -1, s2)]\n",
    "\n",
    "#--------------------------------------------------------------------------------#\n",
    "\n",
    "def reconstruct_path(cameFrom, current):\n",
    "    \"\"\"\n",
    "    Reconstructs the path from start node to the current node\n",
    "    \n",
    "    param :\n",
    "        - cameFrom: map (dict) containing for each node the node immediately \n",
    "                     preceding it on the cheapest path currently known.\n",
    "        - param current : current node (x, y)\n",
    "        \n",
    "    return : \n",
    "        - total_path : list of nodes from start to current node\n",
    "    \"\"\"\n",
    "    \n",
    "    # The first item is the current node\n",
    "    total_path = [current]\n",
    "    \n",
    "    while current in cameFrom.keys():\n",
    "        # Add the previous node to the start of the list\n",
    "        total_path.insert(0, cameFrom[current]) \n",
    "        current = cameFrom[current]\n",
    "    return total_path\n",
    "\n",
    "#--------------------------------------------------------------------------------#\n",
    "\n",
    "def A_Star(start, goal, h, coords, occupancy_grid):\n",
    "    \"\"\"\n",
    "    Implementation of the A* algorithm, finding the cheapest path from start to goal \n",
    "    \n",
    "    param :\n",
    "        - start : start node (x, y)\n",
    "        - goal: goal node (x, y)\n",
    "        - h : heuristic function (distance to goal)\n",
    "        - coords : coordinates in the grid\n",
    "        - occupancy_grid: downsized grid map with obstacle information\n",
    "    \n",
    "    return : \n",
    "        - a tuple containing the path distance and the path information in an array of indexs\n",
    "    \"\"\"\n",
    "    \n",
    "    # Size of the downscaled grid\n",
    "    max_val_x = occupancy_grid.shape[0]\n",
    "    max_val_y = occupancy_grid.shape[1]\n",
    "    \n",
    "    # Check if the start and goal are within the limits of the grid's size\n",
    "    for point in [start, goal]:\n",
    "        for coord in point:\n",
    "            assert coord>=0 and coord<max_val_y, \"start or end goal not contained in the map\"\n",
    "    \n",
    "    # Check if start and goal nodes are situated on an obstacle\n",
    "    if occupancy_grid[start[0], start[1]]:\n",
    "        raise Exception('Start node is not traversable')\n",
    "\n",
    "    if occupancy_grid[goal[0], goal[1]]:\n",
    "        raise Exception('Goal node is not traversable')\n",
    "    \n",
    "    # Information on the possible movements\n",
    "    movements = _get_movements_8n()\n",
    "    \n",
    "    # --------------------------------------------------------------#\n",
    "    #                  A* ALGORITHM IMPLEMENTATION                  #\n",
    "    # --------------------------------------------------------------#\n",
    "    \n",
    "    # The set of visited nodes, which are the starting points for neighbor exploration\n",
    "    # The start node is the only one known yet.\n",
    "    openSet = [start]\n",
    "    \n",
    "    # The set of visited nodes that no longer need to be expanded.\n",
    "    closedSet = []\n",
    "\n",
    "    # map containing for each node the node immediately preceding it on the cheapest path currently known.\n",
    "    cameFrom = dict()\n",
    "\n",
    "    # For node n, gScore[n] is the cost of the cheapest path from start to n currently known.\n",
    "    gScore = dict(zip(coords, [np.inf for x in range(len(coords))]))\n",
    "    gScore[start] = 0\n",
    "\n",
    "    # For node n, fScore[n] := gScore[n] + h(n). map with default value of Infinity\n",
    "    fScore = dict(zip(coords, [np.inf for x in range(len(coords))]))\n",
    "    fScore[start] = h[start]\n",
    "    \n",
    "\n",
    "    # While there are still cells to explore\n",
    "    while openSet != []:\n",
    "        \n",
    "        # The node in openSet having the lowest fScore[] value\n",
    "        fScore_openSet = {key:val for (key,val) in fScore.items() if key in openSet}\n",
    "        current = min(fScore_openSet, key=fScore_openSet.get)\n",
    "        del fScore_openSet\n",
    "        \n",
    "        # If the goal is reached, reconstruct and return the obtained path\n",
    "        if current == goal:\n",
    "            return reconstruct_path(cameFrom, current), closedSet\n",
    "\n",
    "        openSet.remove(current)\n",
    "        closedSet.append(current)\n",
    "        \n",
    "        # For each neighbor of the current node \n",
    "        for dx, dy, deltacost in movements:\n",
    "            \n",
    "            neighbor = (current[0]+dx, current[1]+dy)\n",
    "            \n",
    "            # If the node is not in the map, skip\n",
    "            if (neighbor[0] >= occupancy_grid.shape[0]) or (neighbor[1] >= occupancy_grid.shape[1]) or (neighbor[0] < 0) or (neighbor[1] < 0):\n",
    "                continue\n",
    "            \n",
    "            # If the node is occupied or has already been visited, skip\n",
    "            if (occupancy_grid[neighbor[0], neighbor[1]]) or (neighbor in closedSet): \n",
    "                continue\n",
    "            \n",
    "            # Computation of the rotational cost by comparing the previous position, the current one and the next one\n",
    "            if(not (current == start)):\n",
    "                vector_prev = ([current[0] - (cameFrom[current])[0], current[1] - (cameFrom[current])[1]]) \n",
    "                vector_next = ([neighbor[0] - current[0], neighbor[1] - current[1]]) \n",
    "                angle = np.arccos(np.dot(vector_prev, vector_next) / (np.linalg.norm(vector_prev) * np.linalg.norm(vector_next)))\n",
    "                rotation_cost = angle * ROTATION_COST\n",
    "            else:\n",
    "                rotation_cost = 0\n",
    "                \n",
    "            # The movement cost to the neighbor is the linear movement with deltacost and the rotation with rotational_cost\n",
    "            # Tentative_gScore is the distance from start to the neighbor through current\n",
    "            tentative_gScore = gScore[current] + deltacost + rotation_cost\n",
    "            \n",
    "            if neighbor not in openSet:\n",
    "                openSet.append(neighbor)\n",
    "            \n",
    "            # If the path to the neighbor is better than any previous one, keep it\n",
    "            if tentative_gScore < gScore[neighbor]:\n",
    "                cameFrom[neighbor] = current\n",
    "                gScore[neighbor] = tentative_gScore\n",
    "                fScore[neighbor] = gScore[neighbor] + h[neighbor]\n",
    "\n",
    "    # Open set is empty but goal was never reached\n",
    "    print(\"No path found to goal\")\n",
    "    return [], closedSet\n",
    "\n",
    "#--------------------------------------------------------------------------------#\n",
    "\n",
    "def path_planning(robot_vector, goal_center, global_obstacle):\n",
    "    \"\"\"\n",
    "    This function calls the other path planning functions to compute the cheapest path\n",
    "    \n",
    "    param :\n",
    "        - robot_vector : current position of the Thymio robot  \n",
    "        - goal_center : center position of the goal\n",
    "        - global_obstacle : original grid with obstacle information\n",
    "    \n",
    "    return :\n",
    "        - path_final : list of coordinates forming the cheapest path\n",
    "    \"\"\"\n",
    "    \n",
    "    # Generate the downsized obstacle grid\n",
    "    occupancy_grid = create_downsized_grid(global_obstacle)\n",
    "    \n",
    "    # Size of the downscaled grid\n",
    "    max_val_x = occupancy_grid.shape[0]\n",
    "    max_val_y = occupancy_grid.shape[1]\n",
    "    \n",
    "    # Define the start and end goal while changing axis to use matrix coordinates\n",
    "    start = (int(robot_vector[1]/reduction_coeff), int(robot_vector[0]/reduction_coeff))\n",
    "    goal = (int(goal_center[1]/reduction_coeff), int(goal_center[0]/reduction_coeff))\n",
    "\n",
    "    # List of all coordinates in the grid\n",
    "    w,z = np.mgrid[0:max_val_x:1, 0:max_val_y:1]\n",
    "    pos = np.empty(w.shape + (2,))\n",
    "    pos[:, :, 0] = w; pos[:, :, 1] = z\n",
    "    pos = np.reshape(pos, (w.shape[0]*w.shape[1], 2))\n",
    "    coords = list([(int(w[0]), int(w[1])) for w in pos])\n",
    "    \n",
    "    # Define the heuristic, here = distance to goal ignoring obstacles\n",
    "    h = np.linalg.norm(pos - goal, axis=-1)\n",
    "    h = dict(zip(coords, h))\n",
    "\n",
    "    # Run the A* algorithm    \n",
    "    path, visitedNodes = A_Star(start, goal, h, coords, occupancy_grid)\n",
    "    \n",
    "    # Change axis to plot with same coordinates as the grid\n",
    "    path = np.array(path).reshape(-1, 2).transpose()\n",
    "    visitedNodes = np.array(visitedNodes).reshape(-1, 2).transpose()\n",
    "\n",
    "    # Multiply the path coordinates by reduction_coeff to go back to the original grid scale\n",
    "    path_final = path * reduction_coeff\n",
    "    \n",
    "    # Change axis to go back to the original coordinates\n",
    "    path_final[[0,1]] = path_final[[1,0]] \n",
    "    \n",
    "    # Replace the first and last node of the path to fit the true coordinates of the start and goal position\n",
    "    path_final[0][0] = robot_vector[0]\n",
    "    path_final[1][0] = robot_vector[1]\n",
    "    path_final[0][-1] = goal_center[0]\n",
    "    path_final[1][-1] = goal_center[1]\n",
    "    \n",
    "    # Displaying the map and the path information\n",
    "    cmap = colors.ListedColormap(['white', 'red'])\n",
    "    fig_astar, ax_astar = create_empty_plot(max_val_x, max_val_y)\n",
    "    ax_astar.imshow(occupancy_grid, cmap=cmap)\n",
    "    # Plot the best path found and the list of visited nodes\n",
    "    ax_astar.scatter(visitedNodes[1], visitedNodes[0], marker=\"o\", color = 'orange',label=('visited nodes'));\n",
    "    ax_astar.plot(path[1], path[0], marker=\"o\", color = 'blue',label=('path'));\n",
    "    ax_astar.scatter(start[1], start[0], marker=\"o\", color = 'green', s=200,label=('start'));\n",
    "    ax_astar.scatter(goal[1], goal[0], marker=\"o\", color = 'cyan', s=200,label=('goal'));\n",
    "    ax_astar.set_xlabel('x (pixels)')\n",
    "    ax_astar.set_ylabel('y (pixels)')\n",
    "    ax_astar.set_title('Global path found with A*')\n",
    "    ax_astar.legend();\n",
    "    \n",
    "    return path_final"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc1fac0",
   "metadata": {},
   "source": [
    "---\n",
    "<a id=\"filtering\"></a>\n",
    "# Filtering\n",
    "\n",
    "\n",
    "The localization of the Thymio robot is performed using a Kalman filter. This filtering method is well suited to estimating the position and orientation of a mobile robot from noisy or incomplete measurements. The design of the filter in this project is based on using the position ($x, y$) and orientation ($\\theta$) provided by the camera as measurements. In addition, the speed of the robot, provided by the wheel speed sensors ($v_r, v_l$), is used as a prediction. In short, the Kalman filter merges a prediction of the system's future state with a measurement of that state to estimate position probabilistically.\n",
    "\n",
    "<a id=\"state-space-model\"></a>\n",
    "## State-space model\n",
    "\n",
    "### Prediction\n",
    "\n",
    "To estimate the robot's future position, a state-space model needs to be developed: \n",
    "\n",
    "$$\\hat{s}_{a\\_priori}^{t+1} = A \\cdot \\hat{s}_{a\\_posteriori}^{t} + B \\cdot u^{t} + q^t$$\n",
    "\n",
    "The prediction of the future state is referred to as $\\hat{s}_{a\\_priori}^{t+1}$, i.e. the a priori estimate at time t+1. Since the state of the system is defined by its position ($x, y$) and orientation ($\\theta$), this gives: \n",
    "\n",
    "$$\\hat{s}_{a\\_priori}^{t+1} = \\begin{pmatrix}\n",
    "\\hat{x}_{a\\_priori}^{t+1} \\\\\\\\\n",
    "\\hat{y}_{a\\_priori}^{t+1} \\\\\\\\\n",
    "\\hat{\\theta}_{a\\_priori}^{t+1}\n",
    "\\end{pmatrix}$$\n",
    "\n",
    "The current state corresponds to the term $\\hat{s}_{a\\_posteriori}^{t}$, which is the a posteriori estimate at time t. In the same way as above, this gives:\n",
    "\n",
    "$$\\hat{s}_{a\\_posteriori}^{t} = \\begin{pmatrix}\n",
    "\\hat{x}_{a\\_posteriori}^{t} \\\\\\\\\n",
    "\\hat{y}_{a\\_posteriori}^{t} \\\\\\\\\n",
    "\\hat{\\theta}_{a\\_posteriori}^{t}\n",
    "\\end{pmatrix}$$\n",
    "\n",
    "The system input at time t is represented by the vector $u^{t}$. This is made up of two terms: translational speed ($v$) and rotational speed ($\\omega$). \n",
    "\n",
    "$$u^t = \\begin{pmatrix}\n",
    "v \\\\\\\\\n",
    "\\omega\n",
    "\\end{pmatrix} $$\n",
    "\n",
    "These are defined on the basis of the speeds measured by the wheel speed sensors, i.e. the right ($v_r$) and left ($v_l$) speeds, and the spacing between the two wheels ($e$).\n",
    "\n",
    "$$ v = \\cfrac{v_r + v_l}{2} \\qquad\\qquad \\omega = \\cfrac{v_r-v_l}{e} $$ \n",
    "\n",
    "Matrix A characterizes the evolution of the system state, while matrix B describes the impact of the input on the future state. An odometry-based approach allows us to determine these two matrices by considering a very short time interval ($\\delta t$). During this time interval, the robot rotates by $\\delta \\theta = \\omega \\cdot \\delta t$. Knowing this, and referring to the diagram below, the following system of equations can be established: \n",
    "\n",
    "$$\\begin{equation}\n",
    "\\begin{cases}\n",
    "\\hat{x}_{a\\_priori}^{t+1} = \\hat{x}_{a\\_posteriori}^{t} + v \\cdot \\cos\\left(\\hat{\\theta}_{a\\_posteriori}^{t} + \\delta \\theta^t \\right) \\cdot \\delta t \\\\\n",
    "\\hat{y}_{a\\_priori}^{t+1} = \\hat{y}_{a\\_posteriori}^{t} + v \\cdot \\sin\\left(\\hat{\\theta}_{a\\_posteriori}^{t} + \\delta \\theta^t \\right) \\cdot \\delta t \\\\\n",
    "\\hat{\\theta}_{a\\_priori}^{t+1} = \\hat{\\theta}_{a\\_posteriori}^{t} + \\omega \\cdot \\delta t\n",
    "\\end{cases}\n",
    "\\end{equation}$$\n",
    "\n",
    "![state-space_model](Images/schema1.jpg)\n",
    "\n",
    "The matrix form of this system therefore becomes:\n",
    "\n",
    "$$\\begin{equation}\n",
    "A = \\begin{bmatrix} \n",
    "1 & 0 & 0\\\\ \n",
    "0 & 1 & 0 \\\\ \n",
    "0 & 0 & 1\n",
    "\\end{bmatrix}\n",
    "\\qquad\\qquad\n",
    "B = \\begin{bmatrix} \n",
    "\\cos\\left(\\hat{\\theta}_{a\\_posteriori}^{t} + \\delta \\theta^t \\right) \\cdot \\delta t & 0\\\\\n",
    "\\sin\\left(\\hat{\\theta}_{a\\_posteriori}^{t} + \\delta \\theta^t \\right) \\cdot \\delta t & 0 \\\\\n",
    "0 & \\delta t \n",
    "\\end{bmatrix}\n",
    "\\end{equation}$$\n",
    "\n",
    "The final term $q^t$ of this state-space model represents the stochastic perturbation of the state with covariance matrix Q defined as follows:\n",
    "\n",
    "$$\n",
    "Q = \\begin{bmatrix} \n",
    "q_1 & 0 & 0\\\\ \n",
    "0 & q_2 & 0 \\\\ \n",
    "0 & 0 & q_3\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "These diagonal coefficients can be evaluated using an approach similar to that used in Exercise 8 of the MICRO-452 course. \n",
    "\n",
    "The experimentally estimated coefficients are as follows: q1=q2=4 (equivalent to a standard deviation of 2 pixels) and q3=0.03 (equivalent to a standard deviation of 10 degrees).\n",
    "\n",
    "\n",
    "### Measurement\n",
    "\n",
    "Having explored the prediction phase of the state-space model, attention now turns to the second essential part: updating the measurements. This stage aims to refine the predictions by integrating real information captured by the camera. The formula governing this step is :\n",
    "\n",
    "$$ m^{t+1} = C \\cdot s^{t+1} + r^{t+1}$$ \n",
    "\n",
    "Measurements taken at time t+1 are represented here by the term $m_{t+1}$. The data collected by the camera are therefore:\n",
    "\n",
    "$$m^{t+1} = \\begin{pmatrix}\n",
    "x_{captured}^{t+1} \\\\\\\\\n",
    "y_{captured}^{t+1} \\\\\\\\\n",
    "\\theta_{captured}^{t+1}\n",
    "\\end{pmatrix}$$\n",
    "\n",
    "The robot's position ($x, y$) and orientation ($\\theta$) measured by the camera are used directly as system outputs, without any transformation. The matrix C linking the measurements to the state is therefore defined as follows:\n",
    "\n",
    "$$C = \\begin{bmatrix} \n",
    "1 & 0 & 0\\\\ \n",
    "0 & 1 & 0 \\\\ \n",
    "0 & 0 & 1\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "The term $s^{t+1}$ simply represents the state of the system at time t+1:\n",
    "\n",
    "$$s^{t+1} = \\begin{pmatrix}\n",
    "x^{t+1} \\\\\\\\\n",
    "y^{t+1} \\\\\\\\\n",
    "\\theta^{t+1}\n",
    "\\end{pmatrix}$$\n",
    "\n",
    "Finally, the last term $r^{t+1}$ of this equation represents noise on measurements with a covariance matrix R defined as follows:\n",
    "\n",
    "$$\n",
    "R = \\begin{bmatrix} \n",
    "r_1 & 0 & 0\\\\ \n",
    "0 & r_2 & 0 \\\\ \n",
    "0 & 0 & r_3\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Based on the precision of the camera, the following coefficients have been estimated: r1=r2=1 (equivalent to a standard deviation of 1 pixel) and r3=0.01 (equivalent to a standard deviation of 6 degrees).\n",
    "\n",
    "Note: When the camera's view is obstructed, estimation is only possible using the prediction model.\n",
    "\n",
    "\n",
    "<a id=\"kalman-filter\"></a>\n",
    "## Kalman filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f91bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_initialization():\n",
    "    \"\"\"\n",
    "    Initialize the various vectors and matrices requiered for filtering\n",
    "    \n",
    "    robot_vector: position (x and y) and orientation (theta) taken from the camera vision\n",
    "    \"\"\"\n",
    "    \n",
    "    global s_prev_est_a_posteriori, P_prev_est_a_posteriori, A, B, u, C, Q, R\n",
    "\n",
    "    ## Previous State A Posteriori Estimation Vector\n",
    "    # Vector representing the estimated state of the system at the previous time step\n",
    "    s_prev_est_a_posteriori = robot_vector\n",
    "       \n",
    "    ## Previous State A Posteriori Covariance Matrix\n",
    "    # Matrix representing the estimated precision of the previous estimated state (same as R)\n",
    "    P_prev_est_a_posteriori = np.array([[1, 0, 0], \n",
    "                                        [0, 1, 0], \n",
    "                                        [0, 0, 0.01]]) \n",
    "    \n",
    "    ## State Matrix\n",
    "    # Matrix defining how the system evolves from one time step to the next\n",
    "    A = np.array([[1, 0, 0], \n",
    "                  [0, 1, 0], \n",
    "                  [0, 0, 1]])\n",
    "        \n",
    "    ## Input Matrix \n",
    "    # Matrix describing the impact of the input on the state\n",
    "    B = np.array([[1, 0], \n",
    "                  [0, 1], \n",
    "                  [0, 0]]); \n",
    "        \n",
    "    ## Input Vector\n",
    "    # Vector representing control inputs applied to the system \n",
    "    u = np.array([0, 0])\n",
    "    \n",
    "    ## Output Matrix\n",
    "    # Matrix linking measurements to state\n",
    "    C = np.array([[1, 0, 0], \n",
    "                  [0, 1, 0], \n",
    "                  [0, 0, 1]])\n",
    "        \n",
    "    ## Process Noise Covariance Matrix\n",
    "    # Covariance matrix representing uncertainty in system dynamics\n",
    "    Q = np.array([[4, 0, 0], \n",
    "                  [0, 4, 0], \n",
    "                  [0, 0, 0.03]])\n",
    "    \n",
    "    ## Measurement Noise Covariance Matrix\n",
    "    # Matrix representing uncertainty of camera measurements\n",
    "    R = np.array([[1, 0, 0], \n",
    "                  [0, 1, 0], \n",
    "                  [0, 0, 0.01]])\n",
    "\n",
    "#--------------------------------------------------------------------------------#\n",
    "    \n",
    "def update_input(v_l,v_r,update_time, direction_rotation):\n",
    "    \"\"\"\n",
    "    Update the input vector and matrix\n",
    "    \n",
    "    v_l: robot x position deduced from the camera vision\n",
    "    v_r: robot y position deduced from the camera vision\n",
    "    update_time: robot theta orientation deduced from the camera vision\n",
    "    \"\"\"\n",
    "    \n",
    "    global B,u\n",
    "    \n",
    "    Thymio_to_mms = 0.349\n",
    "    mm_to_px = 100/137\n",
    "    \n",
    "    # Average translational speed\n",
    "    v = (v_r +v_l)/2 # Thymio speed (T)\n",
    "    v = v * Thymio_to_mms * mm_to_px # Speed in px/s (T -> mm/s -> px/s)\n",
    "\n",
    "    # Average rotational speed\n",
    "    w = (v_r -v_l)*Thymio_to_mms/robot_diameter # Angular speed in rad/s\n",
    "    \n",
    "    if (direction_rotation == TURN_RIGHT):\n",
    "        w = -w\n",
    "    \n",
    "    # Input vector\n",
    "    u = np.array([v, w]) \n",
    "    \n",
    "    # Angle variation\n",
    "    delta_theta = w * update_time\n",
    "    \n",
    "    # Input matrix\n",
    "    B = np.array([[np.cos(delta_theta + s_prev_est_a_posteriori[2])*update_time, 0],\n",
    "                  [-np.sin(delta_theta + s_prev_est_a_posteriori[2])*update_time, 0], \n",
    "                  [0, update_time]]); \n",
    "    \n",
    "#--------------------------------------------------------------------------------#\n",
    "\n",
    "def kalman_filter(s_prev_est_a_posteriori, P_prev_est_a_posteriori):\n",
    "    \"\"\"\n",
    "    Estimates the current state using the input sensor data and the previous state\n",
    "    \n",
    "    param s_prev_est_a_posteriori: previous state a posteriori estimation\n",
    "    param P_prev_est_a_posteriori: previous state a posteriori covariance\n",
    "    \n",
    "    return s_est_a_posteriori: new a posteriori state estimation\n",
    "    return P_est_a_posteriori: new a posteriori state covariance\n",
    "    \"\"\"\n",
    "    \n",
    "    ## Prediciton through the a priori estimate\n",
    "    # estimated mean of the state\n",
    "    s_est_a_priori = np.dot(A, s_prev_est_a_posteriori)+ np.dot(B, u);\n",
    "    path_apriori.append(s_prev_est_a_posteriori)\n",
    "    \n",
    "    # Estimated covariance of the state\n",
    "    P_est_a_priori = np.dot(A, np.dot(P_prev_est_a_posteriori, A.T)) + Q\n",
    "    \n",
    "    ## Update         \n",
    "    # m, C, and R for a posteriori estimate, depending on the detection of the camera\n",
    "    if camera_on == True:\n",
    "        m = robot_vector\n",
    "        path_camera.append(m)\n",
    "        # innovation / measurement residual\n",
    "        i = m - np.dot(C, s_est_a_priori);\n",
    "        # measurement prediction covariance\n",
    "        S = np.dot(C, np.dot(P_est_a_priori, C.T)) + R;     \n",
    "        # Kalman gain (tells how much the predictions should be corrected based on the measurements)\n",
    "        K = np.dot(P_est_a_priori, np.dot(C.T, np.linalg.inv(S)));\n",
    "        # a posteriori estimate\n",
    "        s_est_a_posteriori = s_est_a_priori + np.dot(K,i);\n",
    "        P_est_a_posteriori = P_est_a_priori - np.dot(K,np.dot(C, P_est_a_priori));\n",
    "    else:\n",
    "        K = 0 # Kalman gain is null because the camera can't deliver any data\n",
    "        # a posteriori estimate\n",
    "        s_est_a_posteriori = s_est_a_priori;\n",
    "        P_est_a_posteriori = P_est_a_priori;\n",
    "        \n",
    "    return s_est_a_posteriori, P_est_a_posteriori"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62802948",
   "metadata": {},
   "source": [
    "---\n",
    "<a id=\"local-navigation\"></a>\n",
    "\n",
    "# Local navigation\n",
    "\n",
    "\n",
    "Local navigation involves adjusting the robot's path to navigate around unexpected obstacles in its immediate vicinity. This often requires the robot to deviate from the optimal path to execute avoidance maneuvers. Thymio has to navigate past random obstacles, not captured in the vision and not factored into the global path planning. \n",
    "\n",
    "Once the obstacle is successfully navigated, Thymio can either return to the optimal path or calculate a new one. \n",
    "\n",
    "Thymio is equipped with five front horizontal proximity sensors for it to detect unknown objects that emerge in its path. Our primary objective is to help Thymio skillfully navigate around these local obstacles whilst enabling ample time for subsequent planning of an optimal path toward the ultimate goal.\n",
    "\n",
    "**Input :**\n",
    "\n",
    "    - Horizontal proximity sensor values; constantly updated\n",
    "\n",
    "**Output :**\n",
    "\n",
    "    - Command for controlling the robot's translational and rotational motion\n",
    "\n",
    "**Challenges :**\n",
    "\n",
    "    - Determining the colour of our physical obstacles have to be white, to not have the camera capture the obstacle in global vision.\n",
    "    \n",
    "    - Determining the shape of our obstacles have to be cylindrical for an optimal maneuver around it.\n",
    "    \n",
    "    - Determining the fixed distance to move forward and theta (angle) for Thymio's rotation was challenging to set without prior knowledge of the obstacle's shape. If the distance was too short or the angle too small, the robot will run into the obstacle. This tok us multiple tries to reach the optimal `distance_forward` and `theta`, as it was challenging to determine when an obstacle is considered cleared. Thus we had to finetune our parameters multiple times to ensure the success of our Thymio's obstacle avoidance. \n",
    "    \n",
    "    - Determining the obstacle threshold for when Thymio should initiate the local avoidance, `obstThrh`.\n",
    "\n",
    "### Parameters \n",
    "\n",
    "| Name                | Purpose                                                                           | Units | Global?|\n",
    "| :------------------- | :------------------------------------------------------------------------------- |-------|-----|\n",
    "| `obstThrh`      | High obstacle threshold for switching from global navigation to local navigation state| Int   | Y |\n",
    "| `angle_turned` | Total angle turned since initial direction, before local avoidance                     | Radians| Y |\n",
    "| `theta` | Angle to turn back, for Thymio to be aligned with initial direction before local avoidance    | Radians | N |\n",
    "| `distance_forward`| Amount of distance to move forward  | Meters | Y |\n",
    "| `diff_lr`| Calculated difference between the most left and most right sensors | Int | N |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "769a00a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def local_avoidance(sensor_prox, theta, dist):\n",
    "    \"\"\"\n",
    "    1a. Obstacle detected right in front: rotate left + go forward\n",
    "    1b. Obstacle detected near sides: rotate + go forward\n",
    "    2. Determine rotation left or right\n",
    "    3. Wall following of the obstacle\n",
    "    4. After obstacle clearance, turn back to initial direction\n",
    "    \"\"\"\n",
    "    # Global variables\n",
    "    global node, obstThrh\n",
    "    \n",
    "    # Calculate difference in Left & Right sensors\n",
    "    diff_lr = sensor_prox[0] - sensor_prox[4]\n",
    "    \n",
    "    # Object detected right in front\n",
    "    if sensor_prox[2] > obstThrh: \n",
    "        # Rotate to the left + go forward\n",
    "        await turn(pi/4)  \n",
    "        theta += pi/4\n",
    "        print(\"Front obstacle, turning right\")\n",
    "        await move_forward(100*dist)\n",
    "        await turn(-theta/2)\n",
    "        print(\"Turning back to initial direction\")\n",
    "        await move_forward(dist*100)\n",
    "        await turn(-theta/2)\n",
    "        await turn(theta/2)\n",
    "        return\n",
    "    else:\n",
    "        # Obstacle threshold\n",
    "        if max(sensor_prox) > obstThrh:\n",
    "            if (sensor_prox[0] + sensor_prox[1]) > (sensor_prox[3] + sensor_prox[4]):\n",
    "                await turn(pi/24)\n",
    "                theta += pi/24\n",
    "                await move_forward(dist*2)\n",
    "                print(\"Left obstacle, turning right\")\n",
    "                if (diff_lr) > 0:\n",
    "                    await turn(pi/30)\n",
    "                    theta += pi/30\n",
    "                    await move_forward(dist)\n",
    "                    print(\"Left wall following\")\n",
    "                else:\n",
    "                    await move_forward(dist)\n",
    "                    print(\"Not wall following\")\n",
    "            elif (sensor_prox[3] + sensor_prox[4]) > (sensor_prox[0] + sensor_prox[1]):\n",
    "                await turn(-pi/24)\n",
    "                theta -= pi/24\n",
    "                await move_forward(dist*2)\n",
    "                print(\"Right obstacle, turning left\")\n",
    "                if (diff_lr) < 0:\n",
    "                    await turn(-pi/30)\n",
    "                    theta -= pi/30\n",
    "                    await move_forward(dist)\n",
    "                    print(\"Right wall following\")\n",
    "                else:\n",
    "                    await move_forward(dist)\n",
    "                    print(\"Not wall following\")\n",
    "            else:\n",
    "                return  # Do nothing\n",
    "    \n",
    "    # Obstacle cleared, move forward for a while before turning back to initial direction & move forward slightly more\n",
    "    await move_forward(110*dist)\n",
    "    await turn(-theta*dist*3.5)\n",
    "    print(\"Turning back to initial direction\")\n",
    "    await move_forward(140*dist)\n",
    "    await turn(theta*dist*3)\n",
    "    await move_forward(dist)\n",
    "    \n",
    "    await motors_stop()\n",
    "    return\n",
    "\n",
    "# Local avoidance function with sensor values\n",
    "async def la_function(sensor_prox): \n",
    "    await node.wait_for_variables()\n",
    "    \n",
    "    while sum(sensor_prox[i] > obstThrh for i in range(0, 5)) > 0: \n",
    "        sensor_prox = node[\"prox.horizontal\"]\n",
    "        print(list(sensor_prox))\n",
    "        await local_avoidance(sensor_prox, angle_turned, distance_forward)\n",
    "        await client.sleep(0.2)\n",
    "        await motors_stop()\n",
    "        print(\"Completed Local Avoidance!\")\n",
    "        return True\n",
    "    return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cfd665b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f2b1422c",
   "metadata": {},
   "source": [
    "---\n",
    "<a id=\"motion-control\"></a>\n",
    "# Motion control\n",
    "\n",
    "<a id=\"connection-functions\"></a>\n",
    "## Connection functions\n",
    "\n",
    "Initially, functions facilitating the establishment and termination of the connection with the robot have been implemented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8a421b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thymio connection\n",
    "async def connect_Thymio():\n",
    "    \"\"\"\n",
    "    Establish a connection with the Thymio if possible\n",
    "    \"\"\"\n",
    "    global node, client\n",
    "    try:\n",
    "        client = ClientAsync()\n",
    "        node = await asyncio.wait_for(client.wait_for_node(), timeout=2.0)\n",
    "        await node.lock()\n",
    "        print(\"Thymio connected\")\n",
    "\n",
    "    except asyncio.TimeoutError:\n",
    "        print(\"Thymio not connected: Timeout while waiting for node.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Thymio not connected: {str(e)}\")\n",
    "        \n",
    "# Thymio disconnection\n",
    "def disconnect_Thymio():\n",
    "    \"\"\"\n",
    "    Enable to disconnect the Thymio\n",
    "    \"\"\"\n",
    "    aw(node.stop())\n",
    "    aw(node.unlock())\n",
    "    print(\"Thymio disconnected\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8191deba",
   "metadata": {},
   "source": [
    "<a id=\"motion-functions\"></a>\n",
    "## Motion functions\n",
    "\n",
    "The robot’s movement from its starting position to the target is executed using the coordinates of the global path. To transition from one coordinate to the next, a specific method has been adopted. The robot begins by orienting itself towards the next point, taking into account its current orientation. Once the orientation is adjusted, the robot performs a straight-line movement to the next point. This step is repeated from position to position until the final destination is reached. The basic functions therefore include: activation and deactivation of motors, rotation by a defined angle, and linear movement over a defined distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "798c0f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thymio set motors speeds  \n",
    "async def set_speeds(left_speed, right_speed):\n",
    "    \"\"\"\n",
    "    Enable to set the speed of the Thymio's wheels\n",
    "    \"\"\"\n",
    "    global node\n",
    "    v = {\n",
    "        \"motor.left.target\":  [left_speed],\n",
    "        \"motor.right.target\": [right_speed],\n",
    "    }\n",
    "    await node.set_variables(v)\n",
    "    \n",
    "# Thymio motors stop     \n",
    "async def motors_stop():\n",
    "    \"\"\"\n",
    "    Stop the Thymio\n",
    "    \"\"\"\n",
    "    global node\n",
    "    v = {\n",
    "        \"motor.left.target\":  [0],\n",
    "        \"motor.right.target\": [0],\n",
    "    }\n",
    "    await node.set_variables(v)    \n",
    "\n",
    "# Conversion factors\n",
    "Thymio_to_mms = 0.349\n",
    "px_to_mm = 137/100\n",
    "# Constants\n",
    "ROTATION_SPEED = 100\n",
    "TIME_FULL_TURN = (8800/1000)\n",
    "\n",
    "# Thymio turns a specied angle\n",
    "async def turn(angle):\n",
    "    # Calculate the time needed to turn through the required angle\n",
    "    rotation_time = (abs(angle) / (2*np.pi)) * TIME_FULL_TURN\n",
    "\n",
    "    # Turn robot on itself\n",
    "    # Check the sign of angle\n",
    "    if np.sign(angle) > 0:\n",
    "        # If angle is positive, turn left\n",
    "        await set_speeds(-ROTATION_SPEED, ROTATION_SPEED)\n",
    "        left_or_right = TURN_LEFT\n",
    "    else:\n",
    "        # If angle is negative, turn right\n",
    "        await set_speeds(ROTATION_SPEED, -ROTATION_SPEED)\n",
    "        left_or_right = TURN_RIGHT\n",
    "\n",
    "    # Wait required time\n",
    "    time.sleep(rotation_time)\n",
    "    return rotation_time, left_or_right\n",
    "\n",
    "# Constants\n",
    "FORWARD_SPEED = 200 \n",
    "TIME_PER_MM = 15.5/1000  # Time it takes for the robot to travel one meter at base speed\n",
    "\n",
    "async def move_forward(distance_px):\n",
    "    # Calculate the time needed to travel the requested distance\n",
    "    \n",
    "    distance_mm = distance_px * px_to_mm\n",
    "    travel_time = (distance_mm) * TIME_PER_MM\n",
    "    \n",
    "    # Robot moves forward\n",
    "    await set_speeds(FORWARD_SPEED, FORWARD_SPEED)\n",
    "\n",
    "    # Wait for the necessary time\n",
    "    time.sleep(travel_time)\n",
    "    return travel_time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c28cd260",
   "metadata": {},
   "source": [
    "<a id=\"motion-control-function\"></a>\n",
    "## Motion control function\n",
    "\n",
    "The management of one coordinate to another is entirely handled by the following function, which determines whether the next movement is a rotation or a translation, and establishes the angle or distance to be covered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf49d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def reach_next_node(next_node, mode, estimated_pos):\n",
    "\n",
    "    # Vector between the estimated position and the next position in the global path\n",
    "    vector_next_node = np.array([0,0])  \n",
    "    vector_next_node[0] = path_final[0][next_node] - estimated_pos[0]\n",
    "    vector_next_node[1] = path_final[1][next_node] - estimated_pos[1] \n",
    "    \n",
    "    # Normalized angle between the estimated position and the next position in the global path\n",
    "    gamma = -math.atan2(vector_next_node[1], vector_next_node[0]) - estimated_pos[2]\n",
    "    gamma = (gamma + np.pi) % (2*np.pi) - np.pi\n",
    "    \n",
    "    # Distance separating the estimated position and the next position in the global path\n",
    "    path_next_node = np.array([path_final[0][next_node], path_final[1][next_node]])\n",
    "    path_current_node = np.array([estimated_pos[0], estimated_pos[1]])\n",
    "    d = np.linalg.norm(path_next_node - path_current_node)\n",
    "    \n",
    "    if(not mode):\n",
    "        if(abs(gamma) > ANGLE_THRESHOLD):\n",
    "            time_r, left_or_right = await turn(gamma)\n",
    "        else: \n",
    "            time_r = 0 \n",
    "            left_or_right = 1  \n",
    "        return time_r, left_or_right \n",
    "        \n",
    "    if (mode):\n",
    "        if( d > FORWARD_THRESHOLD):\n",
    "            time_f = await move_forward(d)\n",
    "            return time_f"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9114a36c",
   "metadata": {},
   "source": [
    "---\n",
    "<a id=\"project-structure\"></a>\n",
    "# Project structure\n",
    "\n",
    "\n",
    "The project was realized by assembling various components. The camera, an essential element, provides the vision necessary for global navigation and filtering. It plays a crucial role in detecting the robot, the target, and fixed obstacles, thus allowing the calculation of the optimal path. It also provides position data ($x,y$) and the orientation ($\\theta$) of the robot, which are indispensable for estimating its position when using the Kalman filter.\n",
    "\n",
    "Global navigation generates the coordinates of the path to follow to reach the target. The Kalman filter, on the other hand, receives position and orientation data measured by the camera, as well as information related to motion control (wheel speeds and travel time). It outputs the best estimate of Thymio's real position.\n",
    "\n",
    "Finally, the robot constantly interacts with local navigation and motion control to avoid local obstacles and move towards its destination. The motor commands are used to move the robot, while the frontal proximity sensors are used to detect local obstacles. \n",
    "\n",
    "![interactions](Images/diag1.jpg)\n",
    "\n",
    "The functioning of the project is detailed below. Following initialization, a mapping is carried out, allowing the calculation of the global path. The program then enters a loop that continues until the robot reaches its destination. At each iteration of this loop, the robot checks whether it has been kidnnaped or if a mobile obstacle is in its path. If either of these situations occurs, the robot recalculates a global path (after avoiding any potential obstacle). If none of these situations occur, the robot performs the movement defined by the motion control. Once the movement is completed, the robot checks whether its current position corresponds to its final destination. It should be noted that the robot is capable of moving with or without the information provided by the camera.\n",
    "\n",
    "![interactions](Images/diag2.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc20a4a7",
   "metadata": {},
   "source": [
    "---\n",
    "<a id=\"runnable-cells\"></a>\n",
    "# Runnable cells"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cbea251",
   "metadata": {},
   "source": [
    "<a id=\"functions-declaration\"></a>\n",
    "## Functions declaration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62c3543",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_color_circle(image, lower_color_bound, upper_color_bound):\n",
    "    # Convert the input image from BGR to HSV color space for easier color detection\n",
    "    hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "    # Create a mask that isolates the pixels within the specified color range\n",
    "    mask = cv2.inRange(hsv, lower_color_bound, upper_color_bound)\n",
    "\n",
    "    # Apply the mask to the original image to keep only the colors of interest\n",
    "    color_only = cv2.bitwise_and(image, image, mask=mask)\n",
    "\n",
    "    # Convert the masked image to grayscale to prepare for circle detection\n",
    "    gray = cv2.cvtColor(color_only, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Apply Canny edge detection to the grayscale image to highlight edges\n",
    "    edges = cv2.Canny(gray, 100, 200)\n",
    "\n",
    "    # Use Hough Circle Transform to detect circles in the edged image\n",
    "    circles = cv2.HoughCircles(edges, cv2.HOUGH_GRADIENT, 1, 20,\n",
    "                               param1=20, param2=15, minRadius=10, maxRadius=50)\n",
    "    \n",
    "    # Check if any circles were detected\n",
    "    if circles is not None:\n",
    "        # Convert circles' dimensions from float to integers\n",
    "        circles = np.uint16(np.around(circles[0, :]))\n",
    "\n",
    "        # Return a list of tuples, each representing a circle with (x, y) center coordinates and radius\n",
    "        return [(circle[0], circle[1], circle[2]) for circle in circles]\n",
    "    else:\n",
    "        # Return an empty list if no circles are detected\n",
    "        return []\n",
    "    \n",
    "\n",
    "#--------------------------------------------------------------------------------#\n",
    "\n",
    "def crop_roi_from_circles(image, circles):\n",
    "    if circles is not None and len(circles) >= 4:\n",
    "\n",
    "        points = np.array([circle[:2] for circle in circles], dtype=np.float32)\n",
    "\n",
    "        rect = cv2.boundingRect(points)\n",
    "\n",
    "         # Cropping the image within the defined boundaries\n",
    "        x, y, w, h = rect\n",
    "        cropped_image = image[y:y+h, x:x+w]\n",
    "        return cropped_image,(x, y, w, h)\n",
    "    else:\n",
    "        print(\"Edge circles not detected correctly\")\n",
    "        return image,None \n",
    "    \n",
    "\n",
    "#--------------------------------------------------------------------------------#\n",
    "\n",
    "def detect_obstacle_contours(image, area_threshold, kernel_size):\n",
    "    \"\"\"\n",
    "    Detects and dilates obstacle contours in the given image.\n",
    "    :param image: Input image.\n",
    "    :param area_threshold: Area threshold for filtering contours.\n",
    "    :param kernel_size: Size of the kernel used for dilation.\n",
    "    :return: Image with obstacle contours drawn.\n",
    "    \"\"\"\n",
    "    hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "    mask_black = cv2.inRange(hsv, lower_black_bound, upper_black_bound)\n",
    "    contours, _ = cv2.findContours(mask_black, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "     # Filter out contours that are smaller than the specified area threshold\n",
    "    filtered_contours = [cnt for cnt in contours if cv2.contourArea(cnt) > area_threshold]\n",
    "    # dilation kernel of the specified size (how the size of the Thymio is taken into account)\n",
    "    kernel = np.ones((kernel_size, kernel_size), np.uint8)\n",
    "    # Dilate the mask to make the contours more pronounced\n",
    "    mask_dilated = cv2.dilate(mask_black, kernel, iterations=1)\n",
    "    # Find contours \n",
    "    dilated_contours, _ = cv2.findContours(mask_dilated, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    # Filter out dilated contours that are smaller than the increased area threshold\n",
    "    filtered_dilated_contours = [cnt for cnt in dilated_contours if cv2.contourArea(cnt) > area_threshold+10000]\n",
    "    contour_image = image.copy()\n",
    "    # Draw the original contours in green and the dilated contours in red:\n",
    "    cv2.drawContours(contour_image, filtered_contours, -1, (0, 255, 0), 2)\n",
    "    cv2.drawContours(contour_image, filtered_dilated_contours, -1, (0, 0, 255), 2)\n",
    "    return contour_image, filtered_contours, filtered_dilated_contours\n",
    "    \n",
    "#--------------------------------------------------------------------------------#\n",
    "    \n",
    "def create_obstacle_matrix(image, dilated_contours):\n",
    "    # Get the height and width of the input image\n",
    "    height, width = image.shape[:2]\n",
    "    # Create a matrix (of the same size as the image) filled with ones\n",
    "    # Ones indicate non-obstacle areas initially\n",
    "    obstacle_matrix = np.ones((height, width), dtype=np.uint8)\n",
    "    for contour in dilated_contours:\n",
    "        cv2.fillPoly(obstacle_matrix, [contour], 0)\n",
    "\n",
    "    # Return the obstacle matrix where obstacles are marked as zeros (for the global path planning)\n",
    "    return obstacle_matrix\n",
    "\n",
    "\n",
    "#--------------------------------------------------------------------------------#\n",
    "\n",
    "def find_rectangle_center(image, lower_colour_bound, upper_colour_bound, area_threshold=1000):\n",
    "    # convert HSV\n",
    "    hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "    # colour mask \n",
    "    mask = cv2.inRange(hsv, lower_colour_bound, upper_colour_bound)\n",
    "\n",
    "    ## Loop through each contour to identify the one matching the rectangle\n",
    "    contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    # Check if the contour area is larger than the specified threshold\n",
    "    for contour in contours:\n",
    "        if cv2.contourArea(contour) > area_threshold:\n",
    "            # Calculate the bounding box for the contour\n",
    "            x, y, w, h = cv2.boundingRect(contour)\n",
    "\n",
    "            # Erase the rectangle by drawing it in white\n",
    "            cv2.rectangle(image, (x, y), (x + w, y + h), (255, 255, 255), -1)\n",
    "\n",
    "            # Calculate the center of the rectangle\n",
    "            center = (x + w // 2, y + h // 2)\n",
    "            return center\n",
    "    # Return None if no suitable rectangle is found\n",
    "    return None\n",
    "\n",
    "\n",
    "#--------------------------------------------------------------------------------#\n",
    "\n",
    "def preprocess_image(video_capture, lower_goal_colour_bound, upper_goal_colour_bound):\n",
    "    ret, initial_frame = video_capture.read()\n",
    "    if ret:\n",
    "        #initial_frame, cropping_coords = crop_largest_white_area(initial_frame, 200000)\n",
    "        blue_circles = detect_color_circle(initial_frame, lower_goal_colour_bound, upper_goal_colour_bound)\n",
    "        # Crop the region of interest (ROI) from the circles detected\n",
    "        initial_frame ,cropping_coords= crop_roi_from_circles(initial_frame, blue_circles)\n",
    "        # Find the center of the blue rectangle (the goal area)\n",
    "        goal_center = find_rectangle_center(initial_frame, lower_blue_bound, upper_blue_bound,2000)\n",
    "        if goal_center:\n",
    "            # Store the coordinates of the detected yellow circle\n",
    "            yellow_circle_coords = goal_center \n",
    "            radius = 10  \n",
    "            color = (0, 255, 255)  \n",
    "            cv2.circle(initial_frame, yellow_circle_coords, radius, color, 3)\n",
    "        # Get the robot's current position and orientation vector\n",
    "        robot_vector = robot_info(initial_frame)\n",
    "        if robot_vector[0] and robot_vector[1]:\n",
    "            cv2.circle(initial_frame, (robot_vector[0], robot_vector[1]),80, (255, 255, 255), -1)\n",
    "        # Detect obstacle contours in the frame\n",
    "        contour_image = detect_obstacle_contours(initial_frame, 2000, 80)\n",
    "        # Create an obstacle matrix based on the detected contours\n",
    "        global_obstacle = create_obstacle_matrix(initial_frame,contour_image[2])\n",
    "         # Flip and display the initial frame with contours for visual inspection\n",
    "        initial_frame = np.flipud(initial_frame)\n",
    "        plt.imshow(cv2.cvtColor(contour_image[0], cv2.COLOR_BGR2RGB))\n",
    "        plt.title('Initial Contours')\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "        # Return the processed frame, cropping coordinates, contour image, global obstacle matrix, goal center, and robot vector\n",
    "        return initial_frame, cropping_coords, contour_image, global_obstacle,goal_center, robot_vector\n",
    "    else:\n",
    "        # Return None for all outputs if the frame wasn't captured\n",
    "        return None, None, None, None, None, None\n",
    "    \n",
    "\n",
    "#--------------------------------------------------------------------------------#\n",
    "    \n",
    "def robot_info(frame):\n",
    "    global camera_on\n",
    "    red_circles = detect_color_circle(frame, lower_red_bound, upper_red_bound)\n",
    "    green_circles = detect_color_circle(frame, lower_green_bound, upper_green_bound)\n",
    "    \n",
    "    if red_circles and green_circles and len(red_circles) >= 2:\n",
    "        # Calculate robot's position and orientation vector\n",
    "        # Calculate the midpoint between the centers of the red circles\n",
    "        midpoint = ((red_circles[0][0] + red_circles[1][0]) // 2,\n",
    "        (red_circles[0][1] + red_circles[1][1]) // 2)\n",
    "        # Calculate the directional vector\n",
    "        direction = np.array([midpoint[0] - green_circles[0][0], midpoint[1] - green_circles[0][1]])\n",
    "\n",
    "        # Normalize and extend the vector\n",
    "        length = 30  # Additional length\n",
    "        direction = direction / np.linalg.norm(direction) * length\n",
    "\n",
    "        # Calculate the new endpoint\n",
    "        new_endpoint = (int(green_circles[0][0] + direction[0]), int(green_circles[0][1] + direction[1]))\n",
    "\n",
    "        # Draw the extended arrow\n",
    "        cv2.arrowedLine(frame, new_endpoint, green_circles[0][:2], (0, 0, 0), 3)\n",
    "\n",
    "        # Calculate the angle of orientation with respect to the x-axis\n",
    "        dx =   midpoint[0] - green_circles[0][0]\n",
    "        dy =   midpoint[1] - green_circles[0][1]\n",
    "        angle = math.atan2(dy, dx)\n",
    "        angle_degrees = math.degrees(angle)\n",
    "        if(angle_degrees>= 0):\n",
    "            angle_degrees =180 - angle_degrees\n",
    "        elif (angle_degrees < 0):\n",
    "            angle_degrees = -(180 + angle_degrees)\n",
    "        # Store the robot's orientation vector\n",
    "        robot_vector = (midpoint[0], midpoint[1], np.radians(angle_degrees)) \n",
    "        # Display the information on the frame\n",
    "        cv2.putText(frame, f'Angle: {angle_degrees:.2f} degrees', (10, 30), \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n",
    "\n",
    "        cv2.putText(frame, f'Midpoint: ({midpoint[0]}, {midpoint[1]})', (10, 70),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n",
    "\n",
    "        cv2.circle(frame, midpoint,10, (255, 0, 0), -1)\n",
    "        camera_on = True\n",
    "        return midpoint[0], midpoint[1], angle_degrees\n",
    "\n",
    "    else:\n",
    "        camera_on = False\n",
    "        return None,None,None\n",
    "\n",
    "    \n",
    "#--------------------------------------------------------------------------------#\n",
    "    \n",
    "def process_image(frame, cropping_coords, contour_image, update_rate):\n",
    "    if cropping_coords is not None:\n",
    "        x, y, w, h = cropping_coords\n",
    "        frame = frame[y:y+h, x:x+w]\n",
    "    # Detect red and green circles\n",
    "    robot_vector = robot_info(frame)\n",
    "    if  robot_vector is not None:\n",
    "        cv2.drawContours(frame, contour_image[1], -1, (0, 255, 0), 2)\n",
    "        cv2.drawContours(frame, contour_image[2], -1, (0, 0, 255), 2)\n",
    "\n",
    "        # Display the resulting frame\n",
    "        cv2.imshow('Robot Detection', frame)\n",
    "    return robot_vector\n",
    "\n",
    "#--------------------------------------------------------------------------------#\n",
    "\n",
    "def x_robot_projection_to_ground(x_pos_robot):\n",
    "    return round(0.96*x_pos_robot+16.67)\n",
    "    \n",
    "#--------------------------------------------------------------------------------#\n",
    "\n",
    "def y_robot_projection_to_ground(y_pos_robot):\n",
    "    return round(0.95*y_pos_robot+14.29)\n",
    "\n",
    "#--------------------------------------------------------------------------------#\n",
    "\n",
    "def  create_downsized_grid(global_obstacle):\n",
    "    \"\"\" \n",
    "    Creation of the downsized grid occupation_grid with a scale of reduction_coeff\n",
    "    \n",
    "    param : \n",
    "        - global_obstacle : grid containing the obstacle positions with original size\n",
    "    return : \n",
    "        - occupancy_grid: Downsized grid with averaged obstacle positions\n",
    "    \"\"\"\n",
    "\n",
    "    # Invert the data to have a value of 0 for free cells and 1 for occupied cells.\n",
    "    global_obstacle = np.logical_not( global_obstacle )  \n",
    "    max_val_x_init = global_obstacle.shape[0]\n",
    "    max_val_y_init = global_obstacle.shape[1]\n",
    "\n",
    "    # The size of the new grid is calculated from the size of the original grid global_obstacle\n",
    "    max_val_x = int(max_val_x_init / reduction_coeff)\n",
    "    max_val_y = int(max_val_y_init / reduction_coeff)\n",
    "\n",
    "    # Creation of the downsized grid with\n",
    "    occupancy_grid = np.zeros((max_val_x, max_val_y), dtype=int)\n",
    "\n",
    "    # Inspection of squares (of size reduction_coeff*reduction_coeff) in the original grid\n",
    "    for i in range (max_val_x):\n",
    "        for j in range (max_val_y):\n",
    "            sum_pixels = 0\n",
    "\n",
    "            for k in range (reduction_coeff):\n",
    "\n",
    "                # Verification that the indexs indice_x and indice_y are in the grid range\n",
    "                indice_x = int(i * reduction_coeff - reduction_coeff/2 + k)\n",
    "                if (indice_x < 0):\n",
    "                    indice_x = 0\n",
    "                elif (indice_x > (max_val_x_init - 1)):\n",
    "                    indice_x = max_val_x_init -1\n",
    "\n",
    "                indice_y = int(j * reduction_coeff - reduction_coeff/2 + k)\n",
    "                if (indice_y < 0):\n",
    "\n",
    "                    indice_y = 0\n",
    "                elif (indice_y > (max_val_y_init - 1)):\n",
    "                    indice_y = max_val_y_init -1\n",
    "\n",
    "                # The sum of all values present in the inspected square is stored in sum_pixels\n",
    "                sum_pixels = sum_pixels + global_obstacle[indice_x][indice_y]\n",
    "\n",
    "\n",
    "            # If an obstacle is found in the square, a 1 is placed in the corresponding position in the new grid\n",
    "            if sum_pixels == 0:\n",
    "                occupancy_grid[i][j] = 0\n",
    "            else:\n",
    "                occupancy_grid[i][j] = 1\n",
    "\n",
    "    return occupancy_grid\n",
    "\n",
    "#--------------------------------------------------------------------------------#\n",
    "\n",
    "def create_empty_plot(max_val_x, max_val_y):\n",
    "    \"\"\"\n",
    "    Creation of a figure to plot a matrix of max size max_val \n",
    "    \n",
    "    param : \n",
    "        - max_val_x : maximum dimension of the matrix on x axis\n",
    "        - max_val_y : maximum dimension of the matrix on y axis\n",
    "    return:\n",
    "        -  fig : general figure\n",
    "        -  ax : axis information\n",
    "    \"\"\"\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(7,7))\n",
    "    \n",
    "    major_ticks = np.arange(0, max_val_y+1, 5)\n",
    "    minor_ticks = np.arange(0, max_val_y+1, 1)\n",
    "    ax.set_xticks(major_ticks)\n",
    "    ax.set_xticks(minor_ticks, minor=True)\n",
    "    ax.set_yticks(major_ticks)\n",
    "    ax.set_yticks(minor_ticks, minor=True)\n",
    "    ax.grid(which='minor', alpha=0.2)\n",
    "    ax.grid(which='major', alpha=0.5)\n",
    "    \n",
    "    # Definition of axis limits with the information of the matrix size\n",
    "    ax.set_ylim([max_val_x,-1])\n",
    "    ax.set_xlim([-1,max_val_y])\n",
    "    ax.grid(True)\n",
    "    \n",
    "    return fig, ax\n",
    "\n",
    "\n",
    "#--------------------------------------------------------------------------------#\n",
    "\n",
    "def _get_movements_8n():\n",
    "    \"\"\"\n",
    "    Get all possible 8-connectivity movements : up, down, left, right and the 4 diagonals.\n",
    "    \n",
    "    return : \n",
    "        - list of movements with cost [(dx, dy, movement_cost)]\n",
    "    \"\"\"\n",
    "    s2 = math.sqrt(2)\n",
    "    return [(1, 0, 1.0),\n",
    "            (0, 1, 1.0),\n",
    "            (-1, 0, 1.0),\n",
    "            (0, -1, 1.0),\n",
    "            (1, 1, s2),\n",
    "            (-1, 1, s2),\n",
    "            (-1, -1, s2),\n",
    "            (1, -1, s2)]\n",
    "\n",
    "#--------------------------------------------------------------------------------#\n",
    "\n",
    "def reconstruct_path(cameFrom, current):\n",
    "    \"\"\"\n",
    "    Reconstructs the path from start node to the current node\n",
    "    \n",
    "    param :\n",
    "        - cameFrom: map (dict) containing for each node the node immediately \n",
    "                     preceding it on the cheapest path currently known.\n",
    "        - param current : current node (x, y)\n",
    "        \n",
    "    return : \n",
    "        - total_path : list of nodes from start to current node\n",
    "    \"\"\"\n",
    "    \n",
    "    # The first item is the current node\n",
    "    total_path = [current]\n",
    "    \n",
    "    while current in cameFrom.keys():\n",
    "        # Add the previous node to the start of the list\n",
    "        total_path.insert(0, cameFrom[current]) \n",
    "        current = cameFrom[current]\n",
    "    return total_path\n",
    "\n",
    "#--------------------------------------------------------------------------------#\n",
    "\n",
    "def A_Star(start, goal, h, coords, occupancy_grid):\n",
    "    \"\"\"\n",
    "    Implementation of the A* algorithm, finding the cheapest path from start to goal \n",
    "    \n",
    "    param :\n",
    "        - start : start node (x, y)\n",
    "        - goal: goal node (x, y)\n",
    "        - h : heuristic function (distance to goal)\n",
    "        - coords : coordinates in the grid\n",
    "        - occupancy_grid: downsized grid map with obstacle information\n",
    "    \n",
    "    return : \n",
    "        - a tuple containing the path distance and the path information in an array of indexs\n",
    "    \"\"\"\n",
    "    \n",
    "    # Size of the downscaled grid\n",
    "    max_val_x = occupancy_grid.shape[0]\n",
    "    max_val_y = occupancy_grid.shape[1]\n",
    "    \n",
    "    # Check if the start and goal are within the limits of the grid's size\n",
    "    for point in [start, goal]:\n",
    "        for coord in point:\n",
    "            assert coord>=0 and coord<max_val_y, \"start or end goal not contained in the map\"\n",
    "    \n",
    "    # Check if start and goal nodes are situated on an obstacle\n",
    "    if occupancy_grid[start[0], start[1]]:\n",
    "        raise Exception('Start node is not traversable')\n",
    "\n",
    "    if occupancy_grid[goal[0], goal[1]]:\n",
    "        raise Exception('Goal node is not traversable')\n",
    "    \n",
    "    # Information on the possible movements\n",
    "    movements = _get_movements_8n()\n",
    "    \n",
    "    # --------------------------------------------------------------#\n",
    "    #                  A* ALGORITHM IMPLEMENTATION                  #\n",
    "    # --------------------------------------------------------------#\n",
    "    \n",
    "    # The set of visited nodes, which are the starting points for neighbor exploration\n",
    "    # The start node is the only one known yet.\n",
    "    openSet = [start]\n",
    "    \n",
    "    # The set of visited nodes that no longer need to be expanded.\n",
    "    closedSet = []\n",
    "\n",
    "    # map containing for each node the node immediately preceding it on the cheapest path currently known.\n",
    "    cameFrom = dict()\n",
    "\n",
    "    # For node n, gScore[n] is the cost of the cheapest path from start to n currently known.\n",
    "    gScore = dict(zip(coords, [np.inf for x in range(len(coords))]))\n",
    "    gScore[start] = 0\n",
    "\n",
    "    # For node n, fScore[n] := gScore[n] + h(n). map with default value of Infinity\n",
    "    fScore = dict(zip(coords, [np.inf for x in range(len(coords))]))\n",
    "    fScore[start] = h[start]\n",
    "    \n",
    "\n",
    "    # While there are still cells to explore\n",
    "    while openSet != []:\n",
    "        \n",
    "        # The node in openSet having the lowest fScore[] value\n",
    "        fScore_openSet = {key:val for (key,val) in fScore.items() if key in openSet}\n",
    "        current = min(fScore_openSet, key=fScore_openSet.get)\n",
    "        del fScore_openSet\n",
    "        \n",
    "        # If the goal is reached, reconstruct and return the obtained path\n",
    "        if current == goal:\n",
    "            return reconstruct_path(cameFrom, current), closedSet\n",
    "\n",
    "        openSet.remove(current)\n",
    "        closedSet.append(current)\n",
    "        \n",
    "        # For each neighbor of the current node \n",
    "        for dx, dy, deltacost in movements:\n",
    "            \n",
    "            neighbor = (current[0]+dx, current[1]+dy)\n",
    "            \n",
    "            # If the node is not in the map, skip\n",
    "            if (neighbor[0] >= occupancy_grid.shape[0]) or (neighbor[1] >= occupancy_grid.shape[1]) or (neighbor[0] < 0) or (neighbor[1] < 0):\n",
    "                continue\n",
    "            \n",
    "            # If the node is occupied or has already been visited, skip\n",
    "            if (occupancy_grid[neighbor[0], neighbor[1]]) or (neighbor in closedSet): \n",
    "                continue\n",
    "            \n",
    "            # Computation of the rotational cost by comparing the previous position, the current one and the next one\n",
    "            if(not (current == start)):\n",
    "                vector_prev = ([current[0] - (cameFrom[current])[0], current[1] - (cameFrom[current])[1]]) \n",
    "                vector_next = ([neighbor[0] - current[0], neighbor[1] - current[1]]) \n",
    "                angle = np.arccos(np.dot(vector_prev, vector_next) / (np.linalg.norm(vector_prev) * np.linalg.norm(vector_next)))\n",
    "                rotation_cost = angle * ROTATION_COST\n",
    "            else:\n",
    "                rotation_cost = 0\n",
    "                \n",
    "            # The movement cost to the neighbor is the linear movement with deltacost and the rotation with rotational_cost\n",
    "            # Tentative_gScore is the distance from start to the neighbor through current\n",
    "            tentative_gScore = gScore[current] + deltacost + rotation_cost\n",
    "            \n",
    "            if neighbor not in openSet:\n",
    "                openSet.append(neighbor)\n",
    "            \n",
    "            # If the path to the neighbor is better than any previous one, keep it\n",
    "            if tentative_gScore < gScore[neighbor]:\n",
    "                cameFrom[neighbor] = current\n",
    "                gScore[neighbor] = tentative_gScore\n",
    "                fScore[neighbor] = gScore[neighbor] + h[neighbor]\n",
    "\n",
    "    # Open set is empty but goal was never reached\n",
    "    print(\"No path found to goal\")\n",
    "    return [], closedSet\n",
    "\n",
    "#--------------------------------------------------------------------------------#\n",
    "\n",
    "def path_planning(robot_vector, goal_center, global_obstacle):\n",
    "    \"\"\"\n",
    "    This function calls the other path planning functions to compute the cheapest path\n",
    "    \n",
    "    param :\n",
    "        - robot_vector : current position of the Thymio robot  \n",
    "        - goal_center : center position of the goal\n",
    "        - global_obstacle : original grid with obstacle information\n",
    "    \n",
    "    return :\n",
    "        - path_final : list of coordinates forming the cheapest path\n",
    "    \"\"\"\n",
    "    \n",
    "    # Generate the downsized obstacle grid\n",
    "    occupancy_grid = create_downsized_grid(global_obstacle)\n",
    "    \n",
    "    # Size of the downscaled grid\n",
    "    max_val_x = occupancy_grid.shape[0]\n",
    "    max_val_y = occupancy_grid.shape[1]\n",
    "    \n",
    "    # Define the start and end goal while changing axis to use matrix coordinates\n",
    "    start = (int(robot_vector[1]/reduction_coeff), int(robot_vector[0]/reduction_coeff))\n",
    "    goal = (int(goal_center[1]/reduction_coeff), int(goal_center[0]/reduction_coeff))\n",
    "\n",
    "    # List of all coordinates in the grid\n",
    "    w,z = np.mgrid[0:max_val_x:1, 0:max_val_y:1]\n",
    "    pos = np.empty(w.shape + (2,))\n",
    "    pos[:, :, 0] = w; pos[:, :, 1] = z\n",
    "    pos = np.reshape(pos, (w.shape[0]*w.shape[1], 2))\n",
    "    coords = list([(int(w[0]), int(w[1])) for w in pos])\n",
    "    \n",
    "    # Define the heuristic, here = distance to goal ignoring obstacles\n",
    "    h = np.linalg.norm(pos - goal, axis=-1)\n",
    "    h = dict(zip(coords, h))\n",
    "\n",
    "    # Run the A* algorithm    \n",
    "    path, visitedNodes = A_Star(start, goal, h, coords, occupancy_grid)\n",
    "    \n",
    "    # Change axis to plot with same coordinates as the grid\n",
    "    path = np.array(path).reshape(-1, 2).transpose()\n",
    "    visitedNodes = np.array(visitedNodes).reshape(-1, 2).transpose()\n",
    "\n",
    "    # Multiply the path coordinates by reduction_coeff to go back to the original grid scale\n",
    "    path_final = path * reduction_coeff\n",
    "    \n",
    "    # Change axis to go back to the original coordinates\n",
    "    path_final[[0,1]] = path_final[[1,0]] \n",
    "    \n",
    "    # Replace the first and last node of the path to fit the true coordinates of the start and goal position\n",
    "    path_final[0][0] = robot_vector[0]\n",
    "    path_final[1][0] = robot_vector[1]\n",
    "    path_final[0][-1] = goal_center[0]\n",
    "    path_final[1][-1] = goal_center[1]\n",
    "    \n",
    "    # Displaying the map and the path information\n",
    "    cmap = colors.ListedColormap(['white', 'red'])\n",
    "    fig_astar, ax_astar = create_empty_plot(max_val_x, max_val_y)\n",
    "    ax_astar.imshow(occupancy_grid, cmap=cmap)\n",
    "    # Plot the best path found and the list of visited nodes\n",
    "    ax_astar.scatter(visitedNodes[1], visitedNodes[0], marker=\"o\", color = 'orange',label=('visited nodes'));\n",
    "    ax_astar.plot(path[1], path[0], marker=\"o\", color = 'blue',label=('path'));\n",
    "    ax_astar.scatter(start[1], start[0], marker=\"o\", color = 'green', s=200,label=('start'));\n",
    "    ax_astar.scatter(goal[1], goal[0], marker=\"o\", color = 'cyan', s=200,label=('goal'));\n",
    "    ax_astar.set_xlabel('x (pixels)')\n",
    "    ax_astar.set_ylabel('y (pixels)')\n",
    "    ax_astar.set_title('Global path found with A*')\n",
    "    ax_astar.legend();\n",
    "    \n",
    "    return path_final\n",
    "\n",
    "\n",
    "#--------------------------------------------------------------------------------#\n",
    "\n",
    "def filter_initialization():\n",
    "    \"\"\"\n",
    "    Initialize the various vectors and matrices requiered for filtering\n",
    "    \n",
    "    robot_vector: position (x and y) and orientation (theta) taken from the camera vision\n",
    "    \"\"\"\n",
    "    \n",
    "    global s_prev_est_a_posteriori, P_prev_est_a_posteriori, A, B, u, C, Q, R\n",
    "\n",
    "    ## Previous State A Posteriori Estimation Vector\n",
    "    # Vector representing the estimated state of the system at the previous time step\n",
    "    s_prev_est_a_posteriori = robot_vector\n",
    "       \n",
    "    ## Previous State A Posteriori Covariance Matrix\n",
    "    # Matrix representing the estimated precision of the previous estimated state (same as R)\n",
    "    P_prev_est_a_posteriori = np.array([[1, 0, 0], \n",
    "                                        [0, 1, 0], \n",
    "                                        [0, 0, 0.01]]) \n",
    "    \n",
    "    ## State Matrix\n",
    "    # Matrix defining how the system evolves from one time step to the next\n",
    "    A = np.array([[1, 0, 0], \n",
    "                  [0, 1, 0], \n",
    "                  [0, 0, 1]])\n",
    "        \n",
    "    ## Input Matrix \n",
    "    # Matrix describing the impact of the input on the state\n",
    "    B = np.array([[1, 0], \n",
    "                  [0, 1], \n",
    "                  [0, 0]]); \n",
    "        \n",
    "    ## Input Vector\n",
    "    # Vector representing control inputs applied to the system \n",
    "    u = np.array([0, 0])\n",
    "    \n",
    "    ## Output Matrix\n",
    "    # Matrix linking measurements to state\n",
    "    C = np.array([[1, 0, 0], \n",
    "                  [0, 1, 0], \n",
    "                  [0, 0, 1]])\n",
    "        \n",
    "    ## Process Noise Covariance Matrix\n",
    "    # Covariance matrix representing uncertainty in system dynamics\n",
    "    Q = np.array([[4, 0, 0], \n",
    "                  [0, 4, 0], \n",
    "                  [0, 0, 0.03]])\n",
    "    \n",
    "    ## Measurement Noise Covariance Matrix\n",
    "    # Matrix representing uncertainty of camera measurements\n",
    "    R = np.array([[1, 0, 0], \n",
    "                  [0, 1, 0], \n",
    "                  [0, 0, 0.01]])\n",
    "\n",
    "#--------------------------------------------------------------------------------#\n",
    "    \n",
    "def update_input(v_l,v_r,update_time, direction_rotation):\n",
    "    \"\"\"\n",
    "    Update the input vector and matrix\n",
    "    \n",
    "    v_l: robot x position deduced from the camera vision\n",
    "    v_r: robot y position deduced from the camera vision\n",
    "    update_time: robot theta orientation deduced from the camera vision\n",
    "    \"\"\"\n",
    "    \n",
    "    global B,u\n",
    "    \n",
    "    Thymio_to_mms = 0.349\n",
    "    mm_to_px = 100/137\n",
    "    \n",
    "    # Average translational speed\n",
    "    v = (v_r +v_l)/2 # Thymio speed (T)\n",
    "    v = v * Thymio_to_mms * mm_to_px # Speed in px/s (T -> mm/s -> px/s)\n",
    "\n",
    "    # Average rotational speed\n",
    "    w = (v_r -v_l)*Thymio_to_mms/robot_diameter # Angular speed in rad/s\n",
    "    \n",
    "    if (direction_rotation == TURN_RIGHT):\n",
    "        w = -w\n",
    "    \n",
    "    # Input vector\n",
    "    u = np.array([v, w]) \n",
    "    \n",
    "    # Angle variation\n",
    "    delta_theta = w * update_time\n",
    "    \n",
    "    # Input matrix\n",
    "    B = np.array([[np.cos(delta_theta + s_prev_est_a_posteriori[2])*update_time, 0],\n",
    "                  [-np.sin(delta_theta + s_prev_est_a_posteriori[2])*update_time, 0], \n",
    "                  [0, update_time]]); \n",
    "    \n",
    "#--------------------------------------------------------------------------------#\n",
    "\n",
    "def kalman_filter(s_prev_est_a_posteriori, P_prev_est_a_posteriori):\n",
    "    \"\"\"\n",
    "    Estimates the current state using the input sensor data and the previous state\n",
    "    \n",
    "    param s_prev_est_a_posteriori: previous state a posteriori estimation\n",
    "    param P_prev_est_a_posteriori: previous state a posteriori covariance\n",
    "    \n",
    "    return s_est_a_posteriori: new a posteriori state estimation\n",
    "    return P_est_a_posteriori: new a posteriori state covariance\n",
    "    \"\"\"\n",
    "    \n",
    "    ## Prediciton through the a priori estimate\n",
    "    # estimated mean of the state\n",
    "    s_est_a_priori = np.dot(A, s_prev_est_a_posteriori)+ np.dot(B, u);\n",
    "    path_apriori.append(s_prev_est_a_posteriori)\n",
    "    \n",
    "    # Estimated covariance of the state\n",
    "    P_est_a_priori = np.dot(A, np.dot(P_prev_est_a_posteriori, A.T)) + Q\n",
    "    \n",
    "    ## Update         \n",
    "    # m, C, and R for a posteriori estimate, depending on the detection of the camera\n",
    "    if camera_on == True:\n",
    "        m = robot_vector\n",
    "        path_camera.append(m)\n",
    "        # innovation / measurement residual\n",
    "        i = m - np.dot(C, s_est_a_priori);\n",
    "        # measurement prediction covariance\n",
    "        S = np.dot(C, np.dot(P_est_a_priori, C.T)) + R;     \n",
    "        # Kalman gain (tells how much the predictions should be corrected based on the measurements)\n",
    "        K = np.dot(P_est_a_priori, np.dot(C.T, np.linalg.inv(S)));\n",
    "        # a posteriori estimate\n",
    "        s_est_a_posteriori = s_est_a_priori + np.dot(K,i);\n",
    "        P_est_a_posteriori = P_est_a_priori - np.dot(K,np.dot(C, P_est_a_priori));\n",
    "    else:\n",
    "        K = 0 # Kalman gain is null because the camera can't deliver any data\n",
    "        # a posteriori estimate\n",
    "        s_est_a_posteriori = s_est_a_priori;\n",
    "        P_est_a_posteriori = P_est_a_priori;\n",
    "        \n",
    "    return s_est_a_posteriori, P_est_a_posteriori\n",
    "\n",
    "\n",
    "#--------------------------------------------------------------------------------#\n",
    "\n",
    "async def local_avoidance(sensor_prox, theta, dist):\n",
    "    \"\"\"\n",
    "    1a. Obstacle detected right in front: rotate left + go forward\n",
    "    1b. Obstacle detected near sides: rotate + go forward\n",
    "    2. Determine rotation left or right\n",
    "    3. Wall following of the obstacle\n",
    "    4. After obstacle clearance, turn back to initial direction\n",
    "    \"\"\"\n",
    "    # Global variables\n",
    "    global node, obstThrh\n",
    "    \n",
    "    # Calculate difference in Left & Right sensors\n",
    "    diff_lr = sensor_prox[0] - sensor_prox[4]\n",
    "    \n",
    "    # Object detected right in front\n",
    "    if sensor_prox[2] > obstThrh: \n",
    "        # Rotate to the left + go forward\n",
    "        await turn(pi/4)  \n",
    "        theta += pi/4\n",
    "        print(\"Front obstacle, turning right\")\n",
    "        await move_forward(100*dist)\n",
    "        await turn(-theta/2)\n",
    "        print(\"Turning back to initial direction\")\n",
    "        await move_forward(dist*100)\n",
    "        await turn(-theta/2)\n",
    "        await turn(theta/2)\n",
    "        return\n",
    "    else:\n",
    "        # Obstacle threshold\n",
    "        if max(sensor_prox) > obstThrh:\n",
    "            if (sensor_prox[0] + sensor_prox[1]) > (sensor_prox[3] + sensor_prox[4]):\n",
    "                await turn(pi/24)\n",
    "                theta += pi/24\n",
    "                await move_forward(dist*2)\n",
    "                print(\"Left obstacle, turning right\")\n",
    "                if (diff_lr) > 0:\n",
    "                    await turn(pi/30)\n",
    "                    theta += pi/30\n",
    "                    await move_forward(dist)\n",
    "                    print(\"Left wall following\")\n",
    "                else:\n",
    "                    await move_forward(dist)\n",
    "                    print(\"Not wall following\")\n",
    "            elif (sensor_prox[3] + sensor_prox[4]) > (sensor_prox[0] + sensor_prox[1]):\n",
    "                await turn(-pi/24)\n",
    "                theta -= pi/24\n",
    "                await move_forward(dist*2)\n",
    "                print(\"Right obstacle, turning left\")\n",
    "                if (diff_lr) < 0:\n",
    "                    await turn(-pi/30)\n",
    "                    theta -= pi/30\n",
    "                    await move_forward(dist)\n",
    "                    print(\"Right wall following\")\n",
    "                else:\n",
    "                    await move_forward(dist)\n",
    "                    print(\"Not wall following\")\n",
    "            else:\n",
    "                return  # Do nothing\n",
    "    \n",
    "    # Obstacle cleared, move forward for a while before turning back to initial direction & move forward slightly more\n",
    "    await move_forward(110*dist)\n",
    "    await turn(-theta*dist*3.5)\n",
    "    print(\"Turning back to initial direction\")\n",
    "    await move_forward(140*dist)\n",
    "    await turn(theta*dist*3)\n",
    "    await move_forward(dist)\n",
    "    \n",
    "    await motors_stop()\n",
    "    return\n",
    "\n",
    "\n",
    "#--------------------------------------------------------------------------------#\n",
    "\n",
    "# Local avoidance function with sensor values\n",
    "async def la_function(sensor_prox): \n",
    "    await node.wait_for_variables()\n",
    "    \n",
    "    while sum(sensor_prox[i] > obstThrh for i in range(0, 5)) > 0: \n",
    "        sensor_prox = node[\"prox.horizontal\"]\n",
    "        print(list(sensor_prox))\n",
    "        await local_avoidance(sensor_prox, angle_turned, distance_forward)\n",
    "        await client.sleep(0.2)\n",
    "        await motors_stop()\n",
    "        print(\"Completed Local Avoidance!\")\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "#--------------------------------------------------------------------------------#\n",
    "\n",
    "# Thymio connection\n",
    "async def connect_Thymio():\n",
    "    \"\"\"\n",
    "    Establish a connection with the Thymio if possible\n",
    "    \"\"\"\n",
    "    global node, client\n",
    "    try:\n",
    "        client = ClientAsync()\n",
    "        node = await asyncio.wait_for(client.wait_for_node(), timeout=2.0)\n",
    "        await node.lock()\n",
    "        print(\"Thymio connected\")\n",
    "\n",
    "    except asyncio.TimeoutError:\n",
    "        print(\"Thymio not connected: Timeout while waiting for node.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Thymio not connected: {str(e)}\")\n",
    "        \n",
    "        \n",
    "#--------------------------------------------------------------------------------#\n",
    "        \n",
    "# Thymio disconnection\n",
    "def disconnect_Thymio():\n",
    "    \"\"\"\n",
    "    Enable to disconnect the Thymio\n",
    "    \"\"\"\n",
    "    aw(node.stop())\n",
    "    aw(node.unlock())\n",
    "    print(\"Thymio disconnected\")    \n",
    "    \n",
    "\n",
    "#--------------------------------------------------------------------------------#\n",
    "\n",
    "# Thymio set motors speeds  \n",
    "async def set_speeds(left_speed, right_speed):\n",
    "    \"\"\"\n",
    "    Enable to set the speed of the Thymio's wheels\n",
    "    \"\"\"\n",
    "    global node\n",
    "    v = {\n",
    "        \"motor.left.target\":  [left_speed],\n",
    "        \"motor.right.target\": [right_speed],\n",
    "    }\n",
    "    await node.set_variables(v)\n",
    "    \n",
    "\n",
    "#--------------------------------------------------------------------------------#\n",
    "\n",
    "# Thymio motors stop     \n",
    "async def motors_stop():\n",
    "    \"\"\"\n",
    "    Stop the Thymio\n",
    "    \"\"\"\n",
    "    global node\n",
    "    v = {\n",
    "        \"motor.left.target\":  [0],\n",
    "        \"motor.right.target\": [0],\n",
    "    }\n",
    "    await node.set_variables(v)    \n",
    "\n",
    "\n",
    "\n",
    "#--------------------------------------------------------------------------------#\n",
    "\n",
    "# Thymio turns a specied angle\n",
    "async def turn(angle):\n",
    "    # Calculate the time needed to turn through the required angle\n",
    "    rotation_time = (abs(angle) / (2*np.pi)) * TIME_FULL_TURN\n",
    "\n",
    "    # Turn robot on itself\n",
    "    # Check the sign of angle\n",
    "    if np.sign(angle) > 0:\n",
    "        # If angle is positive, turn left\n",
    "        await set_speeds(-ROTATION_SPEED, ROTATION_SPEED)\n",
    "        left_or_right = TURN_LEFT\n",
    "    else:\n",
    "        # If angle is negative, turn right\n",
    "        await set_speeds(ROTATION_SPEED, -ROTATION_SPEED)\n",
    "        left_or_right = TURN_RIGHT\n",
    "\n",
    "    # Wait required time\n",
    "    time.sleep(rotation_time)\n",
    "    return rotation_time, left_or_right\n",
    "\n",
    "#--------------------------------------------------------------------------------#\n",
    "\n",
    "async def move_forward(distance_px):\n",
    "    # Calculate the time needed to travel the requested distance\n",
    "    \n",
    "    distance_mm = distance_px * px_to_mm\n",
    "    travel_time = (distance_mm) * TIME_PER_MM\n",
    "    \n",
    "    # Robot moves forward\n",
    "    await set_speeds(FORWARD_SPEED, FORWARD_SPEED)\n",
    "\n",
    "    # Wait for the necessary time\n",
    "    time.sleep(travel_time)\n",
    "    return travel_time\n",
    "\n",
    "\n",
    "#--------------------------------------------------------------------------------#\n",
    "\n",
    "async def reach_next_node(next_node, mode, estimated_pos):\n",
    "\n",
    "    # Vector between the estimated position and the next position in the global path\n",
    "    vector_next_node = np.array([0,0])  \n",
    "    vector_next_node[0] = path_final[0][next_node] - estimated_pos[0]\n",
    "    vector_next_node[1] = path_final[1][next_node] - estimated_pos[1] \n",
    "    \n",
    "    # Normalized angle between the estimated position and the next position in the global path\n",
    "    gamma = -math.atan2(vector_next_node[1], vector_next_node[0]) - estimated_pos[2]\n",
    "    gamma = (gamma + np.pi) % (2*np.pi) - np.pi\n",
    "    \n",
    "    # Distance separating the estimated position and the next position in the global path\n",
    "    path_next_node = np.array([path_final[0][next_node], path_final[1][next_node]])\n",
    "    path_current_node = np.array([estimated_pos[0], estimated_pos[1]])\n",
    "    d = np.linalg.norm(path_next_node - path_current_node)\n",
    "    \n",
    "    if(not mode):\n",
    "        if(abs(gamma) > ANGLE_THRESHOLD):\n",
    "            time_r, left_or_right = await turn(gamma)\n",
    "        else: \n",
    "            time_r = 0 \n",
    "            left_or_right = 1  \n",
    "        return time_r, left_or_right \n",
    "        \n",
    "    if (mode):\n",
    "        if( d > FORWARD_THRESHOLD):\n",
    "            time_f = await move_forward(d)\n",
    "            return time_f\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7536c87",
   "metadata": {},
   "source": [
    "<a id=\"capture-image\"></a>\n",
    "## Capture image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e4639e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize video capture from the first camera device\n",
    "video_capture = cv2.VideoCapture(0, cv2.CAP_DSHOW)\n",
    "# Initialize video capture from the first camera device\n",
    "# Set video frame width and height\n",
    "video_capture.set(cv2.CAP_PROP_FRAME_WIDTH, 1080)\n",
    "video_capture.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)\n",
    "\n",
    "# Create a window for display\n",
    "window_name = 'Robot Detection'\n",
    "cv2.namedWindow(window_name, cv2.WINDOW_NORMAL)\n",
    "\n",
    "# Resize the window (width, height)\n",
    "cv2.resizeWindow(window_name, 540, 360)  \n",
    "\n",
    "# Perform initial detection of obstacles and goal\n",
    "# This function will need the video_capture object and color bounds as inputs\n",
    "initial_frame, cropping_coords, contour_image, global_obstacle,goal_center, robot_pose = preprocess_image(video_capture, lower_blue_bound, upper_blue_bound)\n",
    "# Robot update frequency (10 Hz)\n",
    "update_rate = 0.01  # 10 times per second\n",
    "# Check if initial frame is not None\n",
    "if initial_frame is not None:\n",
    "    \n",
    "    try:\n",
    "        while True:\n",
    "            # Record start time of the loop\n",
    "            start_time = time.time()\n",
    "            # Read a new frame from the video capture\n",
    "            ret, frame = video_capture.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            # Process each frame for robot position and other tasks\n",
    "            robot_pose = process_image(frame, cropping_coords, contour_image, update_rate)\n",
    "            \n",
    "            # The coordinates of the robot are then projected to have the true coordinates\n",
    "            robot_vector = np.zeros(3)\n",
    "            robot_vector[0] = int(x_robot_projection_to_ground(robot_pose[0]))\n",
    "            robot_vector[1] = int(y_robot_projection_to_ground(robot_pose[1]))\n",
    "            robot_vector[2] = np.radians(robot_pose[2])\n",
    "            # Wait for a short period to maintain the update frequency and check for 'q' key press to quit\n",
    "            time_to_wait = max(int((start_time + update_rate - time.time()) * 1000), 1)\n",
    "            if cv2.waitKey(time_to_wait) & 0xFF == ord('q'):\n",
    "                break\n",
    "\n",
    "    except Exception as e:\n",
    "        # Print any errors that occur\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "    finally:\n",
    "        # Release the video capture and close all OpenCV windows when done\n",
    "        video_capture.release()\n",
    "        cv2.destroyAllWindows()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe4c734",
   "metadata": {},
   "source": [
    "<a id=\"main\"></a>\n",
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d762f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_final = path_planning(robot_vector, goal_center, global_obstacle)\n",
    "\n",
    "# Initialize video capture\n",
    "video_capture = cv2.VideoCapture(1, cv2.CAP_DSHOW)\n",
    "video_capture.set(cv2.CAP_PROP_FRAME_WIDTH, 1080)\n",
    "video_capture.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)\n",
    "video_capture.set(cv2.CAP_PROP_BUFFERSIZE,1)\n",
    "\n",
    "if not video_capture.isOpened():\n",
    "    raise IOError(\"Could not load the camera\")\n",
    "\n",
    "window_name = 'Robot Detection'\n",
    "cv2.namedWindow(window_name, cv2.WINDOW_NORMAL)\n",
    "\n",
    "# Resize the window (width, height)\n",
    "cv2.resizeWindow(window_name, 540, 360)\n",
    "\n",
    "await connect_Thymio()\n",
    "\n",
    "filter_initialization()\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "\n",
    "        await node.wait_for_variables()\n",
    "        sensor_prox = node[\"prox.horizontal\"]\n",
    "        await client.sleep(0.2)\n",
    "        while sum(sensor_prox[i] > obstThrh for i in range(0, 5)) > 0: \n",
    "            calculate_global_path = await la_function(sensor_prox)\n",
    "\n",
    "        await node.wait_for_variables()\n",
    "        sensor_gr = node[\"prox.ground.reflected\"]\n",
    "        await client.sleep(0.2)\n",
    "        while sum(sensor_gr[i] < GROUND_THRESHOLD for i in range(0, 2)): \n",
    "            calculate_global_path += 1\n",
    "            await motors_stop()\n",
    "\n",
    "        if(calculate_global_path):\n",
    "            await motors_stop()\n",
    "            path_final = path_planning(robot_vector, goal_center, global_obstacle)\n",
    "            calculate_global_path = False\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # Rotation\n",
    "        time_rotation, turn_left_or_right = await reach_next_node(next_node, ROTATION_MODE, s_prev_est_a_posteriori)\n",
    "        update_input(-ROTATION_SPEED, ROTATION_SPEED, time_rotation, turn_left_or_right)\n",
    "        ret, frame = video_capture.read()\n",
    "        robot_pose = process_image(frame, cropping_coords, contour_image, update_rate)\n",
    "\n",
    "        if camera_on:\n",
    "            robot_vector[0] = int(x_robot_projection_to_ground(robot_pose[0]))\n",
    "            robot_vector[1] = int(y_robot_projection_to_ground(robot_pose[1]))\n",
    "            robot_vector[2] = np.radians(robot_pose[2])\n",
    "        s_prev_est_a_posteriori, P_prev_est_a_posteriori = kalman_filter(s_prev_est_a_posteriori, P_prev_est_a_posteriori)\n",
    "\n",
    "        # Linear displacement\n",
    "        forward_time = await reach_next_node(next_node, FORWARD_MODE , s_prev_est_a_posteriori)\n",
    "        update_input(FORWARD_SPEED, FORWARD_SPEED, forward_time, 1)\n",
    "        ret, frame = video_capture.read()\n",
    "        robot_pose = process_image(frame, cropping_coords, contour_image, update_rate)\n",
    "\n",
    "        if camera_on:\n",
    "            robot_vector[0] = int(x_robot_projection_to_ground(robot_pose[0]))\n",
    "            robot_vector[1] = int(y_robot_projection_to_ground(robot_pose[1]))\n",
    "            robot_vector[2] = np.radians(robot_pose[2])\n",
    "        s_prev_est_a_posteriori, P_prev_est_a_posteriori = kalman_filter(s_prev_est_a_posteriori, P_prev_est_a_posteriori)\n",
    "\n",
    "        next_node += 1\n",
    "\n",
    "        if (next_node == path_final.shape[1]):\n",
    "            break\n",
    "\n",
    "    await motors_stop()\n",
    "    disconnect_Thymio()\n",
    "    path_followed = np.array(path_followed)\n",
    "    #print(path_followed)\n",
    "    print(\"Thymio ready !\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "\n",
    "finally:\n",
    "    # Release the capture when everything is finished\n",
    "    video_capture.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee9acd0",
   "metadata": {},
   "source": [
    "---\n",
    "<a id=\"conclusion\"></a>\n",
    "# Conclusion\n",
    "\n",
    "During this project, we have engineered and implemented various components, including vision, filtering, motion control, as well as global and local navigation. Our project is fully functional, and the Thymio is proficient in reaching its destination while adhering to the optimal path and circumventing obstacles through global navigation and local avoidance. We have successfully applied various techniques that we have learned over the semester in the *Basics of Mobile Robotics* course.\n",
    "\n",
    "This project has been intriguing both technically and organizationally. The creation of a comprehensive project within a relatively short timeframe compelled us to make compromises. Nevertheless, the project operates efficiently across a wide array of tests."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
