{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84588a6d-d18e-4ad6-8d06-e251d942fb7c",
   "metadata": {},
   "source": [
    "# Project Report - Groupe 28\n",
    "**Authors :** Celest Angela Tjong, Adrien Louis Baptiste Dupont, Luca Sidoti Pinto, Didier Henri Neuenschwander\n",
    "\n",
    "**Supervisors :** Prof. Francesco Mondada\n",
    "\n",
    "**Date** : 7/12/2023\n",
    "\n",
    "**[MICRO-452] : Basics of mobile robotics**\n",
    "\n",
    "---\n",
    "\n",
    "# Table of Contents\n",
    "* [1. Introduction](#introduction)\n",
    "* [2. Initialisation](#initialisation)\n",
    "* [3. Vision](#vision)\n",
    "    * [3.1. Subsection 1](#vision-subsection-1)\n",
    "    * [3.2. Subsection 2](#vision-subsection-2)\n",
    "* [4. Global Navigation](#global-navigation)\n",
    "    * [4.1 Path planning](#path-planning)\n",
    "    * [4.2 A* implementation](#A*)\n",
    "* [5. Filtering](#filtering)\n",
    "    * [5.1 State-space model](#state-space-model)\n",
    "    * [5.2 Kalman filter](#kalman-filter)\n",
    "* [6. Local Navigation](#local-navigation)\n",
    "* [7. Motion control](#motion-control)\n",
    "    * [7.1 Connection functions](#connection-functions)\n",
    "    * [7.2 Motion functions](#motion-functions)\n",
    "    * [7.3 Motion control function](#motion-control-function)\n",
    "* [8. Projet structure](#project-structure)   \n",
    "* [9. Runnable cells](#runnable-cells)\n",
    "    * [9.1 Functions declaration ](#functions-declaration)\n",
    "    * [9.2 Capture image](#capture-image)\n",
    "    * [9.3 Main](#main)\n",
    "* [10. Conclusion](#conclusion)\n",
    "\n",
    "---\n",
    "<a id=\"introduction\"></a>\n",
    "\n",
    "# Introduction\n",
    "\n",
    "This project, integrated into the MICRO-452 Basics of Mobile Robotics course, aims to combine various essential elements of mobile robotics: vision, global and local navigation, motion control, and filtering.\n",
    "\n",
    "Here is how the project was implemented. Initially, a webcam captures the image of the environment. The necessary information for operation, such as the robot’s position, the map, static obstacles, and the target’s position, are extracted in real-time using conventional image processing techniques. Subsequently, the A* algorithm is used to determine the optimal path. The robot follows this path, while the Kalman filter estimates the position during movement. Knowledge of the path and the robot’s position allows for control of its movement. If the Thymio detects an obstacle using its horizontal proximity sensors, local navigation takes over to avoid a collision. In addition, the robot can find its way back to the target in case of kidnapping. Finally, the Thymio is designed to reach the target even if the camera’s vision is obstructed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e8866c1",
   "metadata": {},
   "source": [
    "---\n",
    "<a id=\"initialisation\"></a>\n",
    "\n",
    "# Initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bda90134",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'numpy._DTypeMeta' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_19920/2551351534.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 57\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\cv2\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    179\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    180\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 181\u001b[1;33m \u001b[0mbootstrap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\cv2\\__init__.py\u001b[0m in \u001b[0;36mbootstrap\u001b[1;34m()\u001b[0m\n\u001b[0;32m    173\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    174\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0msubmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0m__collect_extra_submodules\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDEBUG\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 175\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[0m__load_extra_py_code_for_module\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"cv2\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msubmodule\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDEBUG\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    176\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mDEBUG\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Extra Python code for\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msubmodule\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"is loaded\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\cv2\\__init__.py\u001b[0m in \u001b[0;36m__load_extra_py_code_for_module\u001b[1;34m(base, name, enable_debug_print)\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[0mnative_module\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodule_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m         \u001b[0mpy_module\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodule_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0menable_debug_print\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\importlib\\__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m    125\u001b[0m                 \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    126\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 127\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    128\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\cv2\\typing\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNumpyVersion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;34m\"1.20.0\"\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mversion_info\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m9\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 69\u001b[1;33m     \u001b[0mNumPyArrayGeneric\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtyping\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAny\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgeneric\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     70\u001b[0m \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m     \u001b[0mNumPyArrayGeneric\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'numpy._DTypeMeta' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "# IMPORTS :\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import colors\n",
    "from PIL import Image\n",
    "import math\n",
    "import time\n",
    "from ipywidgets import interactive\n",
    "%matplotlib inline\n",
    "import asyncio\n",
    "from tdmclient import aw, ClientAsync\n",
    "\n",
    "\n",
    "# GLOBAL CONSTANTS :\n",
    "\n",
    "reduction_coeff = 25 # A METTRE EN MAJUSCULE\n",
    "ROTATION_COST = 1\n",
    "ROTATION_MODE = 0\n",
    "FORWARD_MODE = 1\n",
    "ANGLE_THRESHOLD = 0.0\n",
    "FORWARD_THRESHOLD = 1\n",
    "ROTATION_TIME_THRESHOLD = 1.2\n",
    "FORWARD_TIME_THRESHOLD = 100\n",
    "TURN_RIGHT=0\n",
    "TURN_LEFT=1\n",
    "ROTATION_SPEED = 100\n",
    "TIME_FULL_TURN = (8800/1000)\n",
    "\n",
    "# A PASSER EN CAPITALES\n",
    "reduction_coeff = 25\n",
    "Thymio_to_mms = 0.349\n",
    "px_to_mm = 137/100\n",
    "## Parameters for local navigation\n",
    "pi = math.pi\n",
    "angle_turned = 0\n",
    "distance_forward = 1\n",
    "obstThrh = 20     # high obstacle threshold to switch state from global nav to local\n",
    "\n",
    "# Define color thresholds in HSV\n",
    "lower_red_bound = np.array([120, 100, 70])\n",
    "upper_red_bound = np.array([255, 255, 255])\n",
    "lower_green_bound = np.array([60, 50, 100])\n",
    "upper_green_bound = np.array([100, 255, 255])\n",
    "lower_yellow_bound = np.array([0, 50, 120])\n",
    "upper_yellow_bound = np.array([40, 105, 255])\n",
    "lower_black_bound = np.array([0, 0, 0])\n",
    "upper_black_bound = np.array([255, 255, 130])\n",
    "lower_blue_bound = np.array([90, 80, 0])\n",
    "upper_blue_bound = np.array([105, 255, 255])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import cv2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f08bea99",
   "metadata": {},
   "source": [
    "---\n",
    "<a id=\"vision\"></a>\n",
    "## Vision\n",
    "\n",
    "https://docs.opencv.org/4.x/df/d9d/tutorial_py_colorspaces.html\n",
    "\n",
    "\n",
    "The first think to do is to preprocess the image for the colour object needed to be located. It is done by first creating a mask. The mask is created by specifying a range of colors (in HSV color space). Pixels within this color range are marked as 1 (or true), while all other pixels are marked as 0 (or false). it is created using `cv2.inRange` which filters out all colors except those within the specified `lower_color_bound` and `upper_color_bound`. This step isolated the specified color. \n",
    "Then the function `cv2.bitwise_and`, extract the area corresponding with the range of colour of the image given as input. It is done by comaparing each pixel of the image with the mask (same size of the image, comaparing with a logical &). \n",
    "Finally The color-filtered image is converted to grayscale using `cv2.cvtColor` because the subsequent edge detection step (Canny) requires a single-channel image. \n",
    "In order to detect the different coloured form, it is common to begin by using a Canny filter.\n",
    "\n",
    "### Function: `detect_color_circle`\n",
    "\n",
    "#### Purpose:\n",
    "The `detect_color_circle` function is designed to detect circles of a specific color in an image. It employs color filtering, Canny edge detection, and the Hough Circle Transform to achieve this.\n",
    "\n",
    "#### Process:\n",
    "\n",
    "\n",
    "\n",
    "2. **Color Masking**:\n",
    "   - A mask is created using `cv2.inRange` which filters out all colors except those within the specified `lower_color_bound` and `upper_color_bound`. This step isolates the regions of the specified color.\n",
    "\n",
    "3. **Mask Application**:\n",
    "   - The mask is then applied to the original image using `cv2.bitwise_and`. This step ensures that only the parts of the image with the desired color are retained for further processing.\n",
    "\n",
    "4. **Grayscale Conversion**:\n",
    "   - The color-filtered image is converted to grayscale using `cv2.cvtColor` because the subsequent edge detection step (Canny) requires a single-channel image.\n",
    "\n",
    "5. **Canny Edge Detection**:\n",
    "   - `cv2.Canny` is applied to detect edges in the image. It works by identifying areas in the image where sharp changes in intensity occur. The function takes two threshold values (here, 100 and 200) that determine the sensitivity of the edge detection. Edges that are found are used as input for the circle detection.\n",
    "\n",
    "6. **Hough Circle Transform**:\n",
    "   - `cv2.HoughCircles` is used to detect circles in the image. It operates on the principle of the Hough Transform, which is a feature extraction technique used in image analysis. The function detects circles by finding sets of edge points that form a circular shape.\n",
    "   - Parameters like `param1` (higher threshold of the two passed to the Canny edge detector), `param2` (threshold for center detection in the Hough Transform), `minRadius`, and `maxRadius` control the sensitivity and size of the circles to be detected.\n",
    "\n",
    "7. **Output**:\n",
    "   - If circles are detected, the function returns a list of tuples, each containing the `(x, y)` coordinates of the center of a circle and its radius. If no circles are found, it returns an empty list.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f2f8008",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to detect circles of a specific color\n",
    "def detect_color_circle(image, lower_color_bound, upper_color_bound):\n",
    "    # Convert to HSV color space\n",
    "    hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "    # Create a mask for the specified color\n",
    "    mask = cv2.inRange(hsv, lower_color_bound, upper_color_bound)\n",
    "\n",
    "    # Apply the mask to the original image\n",
    "    color_only = cv2.bitwise_and(image, image, mask=mask)\n",
    "\n",
    "    # Convert to grayscale for circle detection\n",
    "    gray = cv2.cvtColor(color_only, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Apply Canny edge detection to help with circle detection\n",
    "    edges = cv2.Canny(gray, 100, 200)\n",
    "\n",
    "    # Use Hough Transform to detect circles\n",
    "    circles = cv2.HoughCircles(edges, cv2.HOUGH_GRADIENT, 1, 20,\n",
    "                           param1=20, param2=15, minRadius=10, maxRadius=50)\n",
    "    \n",
    "    # If circles are detected, return the list of circles with x, y coordinates and radius\n",
    "    if circles is not None:\n",
    "        # Convert the (1, N, 3) array to (N, 3)\n",
    "        circles = np.uint16(np.around(circles[0, :]))          \n",
    "        return [(circle[0], circle[1], circle[2]) for circle in circles]\n",
    "    else:\n",
    "        return []\n",
    "    \n",
    "#--------------------------------------------------------------------------------#\n",
    "    \n",
    "def create_obstacle_mask(image, contours, kernel_size):\n",
    "    \"\"\"\n",
    "    Create a mask with zeros in the areas inside the dilated contours.\n",
    "\n",
    "    :param image: Input image.\n",
    "    :param contours: Contours to dilate and fill in the mask.\n",
    "    :param kernel_size: Size of the kernel used for dilation.\n",
    "    :return: Mask with zeros inside the dilated contours and ones elsewhere.\n",
    "    \"\"\"\n",
    "    # Create an empty mask of the same size as the image\n",
    "    h, w = image.shape[:2]\n",
    "    mask = np.ones((h, w), dtype=np.uint8)\n",
    "\n",
    "    # Perform dilation to increase the size of the black regions\n",
    "    kernel = np.ones((kernel_size, kernel_size), np.uint8)\n",
    "    for contour in contours:\n",
    "        # Create an individual mask for each contour\n",
    "        contour_mask = np.zeros((h, w), dtype=np.uint8)\n",
    "        cv2.fillPoly(contour_mask, [contour], 255)\n",
    "        contour_mask = cv2.dilate(contour_mask, kernel, iterations=1)\n",
    "        \n",
    "        # Combine the individual mask with the global mask\n",
    "        mask = cv2.bitwise_and(mask, cv2.bitwise_not(contour_mask))\n",
    "\n",
    "        #also add the contours\n",
    "        # Let's create a border around the image\n",
    "        border_size = 50\n",
    "        border_color = [0, 0, 0]  # Black border\n",
    "        # Use cv2.copyMakeBorder to add a border around the image\n",
    "        mask_with_border = cv2.copyMakeBorder(mask, border_size, border_size, border_size, border_size,\n",
    "                                           cv2.BORDER_CONSTANT, value=border_color)\n",
    "    \n",
    "    return mask_with_border\n",
    "\n",
    "\n",
    "# Now you have a mask with zeros in the obstacle areas and ones elsewhere\n",
    "# You can return this mask from your function or process it further as needed\n",
    "\n",
    "#--------------------------------------------------------------------------------#\n",
    "\n",
    "def detect_obstacle_contours(image, area_threshold, kernel_size):\n",
    "    \"\"\"\n",
    "    Detects and dilates obstacle contours in the given image.\n",
    "    :param image: Input image.\n",
    "    :param area_threshold: Area threshold for filtering contours.\n",
    "    :param kernel_size: Size of the kernel used for dilation.\n",
    "    :return: Image with obstacle contours drawn.\n",
    "    \"\"\"\n",
    "    hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "    mask_black = cv2.inRange(hsv, lower_black_bound, upper_black_bound)\n",
    "    contours, _ = cv2.findContours(mask_black, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    filtered_contours = [cnt for cnt in contours if cv2.contourArea(cnt) > area_threshold]\n",
    "    kernel = np.ones((kernel_size, kernel_size), np.uint8)\n",
    "    mask_dilated = cv2.dilate(mask_black, kernel, iterations=1)\n",
    "    dilated_contours, _ = cv2.findContours(mask_dilated, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    filtered_dilated_contours = [cnt for cnt in dilated_contours if cv2.contourArea(cnt) > area_threshold+10000]\n",
    "    contour_image = image.copy()\n",
    "    cv2.drawContours(contour_image, filtered_contours, -1, (0, 255, 0), 2)\n",
    "    cv2.drawContours(contour_image, filtered_dilated_contours, -1, (0, 0, 255), 2)\n",
    "    return contour_image, filtered_contours, filtered_dilated_contours\n",
    "\n",
    "#--------------------------------------------------------------------------------#\n",
    "\n",
    "def create_obstacle_matrix(image, dilated_contours):\n",
    "    height, width = image.shape[:2]\n",
    "    obstacle_matrix = np.ones((height, width), dtype=np.uint8)\n",
    "\n",
    "    for contour in dilated_contours:\n",
    "        # Remplir chaque contour dilaté avec 0 (obstacle)\n",
    "        cv2.fillPoly(obstacle_matrix, [contour], 0)\n",
    "\n",
    "    return obstacle_matrix\n",
    "\n",
    "#--------------------------------------------------------------------------------#\n",
    "\n",
    "def crop_roi_from_circles(image, circles):\n",
    "    if circles is not None and len(circles) >= 4:\n",
    "        # Assurez-vous que les points sont dans le format correct\n",
    "        points = np.array([circle[:2] for circle in circles], dtype=np.float32)\n",
    "\n",
    "        # Calcul de la boîte englobante\n",
    "        rect = cv2.boundingRect(points)\n",
    "\n",
    "        # Recadrage de l'image\n",
    "        x, y, w, h = rect\n",
    "        cropped_image = image[y:y+h, x:x+w]\n",
    "        return cropped_image,(x, y, w, h)\n",
    "    else:\n",
    "        print(\"Nombre insuffisant de cercles détectés ou format incorrect.\")\n",
    "        return image,None \n",
    "\n",
    "#--------------------------------------------------------------------------------#\n",
    "\n",
    "def find_rectangle_center(image, lower_colour_bound, upper_colour_bound, area_threshold=1000):\n",
    "    # Convertir l'image en espace de couleur HSV\n",
    "    hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "    # Créer un masque pour la couleur \n",
    "    mask = cv2.inRange(hsv, lower_colour_bound, upper_colour_bound)\n",
    "\n",
    "    # Trouver les contours dans le masque\n",
    "    contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    # Identifier le contour qui correspond au rectangle \n",
    "    for contour in contours:\n",
    "        if cv2.contourArea(contour) > area_threshold:\n",
    "            # Calculer la boîte englobante pour le contour\n",
    "            x, y, w, h = cv2.boundingRect(contour)\n",
    "\n",
    "            # Effacer le rectangle en le dessinant en blanc\n",
    "            cv2.rectangle(image, (x, y), (x + w, y + h), (255, 255, 255), -1)\n",
    "\n",
    "            # Calculer le centre du rectangle\n",
    "            center = (x + w // 2, y + h // 2)\n",
    "            return center\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e736490",
   "metadata": {},
   "source": [
    "In each frame, there are multiple elements that need to be displayed. The first category includes static elements such as the contours of obstacles and the goal point. Since these do not change over time, it is computationally more efficient to identify and locate them in the first frame, and then display them consistently in subsequent frames.\n",
    "\n",
    "On the other hand, elements related to the robot's localization must be determined in each frame. To gather information about its orientation and location, three circles are placed on the top of the robot. A green circle is located at the middle back, and two red circles are positioned on each side, with their centers aligned with the robot's ceeris. This arrangement facilitates the extraction of necessary information. By connecting the centers of the two red circles and creating a midpoint, a vector c thenan be formed by connecting this midpoint to the center of the green circ.It might also be feasible to use only two markers (circles), one at the back and one at the front, distinguished by their color. However, the marker at the front would hide the press button.\n",
    "\n",
    "### Graphe parallaxe\n",
    "\n",
    "In the context of our robotics project, we encountered a specific challenge related to detecting our robot's position. When the robot was perfectly positioned on a specific target, a sheet placed on the ground, it did not report the expected coordinates. We identified that this problem was due to parallax caused by the thickness of the robot. Due to its viewing angle, the aerial camera distorted the real position of the robot. To solve this problem, we projected the robot's position onto a 2D plane and used correction functions to adjust the coordinates. This allowed us to read the robot's position more accurately : \n",
    "\n",
    "For the x axis, we computated an equation fitting our measures : \n",
    "\n",
    "$$ f_{corr,x}(x) = 0.96x + 16.67$$\n",
    "\n",
    "![interactions](Images/graphe_1.jpg)\n",
    "\n",
    "For the y axis : \n",
    "\n",
    "$$ f_{corr,y}(x) = 0.95x + 14.29$$\n",
    "\n",
    "![interactions](Images/graphe_2.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea2c9e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def robot_info(frame):\n",
    "    global camera_on\n",
    "    red_circles = detect_color_circle(frame, lower_red_bound, upper_red_bound)\n",
    "    green_circles = detect_color_circle(frame, lower_green_bound, upper_green_bound)\n",
    "    \n",
    "    if red_circles and green_circles and len(red_circles) >= 2:\n",
    "        # Calculate robot's position and orientation vector\n",
    "        # Calculate the midpoint between the centers of the red circles\n",
    "        midpoint = ((red_circles[0][0] + red_circles[1][0]) // 2,\n",
    "        (red_circles[0][1] + red_circles[1][1]) // 2)\n",
    "        # Calculate the directional vector\n",
    "        direction = np.array([midpoint[0] - green_circles[0][0], midpoint[1] - green_circles[0][1]])\n",
    "\n",
    "        # Normalize and extend the vector\n",
    "        length = 30  # Additional length\n",
    "        direction = direction / np.linalg.norm(direction) * length\n",
    "\n",
    "        # Calculate the new endpoint\n",
    "        new_endpoint = (int(green_circles[0][0] + direction[0]), int(green_circles[0][1] + direction[1]))\n",
    "\n",
    "        # Draw the extended arrow\n",
    "        cv2.arrowedLine(frame, new_endpoint, green_circles[0][:2], (0, 0, 0), 3)\n",
    "\n",
    "        # Calculate the angle of orientation with respect to the x-axis\n",
    "        dx =   midpoint[0] - green_circles[0][0]\n",
    "        dy =   midpoint[1] - green_circles[0][1]\n",
    "        angle = math.atan2(dy, dx)\n",
    "        angle_degrees = math.degrees(angle)\n",
    "        if(angle_degrees>= 0):\n",
    "            angle_degrees =180 - angle_degrees\n",
    "        elif (angle_degrees < 0):\n",
    "            angle_degrees = -(180 + angle_degrees)\n",
    "        robot_vector = (midpoint[0], midpoint[1], np.radians(angle_degrees)) #information of the robot\n",
    "        #print(\"ANGLE \",robot_vector[2])\n",
    "        # Optionally, display the angle\n",
    "        cv2.putText(frame, f'Angle: {angle_degrees:.2f} degrees', (10, 30), \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n",
    "\n",
    "        cv2.putText(frame, f'Midpoint: ({midpoint[0]}, {midpoint[1]})', (10, 70),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n",
    "\n",
    "        cv2.circle(frame, midpoint,10, (255, 0, 0), -1)\n",
    "       # cv2.circle(frame, (x, y), r, (0, 255, 255), 3)\n",
    "        camera_on = True\n",
    "        return midpoint[0], midpoint[1], angle_degrees#information of the robot\n",
    "\n",
    "    else:\n",
    "        camera_on = False\n",
    "        print(\"PASSAGE A FALSE\")\n",
    "        return None,None,None\n",
    "    \n",
    "#--------------------------------------------------------------------------------#\n",
    "    \n",
    "def preprocess_image(video_capture, lower_blue_bound, upper_blue_bound):\n",
    "    ret, initial_frame = video_capture.read()\n",
    "    if ret:\n",
    "        #initial_frame, cropping_coords = crop_largest_white_area(initial_frame, 200000)\n",
    "        blue_circles = detect_color_circle(initial_frame, lower_blue_bound, upper_blue_bound)\n",
    "        initial_frame ,cropping_coords= crop_roi_from_circles(initial_frame, blue_circles)\n",
    "        goal_center = find_rectangle_center(initial_frame, lower_blue_bound, upper_blue_bound,2000)\n",
    "        if goal_center:\n",
    "            # Store the coordinates of the detected yellow circle\n",
    "            yellow_circle_coords = goal_center  # center est déjà un tuple (x, y)\n",
    "            radius = 10  # Rayon du cercle, vous pouvez ajuster cette valeur\n",
    "            color = (0, 255, 255)  # Couleur jaune en BGR\n",
    "            cv2.circle(initial_frame, yellow_circle_coords, radius, color, 3)\n",
    "        robot_vector = robot_info(initial_frame)\n",
    "        if robot_vector[0] and robot_vector[1]:\n",
    "            cv2.circle(initial_frame, (robot_vector[0], robot_vector[1]),80, (255, 255, 255), -1)    \n",
    "        contour_image = detect_obstacle_contours(initial_frame, 2000, 80)\n",
    "        global_obstacle = create_obstacle_matrix(initial_frame,contour_image[2])\n",
    "        # Dessiner le cercle\n",
    "        initial_frame = np.flipud(initial_frame)\n",
    "        plt.imshow(cv2.cvtColor(contour_image[0], cv2.COLOR_BGR2RGB))\n",
    "        plt.title('Initial Contours')\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "        return initial_frame, cropping_coords, contour_image, global_obstacle,goal_center, robot_vector\n",
    "    else:\n",
    "        return None, None, None, None, None, None\n",
    "    \n",
    "#--------------------------------------------------------------------------------#\n",
    "\n",
    "def process_image(frame, cropping_coords, contour_image, update_rate):\n",
    "    if cropping_coords is not None:\n",
    "        x, y, w, h = cropping_coords\n",
    "        frame = frame[y:y+h, x:x+w]\n",
    "    # Detect red and green circles\n",
    "    robot_vector = robot_info(frame)\n",
    "    if  robot_vector is not None:\n",
    "        cv2.drawContours(frame, contour_image[1], -1, (0, 255, 0), 2)\n",
    "        cv2.drawContours(frame, contour_image[2], -1, (0, 0, 255), 2)\n",
    "\n",
    "        # Display the resulting frame\n",
    "        cv2.imshow('Robot Detection', frame)\n",
    "    return robot_vector\n",
    "\n",
    "#--------------------------------------------------------------------------------#\n",
    "\n",
    "def x_robot_projection_to_ground(x_pos_robot):\n",
    "    return round(0.96*x_pos_robot+16.67)\n",
    "\n",
    "#--------------------------------------------------------------------------------#\n",
    "\n",
    "def y_robot_projection_to_ground(y_pos_robot):\n",
    "    return round(0.95*y_pos_robot+14.29) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184e1997",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A DEGAGER AUTRE PART\n",
    "\n",
    "video_capture = cv2.VideoCapture(1, cv2.CAP_DSHOW)\n",
    "\n",
    "video_capture.set(cv2.CAP_PROP_FRAME_WIDTH, 1080)\n",
    "video_capture.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)\n",
    "window_name = 'Robot Detection'\n",
    "cv2.namedWindow(window_name, cv2.WINDOW_NORMAL)\n",
    "\n",
    "# Resize the window (width, height)\n",
    "cv2.resizeWindow(window_name, 540, 360)  \n",
    "\n",
    "# Initial detection of obstacles and goal\n",
    "initial_frame, cropping_coords, contour_image, global_obstacle,goal_center, robot_pose = preprocess_image(video_capture, lower_blue_bound, upper_blue_bound)\n",
    "# Robot update frequency (10 Hz)\n",
    "update_rate = 0.01  # 10 times per second\n",
    "if initial_frame is not None:\n",
    "    try:\n",
    "        while True:\n",
    "            start_time = time.time()\n",
    "            ret, frame = video_capture.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            robot_pose = process_image(frame, cropping_coords, contour_image, update_rate)\n",
    "\n",
    "            # Pause to maintain the update frequency\n",
    "            time_to_wait = max(int((start_time + update_rate - time.time()) * 1000), 1)\n",
    "            if cv2.waitKey(time_to_wait) & 0xFF == ord('q'):\n",
    "                break\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "    finally:\n",
    "        # Release the capture when everything is finished\n",
    "        video_capture.release()\n",
    "        cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "332faefa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A DEGAGER AUTRE PART\n",
    "\n",
    "#print(goal_center)\n",
    "#print(robot_pose)\n",
    "robot_vector = np.zeros(3)\n",
    "robot_vector[0] = int(x_robot_projection_to_ground(robot_pose[0]))\n",
    "robot_vector[1] = int(y_robot_projection_to_ground(robot_pose[1]))\n",
    "robot_vector[2] = np.radians(robot_pose[2])\n",
    "print(robot_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdad22f3",
   "metadata": {},
   "source": [
    "---\n",
    "<a id=\"global-navigation\"></a>\n",
    "# Global navigation\n",
    "\n",
    "<a id=\"path-planning\"></a>\n",
    "## Path planning\n",
    "\n",
    "The path search is transformed into a graph search. A problem encountered is the computational time needed for a grid the size of the image. To reduce this duration a size reduction coefficient is used to downsize the original grid. The value of this coefficient is chosen to have the best trade-off between computational time and precision.\n",
    "\n",
    "| `reduction_coeff` | Computation time |\n",
    "|----------|----------|\n",
    "| 10 | 9.599 |\n",
    "| 15 | 2.298 |\n",
    "| 20 | 0.513 |\n",
    "| 25 | 0.193 |\n",
    "| 30 | 0.099 |\n",
    "\n",
    "The solution with the best trade-off is a reduction factor of 25, allowing an average computational time of 0.193s while keeping acceptable precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a37cf6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def  create_downsized_grid(global_obstacle):\n",
    "    \"\"\" \n",
    "    Creation of the downsized grid occupation_grid with a scale of reduction_coeff\n",
    "    \n",
    "    param : \n",
    "        - global_obstacle : grid containing the obstacle positions with original size\n",
    "    return : \n",
    "        - occupancy_grid: Downsized grid with averaged obstacle positions\n",
    "    \"\"\"\n",
    "\n",
    "    # Invert the data to have a value of 0 for free cells and 1 for occupied cells.\n",
    "    # global_obstacle = np.logical_not( global_obstacle )  \n",
    "    max_val_x_init = global_obstacle.shape[0]\n",
    "    max_val_y_init = global_obstacle.shape[1]\n",
    "\n",
    "    # The size of the new grid is calculated from the size of the original grid global_obstacle\n",
    "    max_val_x = int(max_val_x_init / reduction_coeff)\n",
    "    max_val_y = int(max_val_y_init / reduction_coeff)\n",
    "\n",
    "    # Creation of the downsized grid with\n",
    "    occupancy_grid = np.zeros((max_val_x, max_val_y), dtype=int)\n",
    "\n",
    "    # Inspection of squares (of size reduction_coeff*reduction_coeff) in the original grid\n",
    "    for i in range (max_val_x):\n",
    "        for j in range (max_val_y):\n",
    "            sum_pixels = 0\n",
    "\n",
    "            for k in range (reduction_coeff):\n",
    "\n",
    "                # Verification that the indexs indice_x and indice_y are in the grid range\n",
    "                indice_x = int(i * reduction_coeff - reduction_coeff/2 + k)\n",
    "                if (indice_x < 0):\n",
    "                    indice_x = 0\n",
    "                elif (indice_x > (max_val_x_init - 1)):\n",
    "                    indice_x = max_val_x_init -1\n",
    "\n",
    "                indice_y = int(j * reduction_coeff - reduction_coeff/2 + k)\n",
    "                if (indice_y < 0):\n",
    "\n",
    "                    indice_y = 0\n",
    "                elif (indice_y > (max_val_y_init - 1)):\n",
    "                    indice_y = max_val_y_init -1\n",
    "\n",
    "                # The sum of all values present in the inspected square is stored in sum_pixels\n",
    "                sum_pixels = sum_pixels + global_obstacle[indice_x][indice_y]\n",
    "\n",
    "\n",
    "            # If an obstacle is found in the square, a 1 is placed in the corresponding position in the new grid\n",
    "            if sum_pixels == 0:\n",
    "                occupancy_grid[i][j] = 0\n",
    "            else:\n",
    "                occupancy_grid[i][j] = 1\n",
    "\n",
    "    return occupancy_grid\n",
    "\n",
    "#--------------------------------------------------------------------------------#\n",
    "\n",
    "def create_empty_plot(max_val_x, max_val_y):\n",
    "    \"\"\"\n",
    "    Creation of a figure to plot a matrix of max size max_val \n",
    "    \n",
    "    param : \n",
    "        - max_val_x : maximum dimension of the matrix on x axis\n",
    "        - max_val_y : maximum dimension of the matrix on y axis\n",
    "    return:\n",
    "        -  fig : general figure\n",
    "        -  ax : axis information\n",
    "    \"\"\"\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(7,7))\n",
    "    \n",
    "    major_ticks = np.arange(0, max_val_y+1, 5)\n",
    "    minor_ticks = np.arange(0, max_val_y+1, 1)\n",
    "    ax.set_xticks(major_ticks)\n",
    "    ax.set_xticks(minor_ticks, minor=True)\n",
    "    ax.set_yticks(major_ticks)\n",
    "    ax.set_yticks(minor_ticks, minor=True)\n",
    "    ax.grid(which='minor', alpha=0.2)\n",
    "    ax.grid(which='major', alpha=0.5)\n",
    "    \n",
    "    # Definition of axis limits with the information of the matrix size\n",
    "    ax.set_ylim([max_val_x,-1])\n",
    "    ax.set_xlim([-1,max_val_y])\n",
    "    ax.grid(True)\n",
    "    \n",
    "    return fig, ax\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed6a06a2",
   "metadata": {},
   "source": [
    "<a id=\"path-planning\"></a>\n",
    "## A* implementation\n",
    "\n",
    "The path planning for the Thymio is made using a hybrid A* algorithm. This ensures an optimised search by using the distance to goal to choose the nodes to explore. The formula used is the following : \n",
    "\n",
    "$$ f(n) = g(n) + h(n) $$\n",
    "\n",
    "Here f(n) is the function we try to optimise, with g(n) being the motion cost and h(n) the heuristic function. The motion cost has here been tailored to the Thymio's mechanism, as it takes into account on one side the cost of forward motion with `deltacost` and on the other side the rotational cost with `rotational_cost`. Adding the rotational cost is key to avoid repetitive direction changes.\n",
    "\n",
    "The `path_final` matrix is structured such that the y-coordinates are on the first row and the x-coordinates on the second. Consequently, an inversion of these two rows is required prior to executing the robot's movement.\n",
    "\n",
    "Moreover, the downsizing operation of the original grid introduces inaccuracies regarding the initial and final position of the robot. To mitigate this limitation, the exact coordinates of these two specific positions are preferred over the values returned by the A* algorithm when constructing the movement array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23828a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_movements_8n():\n",
    "    \"\"\"\n",
    "    Get all possible 8-connectivity movements : up, down, left, right and the 4 diagonals.\n",
    "    \n",
    "    return : \n",
    "        - list of movements with cost [(dx, dy, movement_cost)]\n",
    "    \"\"\"\n",
    "    s2 = math.sqrt(2)\n",
    "    return [(1, 0, 1.0),\n",
    "            (0, 1, 1.0),\n",
    "            (-1, 0, 1.0),\n",
    "            (0, -1, 1.0),\n",
    "            (1, 1, s2),\n",
    "            (-1, 1, s2),\n",
    "            (-1, -1, s2),\n",
    "            (1, -1, s2)]\n",
    "\n",
    "#--------------------------------------------------------------------------------#\n",
    "\n",
    "def reconstruct_path(cameFrom, current):\n",
    "    \"\"\"\n",
    "    Reconstructs the path from start node to the current node\n",
    "    \n",
    "    param :\n",
    "        - cameFrom: map (dict) containing for each node the node immediately \n",
    "                     preceding it on the cheapest path currently known.\n",
    "        - param current : current node (x, y)\n",
    "        \n",
    "    return : \n",
    "        - total_path : list of nodes from start to current node\n",
    "    \"\"\"\n",
    "    \n",
    "    # The first item is the current node\n",
    "    total_path = [current]\n",
    "    \n",
    "    while current in cameFrom.keys():\n",
    "        # Add the previous node to the start of the list\n",
    "        total_path.insert(0, cameFrom[current]) \n",
    "        current = cameFrom[current]\n",
    "    return total_path\n",
    "\n",
    "#--------------------------------------------------------------------------------#\n",
    "\n",
    "def A_Star(start, goal, h, coords, occupancy_grid):\n",
    "    \"\"\"\n",
    "    Implementation of the A* algorithm, finding the cheapest path from start to goal \n",
    "    \n",
    "    param :\n",
    "        - start : start node (x, y)\n",
    "        - goal: goal node (x, y)\n",
    "        - h : heuristic function (distance to goal)\n",
    "        - coords : coordinates in the grid\n",
    "        - occupancy_grid: downsized grid map with obstacle information\n",
    "    \n",
    "    return : \n",
    "        - a tuple containing the path distance and the path information in an array of indexs\n",
    "    \"\"\"\n",
    "    \n",
    "    # Size of the downscaled grid\n",
    "    max_val_x = occupancy_grid.shape[0]\n",
    "    max_val_y = occupancy_grid.shape[1]\n",
    "    \n",
    "    # Check if the start and goal are within the limits of the grid's size\n",
    "    for point in [start, goal]:\n",
    "        for coord in point:\n",
    "            assert coord>=0 and coord<max_val_y, \"start or end goal not contained in the map\"\n",
    "    \n",
    "    # Check if start and goal nodes are situated on an obstacle\n",
    "    if occupancy_grid[start[0], start[1]]:\n",
    "        raise Exception('Start node is not traversable')\n",
    "\n",
    "    if occupancy_grid[goal[0], goal[1]]:\n",
    "        raise Exception('Goal node is not traversable')\n",
    "    \n",
    "    # Information on the possible movements\n",
    "    movements = _get_movements_8n()\n",
    "    \n",
    "    # --------------------------------------------------------------#\n",
    "    #                  A* ALGORITHM IMPLEMENTATION                  #\n",
    "    # --------------------------------------------------------------#\n",
    "    \n",
    "    # The set of visited nodes, which are the starting points for neighbor exploration\n",
    "    # The start node is the only one known yet.\n",
    "    openSet = [start]\n",
    "    \n",
    "    # The set of visited nodes that no longer need to be expanded.\n",
    "    closedSet = []\n",
    "\n",
    "    # map containing for each node the node immediately preceding it on the cheapest path currently known.\n",
    "    cameFrom = dict()\n",
    "\n",
    "    # For node n, gScore[n] is the cost of the cheapest path from start to n currently known.\n",
    "    gScore = dict(zip(coords, [np.inf for x in range(len(coords))]))\n",
    "    gScore[start] = 0\n",
    "\n",
    "    # For node n, fScore[n] := gScore[n] + h(n). map with default value of Infinity\n",
    "    fScore = dict(zip(coords, [np.inf for x in range(len(coords))]))\n",
    "    fScore[start] = h[start]\n",
    "    \n",
    "\n",
    "    # While there are still cells to explore\n",
    "    while openSet != []:\n",
    "        \n",
    "        # The node in openSet having the lowest fScore[] value\n",
    "        fScore_openSet = {key:val for (key,val) in fScore.items() if key in openSet}\n",
    "        current = min(fScore_openSet, key=fScore_openSet.get)\n",
    "        del fScore_openSet\n",
    "        \n",
    "        # If the goal is reached, reconstruct and return the obtained path\n",
    "        if current == goal:\n",
    "            return reconstruct_path(cameFrom, current), closedSet\n",
    "\n",
    "        openSet.remove(current)\n",
    "        closedSet.append(current)\n",
    "        \n",
    "        # For each neighbor of the current node \n",
    "        for dx, dy, deltacost in movements:\n",
    "            \n",
    "            neighbor = (current[0]+dx, current[1]+dy)\n",
    "            \n",
    "            # If the node is not in the map, skip\n",
    "            if (neighbor[0] >= occupancy_grid.shape[0]) or (neighbor[1] >= occupancy_grid.shape[1]) or (neighbor[0] < 0) or (neighbor[1] < 0):\n",
    "                continue\n",
    "            \n",
    "            # If the node is occupied or has already been visited, skip\n",
    "            if (occupancy_grid[neighbor[0], neighbor[1]]) or (neighbor in closedSet): \n",
    "                continue\n",
    "            \n",
    "            # Computation of the rotational cost by comparing the previous position, the current one and the next one\n",
    "            if(not (current == start)):\n",
    "                vector_prev = ([current[0] - (cameFrom[current])[0], current[1] - (cameFrom[current])[1]]) \n",
    "                vector_next = ([neighbor[0] - current[0], neighbor[1] - current[1]]) \n",
    "                angle = np.arccos(np.dot(vector_prev, vector_next) / (np.linalg.norm(vector_prev) * np.linalg.norm(vector_next)))\n",
    "                rotation_cost = angle * ROTATION_COST\n",
    "            else:\n",
    "                rotation_cost = 0\n",
    "                \n",
    "            # The movement cost to the neighbor is the linear movement with deltacost and the rotation with rotational_cost\n",
    "            # Tentative_gScore is the distance from start to the neighbor through current\n",
    "            tentative_gScore = gScore[current] + deltacost + rotation_cost\n",
    "            \n",
    "            if neighbor not in openSet:\n",
    "                openSet.append(neighbor)\n",
    "            \n",
    "            # If the path to the neighbor is better than any previous one, keep it\n",
    "            if tentative_gScore < gScore[neighbor]:\n",
    "                cameFrom[neighbor] = current\n",
    "                gScore[neighbor] = tentative_gScore\n",
    "                fScore[neighbor] = gScore[neighbor] + h[neighbor]\n",
    "\n",
    "    # Open set is empty but goal was never reached\n",
    "    print(\"No path found to goal\")\n",
    "    return [], closedSet\n",
    "\n",
    "#--------------------------------------------------------------------------------#\n",
    "\n",
    "def path_planning(robot_vector, goal_center, global_obstacle):\n",
    "    \"\"\"\n",
    "    This function calls the other path planning functions to compute the cheapest path\n",
    "    \n",
    "    param :\n",
    "        - robot_vector : current position of the Thymio robot  \n",
    "        - goal_center : center position of the goal\n",
    "        - global_obstacle : original grid with obstacle information\n",
    "    \n",
    "    return :\n",
    "        - path_final : list of coordinates forming the cheapest path\n",
    "    \"\"\"\n",
    "    \n",
    "    # Generate the downsized obstacle grid\n",
    "    occupancy_grid = create_downsized_grid(global_obstacle)\n",
    "    \n",
    "    # Size of the downscaled grid\n",
    "    max_val_x = occupancy_grid.shape[0]\n",
    "    max_val_y = occupancy_grid.shape[1]\n",
    "    \n",
    "    # Define the start and end goal while changing axis to use matrix coordinates\n",
    "    start = (int(robot_vector[1]/reduction_coeff), int(robot_vector[0]/reduction_coeff))\n",
    "    goal = (int(goal_center[1]/reduction_coeff), int(goal_center[0]/reduction_coeff))\n",
    "\n",
    "    # List of all coordinates in the grid\n",
    "    w,z = np.mgrid[0:max_val_x:1, 0:max_val_y:1]\n",
    "    pos = np.empty(w.shape + (2,))\n",
    "    pos[:, :, 0] = w; pos[:, :, 1] = z\n",
    "    pos = np.reshape(pos, (w.shape[0]*w.shape[1], 2))\n",
    "    coords = list([(int(w[0]), int(w[1])) for w in pos])\n",
    "    \n",
    "    # Define the heuristic, here = distance to goal ignoring obstacles\n",
    "    h = np.linalg.norm(pos - goal, axis=-1)\n",
    "    h = dict(zip(coords, h))\n",
    "\n",
    "    # Run the A* algorithm    \n",
    "    path, visitedNodes = A_Star(start, goal, h, coords, occupancy_grid)\n",
    "    \n",
    "    # Change axis to plot with same coordinates as the grid\n",
    "    path = np.array(path).reshape(-1, 2).transpose()\n",
    "    visitedNodes = np.array(visitedNodes).reshape(-1, 2).transpose()\n",
    "\n",
    "    # Multiply the path coordinates by reduction_coeff to go back to the original grid scale\n",
    "    path_final = path * reduction_coeff\n",
    "    \n",
    "    # Change axis to go back to the original coordinates\n",
    "    path_final[[0,1]] = path_final[[1,0]] \n",
    "    \n",
    "    # Replace the first and last node of the path to fit the true coordinates of the start and goal position\n",
    "    path_final[0][0] = robot_vector[0]\n",
    "    path_final[1][0] = robot_vector[1]\n",
    "    path_final[0][-1] = goal_center[0]\n",
    "    path_final[1][-1] = goal_center[1]\n",
    "    \n",
    "    # Displaying the map and the path information\n",
    "    cmap = colors.ListedColormap(['white', 'red'])\n",
    "    fig_astar, ax_astar = create_empty_plot(max_val_x, max_val_y)\n",
    "    ax_astar.imshow(occupancy_grid, cmap=cmap)\n",
    "    # Plot the best path found and the list of visited nodes\n",
    "    ax_astar.scatter(visitedNodes[1], visitedNodes[0], marker=\"o\", color = 'orange');\n",
    "    ax_astar.plot(path[1], path[0], marker=\"o\", color = 'blue');\n",
    "    ax_astar.scatter(start[1], start[0], marker=\"o\", color = 'green', s=200);\n",
    "    ax_astar.scatter(goal[1], goal[0], marker=\"o\", color = 'purple', s=200);\n",
    "    \n",
    "    return path_final"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc1fac0",
   "metadata": {},
   "source": [
    "---\n",
    "<a id=\"filtering\"></a>\n",
    "# Filtering\n",
    "\n",
    "\n",
    "The localization of the Thymio robot is performed using a Kalman filter. This filtering method is well suited to estimating the position and orientation of a mobile robot from noisy or incomplete measurements. The design of the filter in this project is based on using the position ($x, y$) and orientation ($\\theta$) provided by the camera as measurements. In addition, the speed of the robot, provided by the wheel speed sensors ($v_r, v_l$), is used as a prediction. In short, the Kalman filter merges a prediction of the system's future state with a measurement of that state to estimate position probabilistically.\n",
    "\n",
    "<a id=\"state-space-model\"></a>\n",
    "## State-space model\n",
    "\n",
    "### Prediction\n",
    "\n",
    "To estimate the robot's future position, a state-space model needs to be developed: \n",
    "\n",
    "$$\\hat{s}_{a\\_priori}^{t+1} = A \\cdot \\hat{s}_{a\\_posteriori}^{t} + B \\cdot u^{t} + q^t$$\n",
    "\n",
    "The prediction of the future state is referred to as $\\hat{s}_{a\\_priori}^{t+1}$, i.e. the a priori estimate at time t+1. Since the state of the system is defined by its position ($x, y$) and orientation ($\\theta$), this gives: \n",
    "\n",
    "$$\\hat{s}_{a\\_priori}^{t+1} = \\begin{pmatrix}\n",
    "\\hat{x}_{a\\_priori}^{t+1} \\\\\\\\\n",
    "\\hat{y}_{a\\_priori}^{t+1} \\\\\\\\\n",
    "\\hat{\\theta}_{a\\_priori}^{t+1}\n",
    "\\end{pmatrix}$$\n",
    "\n",
    "The current state corresponds to the term $\\hat{s}_{a\\_posteriori}^{t}$, which is the a posteriori estimate at time t. In the same way as above, this gives:\n",
    "\n",
    "$$\\hat{s}_{a\\_posteriori}^{t} = \\begin{pmatrix}\n",
    "\\hat{x}_{a\\_posteriori}^{t} \\\\\\\\\n",
    "\\hat{y}_{a\\_posteriori}^{t} \\\\\\\\\n",
    "\\hat{\\theta}_{a\\_posteriori}^{t}\n",
    "\\end{pmatrix}$$\n",
    "\n",
    "The system input at time t is represented by the vector $u^{t}$. This is made up of two terms: translational speed ($v$) and rotational speed ($\\omega$). \n",
    "\n",
    "$$u^t = \\begin{pmatrix}\n",
    "v \\\\\\\\\n",
    "\\omega\n",
    "\\end{pmatrix} $$\n",
    "\n",
    "These are defined on the basis of the speeds measured by the wheel speed sensors, i.e. the right ($v_r$) and left ($v_l$) speeds, and the spacing between the two wheels ($e$).\n",
    "\n",
    "$$ v = \\cfrac{v_r + v_l}{2} \\qquad\\qquad \\omega = \\cfrac{v_r-v_l}{e} $$ \n",
    "\n",
    "Matrix A characterizes the evolution of the system state, while matrix B describes the impact of the input on the future state. An odometry-based approach allows us to determine these two matrices by considering a very short time interval ($\\delta t$). During this time interval, the robot rotates by $\\delta \\theta = \\omega \\cdot \\delta t$. Knowing this, and referring to the diagram below, the following system of equations can be established: \n",
    "\n",
    "$$\\begin{equation}\n",
    "\\begin{cases}\n",
    "\\hat{x}_{a\\_priori}^{t+1} = \\hat{x}_{a\\_posteriori}^{t} + v \\cdot \\cos\\left(\\hat{\\theta}_{a\\_posteriori}^{t} + \\delta \\theta^t \\right) \\cdot \\delta t \\\\\n",
    "\\hat{y}_{a\\_priori}^{t+1} = \\hat{y}_{a\\_posteriori}^{t} + v \\cdot \\sin\\left(\\hat{\\theta}_{a\\_posteriori}^{t} + \\delta \\theta^t \\right) \\cdot \\delta t \\\\\n",
    "\\hat{\\theta}_{a\\_priori}^{t+1} = \\hat{\\theta}_{a\\_posteriori}^{t} + \\omega \\cdot \\delta t\n",
    "\\end{cases}\n",
    "\\end{equation}$$\n",
    "\n",
    "![state-space_model](Images/schema1.jpg)\n",
    "\n",
    "The matrix form of this system therefore becomes:\n",
    "\n",
    "$$\\begin{equation}\n",
    "A = \\begin{bmatrix} \n",
    "1 & 0 & 0\\\\ \n",
    "0 & 1 & 0 \\\\ \n",
    "0 & 0 & 1\n",
    "\\end{bmatrix}\n",
    "\\qquad\\qquad\n",
    "B = \\begin{bmatrix} \n",
    "\\cos\\left(\\hat{\\theta}_{a\\_posteriori}^{t} + \\delta \\theta^t \\right) \\cdot \\delta t & 0\\\\\n",
    "\\sin\\left(\\hat{\\theta}_{a\\_posteriori}^{t} + \\delta \\theta^t \\right) \\cdot \\delta t & 0 \\\\\n",
    "0 & \\delta t \n",
    "\\end{bmatrix}\n",
    "\\end{equation}$$\n",
    "\n",
    "The final term $q^t$ of this state-space model represents the stochastic perturbation of the state with covariance matrix Q defined as follows:\n",
    "\n",
    "$$\n",
    "Q = \\begin{bmatrix} \n",
    "q_1 & 0 & 0\\\\ \n",
    "0 & q_2 & 0 \\\\ \n",
    "0 & 0 & q_3\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "These diagonal coefficients can be evaluated using an approach similar to that used in Exercise 8 of the MICRO-452 course. \n",
    "\n",
    "The experimentally estimated coefficients are as follows: q1=q2=4 (equivalent to a standard deviation of 2 pixels) and q3=0.03 (equivalent to a standard deviation of 10 degrees).\n",
    "\n",
    "\n",
    "### Measurement\n",
    "\n",
    "Having explored the prediction phase of the state-space model, attention now turns to the second essential part: updating the measurements. This stage aims to refine the predictions by integrating real information captured by the camera. The formula governing this step is :\n",
    "\n",
    "$$ m^{t+1} = C \\cdot s^{t+1} + r^{t+1}$$ \n",
    "\n",
    "Measurements taken at time t+1 are represented here by the term $m_{t+1}$. The data collected by the camera are therefore:\n",
    "\n",
    "$$m^{t+1} = \\begin{pmatrix}\n",
    "x_{captured}^{t+1} \\\\\\\\\n",
    "y_{captured}^{t+1} \\\\\\\\\n",
    "\\theta_{captured}^{t+1}\n",
    "\\end{pmatrix}$$\n",
    "\n",
    "The robot's position ($x, y$) and orientation ($\\theta$) measured by the camera are used directly as system outputs, without any transformation. The matrix C linking the measurements to the state is therefore defined as follows:\n",
    "\n",
    "$$C = \\begin{bmatrix} \n",
    "1 & 0 & 0\\\\ \n",
    "0 & 1 & 0 \\\\ \n",
    "0 & 0 & 1\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "The term $s^{t+1}$ simply represents the state of the system at time t+1:\n",
    "\n",
    "$$s^{t+1} = \\begin{pmatrix}\n",
    "x^{t+1} \\\\\\\\\n",
    "y^{t+1} \\\\\\\\\n",
    "\\theta^{t+1}\n",
    "\\end{pmatrix}$$\n",
    "\n",
    "Finally, the last term $r^{t+1}$ of this equation represents noise on measurements with a covariance matrix R defined as follows:\n",
    "\n",
    "$$\n",
    "R = \\begin{bmatrix} \n",
    "r_1 & 0 & 0\\\\ \n",
    "0 & r_2 & 0 \\\\ \n",
    "0 & 0 & r_3\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Based on the precision of the camera, the following coefficients have been estimated: r1=r2=1 (equivalent to a standard deviation of 1 pixel) and r3=0.01 (equivalent to a standard deviation of 6 degrees).\n",
    "\n",
    "Note: When the camera's view is obstructed, estimation is only possible using the prediction model.\n",
    "\n",
    "\n",
    "<a id=\"kalman-filter\"></a>\n",
    "## Kalman filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f91bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_initialization():\n",
    "    \"\"\"\n",
    "    Initialize the various vectors and matrices requiered for filtering\n",
    "    \n",
    "    robot_vector: position (x and y) and orientation (theta) taken from the camera vision\n",
    "    \"\"\"\n",
    "    \n",
    "    global s_prev_est_a_posteriori, P_prev_est_a_posteriori, A, B, u, C, Q, R\n",
    "\n",
    "    ## Previous State A Posteriori Estimation Vector\n",
    "    # Vector representing the estimated state of the system at the previous time step\n",
    "    s_prev_est_a_posteriori = robot_vector\n",
    "       \n",
    "    ## Previous State A Posteriori Covariance Matrix\n",
    "    # Matrix representing the estimated precision of the previous estimated state (same as R)\n",
    "    P_prev_est_a_posteriori = np.array([[1, 0, 0], \n",
    "                                        [0, 1, 0], \n",
    "                                        [0, 0, 0.01]]) \n",
    "    \n",
    "    ## State Matrix\n",
    "    # Matrix defining how the system evolves from one time step to the next\n",
    "    A = np.array([[1, 0, 0], \n",
    "                  [0, 1, 0], \n",
    "                  [0, 0, 1]])\n",
    "        \n",
    "    ## Input Matrix \n",
    "    # Matrix describing the impact of the input on the state\n",
    "    B = np.array([[1, 0], \n",
    "                  [0, 1], \n",
    "                  [0, 0]]); \n",
    "        \n",
    "    ## Input Vector\n",
    "    # Vector representing control inputs applied to the system \n",
    "    u = np.array([0, 0])\n",
    "    \n",
    "    ## Output Matrix\n",
    "    # Matrix linking measurements to state\n",
    "    C = np.array([[1, 0, 0], \n",
    "                  [0, 1, 0], \n",
    "                  [0, 0, 1]])\n",
    "        \n",
    "    ## Process Noise Covariance Matrix\n",
    "    # Covariance matrix representing uncertainty in system dynamics\n",
    "    Q = np.array([[4, 0, 0], \n",
    "                  [0, 4, 0], \n",
    "                  [0, 0, 0.03]])\n",
    "    \n",
    "    ## Measurement Noise Covariance Matrix\n",
    "    # Matrix representing uncertainty of camera measurements\n",
    "    R = np.array([[1, 0, 0], \n",
    "                  [0, 1, 0], \n",
    "                  [0, 0, 0.01]])\n",
    "\n",
    "#--------------------------------------------------------------------------------#\n",
    "    \n",
    "def update_input(v_l,v_r,update_time, direction_rotation):\n",
    "    \"\"\"\n",
    "    Update the input vector and matrix\n",
    "    \n",
    "    v_l: robot x position deduced from the camera vision\n",
    "    v_r: robot y position deduced from the camera vision\n",
    "    update_time: robot theta orientation deduced from the camera vision\n",
    "    \"\"\"\n",
    "    \n",
    "    global B,u\n",
    "    \n",
    "    Thymio_to_mms = 0.349\n",
    "    mm_to_px = 100/137\n",
    "    \n",
    "    # Average translational speed\n",
    "    v = (v_r +v_l)/2 # Thymio speed (T)\n",
    "    v = v * Thymio_to_mms * mm_to_px # Speed in px/s (T -> mm/s -> px/s)\n",
    "\n",
    "    # Average rotational speed\n",
    "    w = (v_r -v_l)*Thymio_to_mms/robot_diameter # Angular speed in rad/s\n",
    "    \n",
    "    if (direction_rotation == TURN_RIGHT):\n",
    "        w = -w\n",
    "    \n",
    "    # Input vector\n",
    "    u = np.array([v, w]) \n",
    "    \n",
    "    # Angle variation\n",
    "    delta_theta = w * update_time\n",
    "    \n",
    "    # Input matrix\n",
    "    B = np.array([[np.cos(delta_theta + s_prev_est_a_posteriori[2])*update_time, 0],\n",
    "                  [-np.sin(delta_theta + s_prev_est_a_posteriori[2])*update_time, 0], \n",
    "                  [0, update_time]]); \n",
    "    \n",
    "#--------------------------------------------------------------------------------#\n",
    "\n",
    "def kalman_filter(s_prev_est_a_posteriori, P_prev_est_a_posteriori):\n",
    "    \"\"\"\n",
    "    Estimates the current state using the input sensor data and the previous state\n",
    "    \n",
    "    param s_prev_est_a_posteriori: previous state a posteriori estimation\n",
    "    param P_prev_est_a_posteriori: previous state a posteriori covariance\n",
    "    \n",
    "    return s_est_a_posteriori: new a posteriori state estimation\n",
    "    return P_est_a_posteriori: new a posteriori state covariance\n",
    "    \"\"\"\n",
    "    \n",
    "    ## Prediciton through the a priori estimate\n",
    "    # estimated mean of the state\n",
    "    s_est_a_priori = np.dot(A, s_prev_est_a_posteriori)+ np.dot(B, u);\n",
    "    path_apriori.append(s_prev_est_a_posteriori)\n",
    "    \n",
    "    # Estimated covariance of the state\n",
    "    P_est_a_priori = np.dot(A, np.dot(P_prev_est_a_posteriori, A.T)) + Q\n",
    "    \n",
    "    ## Update         \n",
    "    # m, C, and R for a posteriori estimate, depending on the detection of the camera\n",
    "    if camera_on == True:\n",
    "        m = robot_vector\n",
    "        path_camera.append(m)\n",
    "        # innovation / measurement residual\n",
    "        i = m - np.dot(C, s_est_a_priori);\n",
    "        # measurement prediction covariance\n",
    "        S = np.dot(C, np.dot(P_est_a_priori, C.T)) + R;     \n",
    "        # Kalman gain (tells how much the predictions should be corrected based on the measurements)\n",
    "        K = np.dot(P_est_a_priori, np.dot(C.T, np.linalg.inv(S)));\n",
    "        # a posteriori estimate\n",
    "        s_est_a_posteriori = s_est_a_priori + np.dot(K,i);\n",
    "        P_est_a_posteriori = P_est_a_priori - np.dot(K,np.dot(C, P_est_a_priori));\n",
    "    else:\n",
    "        K = 0 # Kalman gain is null because the camera can't deliver any data\n",
    "        # a posteriori estimate\n",
    "        s_est_a_posteriori = s_est_a_priori;\n",
    "        P_est_a_posteriori = P_est_a_priori;\n",
    "        \n",
    "    return s_est_a_posteriori, P_est_a_posteriori"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62802948",
   "metadata": {},
   "source": [
    "---\n",
    "<a id=\"local-navigation\"></a>\n",
    "\n",
    "# Local navigation\n",
    "\n",
    "\n",
    "Local navigation involves adjusting the robot's path to navigate around unexpected obstacles in its immediate vicinity. This often requires the robot to deviate from the optimal path to execute avoidance maneuvers. Thymio has to navigate past random obstacles, not captured in the vision and not factored into the global path planning. \n",
    "\n",
    "Once the obstacle is successfully navigated, Thymio can either return to the optimal path or calculate a new one. \n",
    "\n",
    "Thymio is equipped with five front horizontal proximity sensors for it to detect unknown objects that emerge in its path. Our primary objective is to help Thymio skillfully navigate around these local obstacles whilst enabling ample time for subsequent planning of an optimal path toward the ultimate goal.\n",
    "\n",
    "**Input :**\n",
    "\n",
    "    - Horizontal proximity sensor values; constantly updated\n",
    "\n",
    "**Output :**\n",
    "\n",
    "    - Command for controlling the robot's translational and rotational motion\n",
    "\n",
    "**Challenges :**\n",
    "\n",
    "    - Determining the colour of our physical obstacles have to be white, to not have the camera capture the obstacle in global vision.\n",
    "    \n",
    "    - Determining the shape of our obstacles have to be cylindrical for an optimal maneuver around it.\n",
    "    \n",
    "    - Determining the fixed distance to move forward and theta (angle) for Thymio's rotation was challenging to set without prior knowledge of the obstacle's shape. If the distance was too short or the angle too small, the robot will run into the obstacle. This tok us multiple tries to reach the optimal `distance_forward` and `theta`, as it was challenging to determine when an obstacle is considered cleared. Thus we had to finetune our parameters multiple times to ensure the success of our Thymio's obstacle avoidance. \n",
    "    \n",
    "    - Determining the obstacle threshold for when Thymio should initiate the local avoidance, `obstThrh`.\n",
    "\n",
    "### Parameters \n",
    "\n",
    "| Name                | Purpose                                                                           | Units | Global?|\n",
    "| :------------------- | :------------------------------------------------------------------------------- |-------|-----|\n",
    "| `obstThrh`      | High obstacle threshold for switching from global navigation to local navigation state| Int   | Y |\n",
    "| `angle_turned` | Total angle turned since initial direction, before local avoidance                     | Radians| Y |\n",
    "| `theta` | Angle to turn back, for Thymio to be aligned with initial direction before local avoidance    | Radians | N |\n",
    "| `distance_forward`| Amount of distance to move forward  | Meters | Y |\n",
    "| `diff_lr`| Calculated difference between the most left and most right sensors | Int | N |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "769a00a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def local_avoidance(sensor_prox, theta, dist):\n",
    "    \"\"\"\n",
    "    1a. Obstacle detected right in front: rotate left + go forward\n",
    "    1b. Obstacle detected near sides: rotate + go forward\n",
    "    2. Determine rotation left or right\n",
    "    3. Wall following of the obstacle\n",
    "    4. After obstacle clearance, turn back to initial direction\n",
    "    \"\"\"\n",
    "    # Global variables\n",
    "    global node, obstThrh\n",
    "    \n",
    "    # Calculate difference in Left & Right sensors\n",
    "    diff_lr = sensor_prox[0] - sensor_prox[4]\n",
    "    \n",
    "    # Object detected right in front\n",
    "    if sensor_prox[2] > obstThrh: \n",
    "        # Rotate to the left + go forward\n",
    "        await turn(pi/4)  \n",
    "        theta += pi/4\n",
    "        print(\"Front obstacle, turning right\")\n",
    "        await move_forward(100*dist)\n",
    "        await turn(-theta/2)\n",
    "        print(\"Turning back to initial direction\")\n",
    "        await move_forward(dist*100)\n",
    "        await turn(-theta/2)\n",
    "        await turn(theta/2)\n",
    "        return\n",
    "    else:\n",
    "        # Obstacle threshold\n",
    "        if max(sensor_prox) > obstThrh:\n",
    "            if (sensor_prox[0] + sensor_prox[1]) > (sensor_prox[3] + sensor_prox[4]):\n",
    "                await turn(pi/24)\n",
    "                theta += pi/24\n",
    "                await move_forward(dist*2)\n",
    "                print(\"Left obstacle, turning right\")\n",
    "                if (diff_lr) > 0:\n",
    "                    await turn(pi/30)\n",
    "                    theta += pi/30\n",
    "                    await move_forward(dist)\n",
    "                    print(\"Left wall following\")\n",
    "                else:\n",
    "                    await move_forward(dist)\n",
    "                    print(\"Not wall following\")\n",
    "            elif (sensor_prox[3] + sensor_prox[4]) > (sensor_prox[0] + sensor_prox[1]):\n",
    "                await turn(-pi/24)\n",
    "                theta -= pi/24\n",
    "                await move_forward(dist*2)\n",
    "                print(\"Right obstacle, turning left\")\n",
    "                if (diff_lr) < 0:\n",
    "                    await turn(-pi/30)\n",
    "                    theta -= pi/30\n",
    "                    await move_forward(dist)\n",
    "                    print(\"Right wall following\")\n",
    "                else:\n",
    "                    await move_forward(dist)\n",
    "                    print(\"Not wall following\")\n",
    "            else:\n",
    "                return  # Do nothing\n",
    "    \n",
    "    # Obstacle cleared, move forward for a while before turning back to initial direction & move forward slightly more\n",
    "    await move_forward(110*dist)\n",
    "    await turn(-theta*dist*3.5)\n",
    "    print(\"Turning back to initial direction\")\n",
    "    await move_forward(140*dist)\n",
    "    await turn(theta*dist*3)\n",
    "    await move_forward(dist)\n",
    "    \n",
    "    await motors_stop()\n",
    "    return\n",
    "\n",
    "# Local avoidance function with sensor values\n",
    "async def la_function(sensor_prox): \n",
    "    await node.wait_for_variables()\n",
    "    \n",
    "    while sum(sensor_prox[i] > obstThrh for i in range(0, 5)) > 0: \n",
    "        sensor_prox = node[\"prox.horizontal\"]\n",
    "        print(list(sensor_prox))\n",
    "        await local_avoidance(sensor_prox, angle_turned, distance_forward)\n",
    "        await client.sleep(0.2)\n",
    "        await motors_stop()\n",
    "        print(\"Completed Local Avoidance!\")\n",
    "        return True\n",
    "    return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cfd665b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f2b1422c",
   "metadata": {},
   "source": [
    "---\n",
    "<a id=\"motion-control\"></a>\n",
    "# Motion control\n",
    "\n",
    "<a id=\"connection-functions\"></a>\n",
    "## Connection functions\n",
    "\n",
    "Initially, functions facilitating the establishment and termination of the connection with the robot have been implemented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c8a421b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thymio connection\n",
    "async def connect_Thymio():\n",
    "    \"\"\"\n",
    "    Establish a connection with the Thymio if possible\n",
    "    \"\"\"\n",
    "    global node, client\n",
    "    try:\n",
    "        client = ClientAsync()\n",
    "        node = await asyncio.wait_for(client.wait_for_node(), timeout=2.0)\n",
    "        await node.lock()\n",
    "        print(\"Thymio connected\")\n",
    "\n",
    "    except asyncio.TimeoutError:\n",
    "        print(\"Thymio not connected: Timeout while waiting for node.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Thymio not connected: {str(e)}\")\n",
    "        \n",
    "# Thymio disconnection\n",
    "def disconnect_Thymio():\n",
    "    \"\"\"\n",
    "    Enable to disconnect the Thymio\n",
    "    \"\"\"\n",
    "    aw(node.stop())\n",
    "    aw(node.unlock())\n",
    "    print(\"Thymio disconnected\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8191deba",
   "metadata": {},
   "source": [
    "<a id=\"motion-functions\"></a>\n",
    "## Motion functions\n",
    "\n",
    "The robot’s movement from its starting position to the target is executed using the coordinates of the global path. To transition from one coordinate to the next, a specific method has been adopted. The robot begins by orienting itself towards the next point, taking into account its current orientation. Once the orientation is adjusted, the robot performs a straight-line movement to the next point. This step is repeated from position to position until the final destination is reached. The basic functions therefore include: activation and deactivation of motors, rotation by a defined angle, and linear movement over a defined distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "798c0f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thymio set motors speeds  \n",
    "async def set_speeds(left_speed, right_speed):\n",
    "    \"\"\"\n",
    "    Enable to set the speed of the Thymio's wheels\n",
    "    \"\"\"\n",
    "    global node\n",
    "    v = {\n",
    "        \"motor.left.target\":  [left_speed],\n",
    "        \"motor.right.target\": [right_speed],\n",
    "    }\n",
    "    await node.set_variables(v)\n",
    "    \n",
    "# Thymio motors stop     \n",
    "async def motors_stop():\n",
    "    \"\"\"\n",
    "    Stop the Thymio\n",
    "    \"\"\"\n",
    "    global node\n",
    "    v = {\n",
    "        \"motor.left.target\":  [0],\n",
    "        \"motor.right.target\": [0],\n",
    "    }\n",
    "    await node.set_variables(v)    \n",
    "\n",
    "# Conversion factors\n",
    "Thymio_to_mms = 0.349\n",
    "px_to_mm = 137/100\n",
    "# Constants\n",
    "ROTATION_SPEED = 100\n",
    "TIME_FULL_TURN = (8800/1000)\n",
    "\n",
    "# Thymio turns a specied angle\n",
    "async def turn(angle):\n",
    "    # Calculate the time needed to turn through the required angle\n",
    "    rotation_time = (abs(angle) / (2*np.pi)) * TIME_FULL_TURN\n",
    "\n",
    "    # Turn robot on itself\n",
    "    # Check the sign of angle\n",
    "    if np.sign(angle) > 0:\n",
    "        # If angle is positive, turn left\n",
    "        await set_speeds(-ROTATION_SPEED, ROTATION_SPEED)\n",
    "        left_or_right = TURN_LEFT\n",
    "    else:\n",
    "        # If angle is negative, turn right\n",
    "        await set_speeds(ROTATION_SPEED, -ROTATION_SPEED)\n",
    "        left_or_right = TURN_RIGHT\n",
    "\n",
    "    # Wait required time\n",
    "    time.sleep(rotation_time)\n",
    "    return rotation_time, left_or_right\n",
    "\n",
    "# Constants\n",
    "FORWARD_SPEED = 200 \n",
    "TIME_PER_MM = 15.5/1000  # Time it takes for the robot to travel one meter at base speed\n",
    "\n",
    "async def move_forward(distance_px):\n",
    "    # Calculate the time needed to travel the requested distance\n",
    "    \n",
    "    distance_mm = distance_px * px_to_mm\n",
    "    travel_time = (distance_mm) * TIME_PER_MM\n",
    "    \n",
    "    # Robot moves forward\n",
    "    await set_speeds(FORWARD_SPEED, FORWARD_SPEED)\n",
    "\n",
    "    # Wait for the necessary time\n",
    "    time.sleep(travel_time)\n",
    "    return travel_time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c28cd260",
   "metadata": {},
   "source": [
    "<a id=\"motion-control-function\"></a>\n",
    "## Motion control function\n",
    "\n",
    "The management of one coordinate to another is entirely handled by the following function, which determines whether the next movement is a rotation or a translation, and establishes the angle or distance to be covered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "edf49d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def reach_next_node(next_node, mode, estimated_pos):\n",
    "\n",
    "    # Vector between the estimated position and the next position in the global path\n",
    "    vector_next_node = np.array([0,0])  \n",
    "    vector_next_node[0] = path_final[0][next_node] - estimated_pos[0]\n",
    "    vector_next_node[1] = path_final[1][next_node] - estimated_pos[1] \n",
    "    \n",
    "    # Normalized angle between the estimated position and the next position in the global path\n",
    "    gamma = -math.atan2(vector_next_node[1], vector_next_node[0]) - estimated_pos[2]\n",
    "    gamma = (gamma + np.pi) % (2*np.pi) - np.pi\n",
    "    \n",
    "    # Distance separating the estimated position and the next position in the global path\n",
    "    path_next_node = np.array([path_final[0][next_node], path_final[1][next_node]])\n",
    "    path_current_node = np.array([estimated_pos[0], estimated_pos[1]])\n",
    "    d = np.linalg.norm(path_next_node - path_current_node)\n",
    "    \n",
    "    if(not mode):\n",
    "        if(abs(gamma) > ANGLE_THRESHOLD):\n",
    "            time_r, left_or_right = await turn(gamma)\n",
    "        else: \n",
    "            time_r = 0 \n",
    "            left_or_right = 1  \n",
    "        return time_r, left_or_right \n",
    "        \n",
    "    if (mode):\n",
    "        if( d > FORWARD_THRESHOLD):\n",
    "            time_f = await move_forward(d)\n",
    "            return time_f"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9114a36c",
   "metadata": {},
   "source": [
    "---\n",
    "<a id=\"project-structure\"></a>\n",
    "# Project structure\n",
    "\n",
    "\n",
    "The project was realized by assembling various components. The camera, an essential element, provides the vision necessary for global navigation and filtering. It plays a crucial role in detecting the robot, the target, and fixed obstacles, thus allowing the calculation of the optimal path. It also provides position data ($x,y$) and the orientation ($\\theta$) of the robot, which are indispensable for estimating its position when using the Kalman filter.\n",
    "\n",
    "Global navigation generates the coordinates of the path to follow to reach the target. The Kalman filter, on the other hand, receives position and orientation data measured by the camera, as well as information related to motion control (wheel speeds and travel time). It outputs the best estimate of Thymio's real position.\n",
    "\n",
    "Finally, the robot constantly interacts with local navigation and motion control to avoid local obstacles and move towards its destination. The motor commands are used to move the robot, while the frontal proximity sensors are used to detect local obstacles. \n",
    "\n",
    "![interactions](Images/diag1.jpg)\n",
    "\n",
    "The functioning of the project is detailed below. Following initialization, a mapping is carried out, allowing the calculation of the global path. The program then enters a loop that continues until the robot reaches its destination. At each iteration of this loop, the robot checks whether it has been kidnnaped or if a mobile obstacle is in its path. If either of these situations occurs, the robot recalculates a global path (after avoiding any potential obstacle). If none of these situations occur, the robot performs the movement defined by the motion control. Once the movement is completed, the robot checks whether its current position corresponds to its final destination. It should be noted that the robot is capable of moving with or without the information provided by the camera.\n",
    "\n",
    "![interactions](Images/diag2.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc20a4a7",
   "metadata": {},
   "source": [
    "---\n",
    "<a id=\"runnable-cells\"></a>\n",
    "# Runnable cells"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cbea251",
   "metadata": {},
   "source": [
    "<a id=\"functions-declaration\"></a>\n",
    "## Functions declaration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62c3543",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b7536c87",
   "metadata": {},
   "source": [
    "<a id=\"capture-image\"></a>\n",
    "## Capture image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e4639e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0fe4c734",
   "metadata": {},
   "source": [
    "<a id=\"main\"></a>\n",
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d762f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "camera_on = False  \n",
    "next_node = 1\n",
    "GROUND_THRESHOLD = 500\n",
    "calculate_global_path = False\n",
    "\n",
    "path_final = path_planning(robot_vector, goal_center, obstacle_matrix)\n",
    "\n",
    "\n",
    "await connect_Thymio()\n",
    "\n",
    "filter_initialization()\n",
    "\n",
    "while True:\n",
    "    \n",
    "    await node.wait_for_variables()\n",
    "    sensor_prox = node[\"prox.horizontal\"]\n",
    "    await client.sleep(0.2)\n",
    "    while sum(sensor_prox[i] > obstThrh for i in range(0, 5)) > 0: \n",
    "        calculate_global_path = await la_function(sensor_prox)\n",
    "        \n",
    "    await node.wait_for_variables()\n",
    "    sensor_gr = node[\"prox.ground.reflected\"]\n",
    "    await client.sleep(0.2)\n",
    "    while sum(sensor_gr[i] < GROUND_THRESHOLD for i in range(0, 2)): \n",
    "        calculate_global_path += 1\n",
    "        await motors_stop()\n",
    "\n",
    "    if(calculate_global_path):\n",
    "        await motors_stop()\n",
    "        path_final = path_planning(robot_vector, goal_center, obstacle_matrix)\n",
    "        calculate_global_path = False\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    # Rotation\n",
    "    time_rotation, turn_left_or_right = await reach_next_node(next_node, ROTATION_MODE, s_prev_est_a_posteriori)\n",
    "    update_input(-ROTATION_SPEED, ROTATION_SPEED, time_rotation, turn_left_or_right)\n",
    "    s_prev_est_a_posteriori, P_prev_est_a_posteriori = kalman_filter(s_prev_est_a_posteriori, P_prev_est_a_posteriori)\n",
    "\n",
    "    # Linear displacement\n",
    "    forward_time = await reach_next_node(next_node, FORWARD_MODE , s_prev_est_a_posteriori)\n",
    "    update_input(FORWARD_SPEED, FORWARD_SPEED, forward_time, 1)\n",
    "    s_prev_est_a_posteriori, P_prev_est_a_posteriori = kalman_filter(s_prev_est_a_posteriori, P_prev_est_a_posteriori)\n",
    "    \n",
    "    next_node += 1\n",
    "    \n",
    "    if (next_node == path_final.shape[1]):\n",
    "        break\n",
    "    \n",
    "await motors_stop()\n",
    "disconnect_Thymio()\n",
    "path_followed = np.array(path_followed)\n",
    "#print(path_followed)\n",
    "print(\"Thymio ready for the game!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee9acd0",
   "metadata": {},
   "source": [
    "---\n",
    "<a id=\"conclusion\"></a>\n",
    "# Conclusion\n",
    "\n",
    "During this project, we have engineered and implemented various components, including vision, filtering, motion control, as well as global and local navigation. Our project is fully functional, and the Thymio is proficient in reaching its destination while adhering to the optimal path and circumventing obstacles through global navigation and local avoidance. We have successfully applied various techniques that we have learned over the semester in the *Basics of Mobile Robotics* course.\n",
    "\n",
    "This project has been intriguing both technically and organizationally. The creation of a comprehensive project within a relatively short timeframe compelled us to make compromises. Nevertheless, the project operates efficiently across a wide array of tests."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
