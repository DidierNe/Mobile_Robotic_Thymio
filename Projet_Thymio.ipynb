{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84588a6d-d18e-4ad6-8d06-e251d942fb7c",
   "metadata": {},
   "source": [
    "# [MICRO-452:] Project Report - Groupe 28\n",
    "**Authors:** Celest Angela Tjong, Adrien Louis Baptiste Dupont, Luca Sidoti Pinto, Didier Henri Neuenschwander\n",
    "**Supervisors:** Prof. Francesco Mondada\n",
    "Date: 17 Novembre 2023\n",
    "\n",
    "[MICRO-452]: **to be changed**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2258c98-b00d-4caf-a0a6-b64e2262d6e0",
   "metadata": {},
   "source": [
    "\n",
    "<span style=\"color:red; font-size:40px;\">use as few personal pronouns as possible (we, our, etc.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "034c8421-4f25-49b6-8bc0-f73cc5369cd5",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "* [1. Introduction](#introduction)\n",
    "* [2. Vision](#vision)\n",
    "    * [2.1. Subsection 1](#vision-subsection-1)\n",
    "    * [2.2. Subsection 2](#vision-subsection-2)\n",
    "* [3. Global Navigation](#global-navigation)\n",
    "* [4. Filtering](#filtering)\n",
    "* [5. Local Navigation](#local-navigation)\n",
    "* [6. Conclusion](#conclusion)\n",
    "lusion)\n",
    "clusion)\n",
    "usion)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd4e09dc-1142-46dd-8485-6bc9b292bf37",
   "metadata": {},
   "source": [
    "## 1 Introduction\n",
    "<a id=\"introduction\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43bfb7cc-aff3-468a-855e-2ef86f760444",
   "metadata": {},
   "source": [
    "## 2 Vision\n",
    "<a id=\"vision\"></a>\n",
    "https://docs.opencv.org/4.x/df/d9d/tutorial_py_colorspaces.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2add8a4f-6198-4070-8728-8f1b016321d7",
   "metadata": {},
   "source": [
    "### Vision Subsection 1\n",
    "<a id=\"Vision-subsection-1\"></a>\n",
    "**Ask to TA if we have to describe every function used in the notebook?**\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ffe28c2-e175-40fb-b718-f526a3b25e0d",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'numpy._DTypeMeta' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_27512/1321274440.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcolors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mPIL\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mImage\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\cv2\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    179\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    180\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 181\u001b[1;33m \u001b[0mbootstrap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\cv2\\__init__.py\u001b[0m in \u001b[0;36mbootstrap\u001b[1;34m()\u001b[0m\n\u001b[0;32m    173\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    174\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0msubmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0m__collect_extra_submodules\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDEBUG\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 175\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[0m__load_extra_py_code_for_module\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"cv2\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msubmodule\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDEBUG\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    176\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mDEBUG\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Extra Python code for\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msubmodule\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"is loaded\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\cv2\\__init__.py\u001b[0m in \u001b[0;36m__load_extra_py_code_for_module\u001b[1;34m(base, name, enable_debug_print)\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[0mnative_module\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodule_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m         \u001b[0mpy_module\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodule_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0menable_debug_print\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\importlib\\__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m    125\u001b[0m                 \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    126\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 127\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    128\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\cv2\\typing\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNumpyVersion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;34m\"1.20.0\"\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mversion_info\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m9\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 69\u001b[1;33m     \u001b[0mNumPyArrayGeneric\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtyping\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAny\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgeneric\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     70\u001b[0m \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m     \u001b[0mNumPyArrayGeneric\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'numpy._DTypeMeta' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import colors\n",
    "from PIL import Image\n",
    "import math\n",
    "import time\n",
    "\n",
    "%matplotlib inline\n",
    "# Define color thresholds in HSV\n",
    "# Note: these thresholds may need to be adjusted for your specific image conditions\n",
    "lower_red_bound = np.array([120, 100, 70])\n",
    "upper_red_bound = np.array([255, 255, 255])\n",
    "lower_green_bound = np.array([60, 50, 100])\n",
    "upper_green_bound = np.array([100, 255, 255])\n",
    "lower_yellow_bound = np.array([0, 30, 120])\n",
    "upper_yellow_bound = np.array([40, 105, 255])\n",
    "lower_black_bound = np.array([0, 0, 0])\n",
    "upper_black_bound = np.array([180, 180, 50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b67ae3-5774-4f12-9345-9cc499b15b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import tdmclient Notebook environment:\n",
    "#import tdmclient.notebook\n",
    "#await tdmclient.notebook.start()\n",
    "# forward\n",
    "#motor_left_target= 100\n",
    "#motor_right_target= 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "926418ed-c1c2-4aa3-ac23-8f7630815551",
   "metadata": {},
   "source": [
    "Function that record a video and save it on the folder of the project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa1ae44-55a6-49f6-b96a-0854adea57c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "video_capture = cv2.VideoCapture(0, cv2.CAP_DSHOW)\n",
    "\n",
    "video_capture.set(cv2.CAP_PROP_FRAME_WIDTH, 1080)\n",
    "video_capture.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)\n",
    "# Obtenir les dimensions de la frame\n",
    "frame_width = int(video_capture.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(video_capture.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "# Définir le codec et créer un objet VideoWriter\n",
    "fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "out = cv2.VideoWriter('output.avi', fourcc, 20.0, (frame_width, frame_height))\n",
    "\n",
    "# Enregistrer la vidéo\n",
    "while video_capture.isOpened():\n",
    "    ret, frame = video_capture.read()\n",
    "    if ret:\n",
    "        # Écrire la frame dans le fichier\n",
    "        out.write(frame)\n",
    "\n",
    "        # Afficher la frame (si vous voulez voir le flux en temps réel)\n",
    "        cv2.imshow('Frame', frame)\n",
    "\n",
    "        # Quitter avec la touche 'q'\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "    else:\n",
    "        break\n",
    "\n",
    "# Libérer les ressources\n",
    "video_capture.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32ca27f-3812-4957-ba8b-73ae4c6c68b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Define a function to detect circles of a specific color\n",
    "def detect_color_circle(image, lower_color_bound, upper_color_bound):\n",
    "    # Convert to HSV color space\n",
    "    hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "    # Create a mask for the specified color\n",
    "    mask = cv2.inRange(hsv, lower_color_bound, upper_color_bound)\n",
    "\n",
    "    # Apply the mask to the original image\n",
    "    color_only = cv2.bitwise_and(image, image, mask=mask)\n",
    "\n",
    "    # Convert to grayscale for circle detection\n",
    "    gray = cv2.cvtColor(color_only, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Apply Canny edge detection to help with circle detection\n",
    "    edges = cv2.Canny(gray, 100, 200)\n",
    "\n",
    "    # Use Hough Transform to detect circles\n",
    "    circles = cv2.HoughCircles(edges, cv2.HOUGH_GRADIENT, 1, 20,\n",
    "                           param1=20, param2=15, minRadius=10, maxRadius=50)\n",
    "    \n",
    "    # If circles are detected, return the list of circles with x, y coordinates and radius\n",
    "    if circles is not None:\n",
    "        # Convert the (1, N, 3) array to (N, 3)\n",
    "        circles = np.uint16(np.around(circles[0, :]))          \n",
    "        return [(circle[0], circle[1], circle[2]) for circle in circles]\n",
    "    else:\n",
    "        return []\n",
    "    \n",
    "            \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f4f67d-21ce-4785-a6f9-de19714850bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_obstacle_mask(image, contours, kernel_size):\n",
    "    \"\"\"\n",
    "    Create a mask with zeros in the areas inside the dilated contours.\n",
    "\n",
    "    :param image: Input image.\n",
    "    :param contours: Contours to dilate and fill in the mask.\n",
    "    :param kernel_size: Size of the kernel used for dilation.\n",
    "    :return: Mask with zeros inside the dilated contours and ones elsewhere.\n",
    "    \"\"\"\n",
    "    # Create an empty mask of the same size as the image\n",
    "    h, w = image.shape[:2]\n",
    "    mask = np.ones((h, w), dtype=np.uint8)\n",
    "\n",
    "    # Perform dilation to increase the size of the black regions\n",
    "    kernel = np.ones((kernel_size, kernel_size), np.uint8)\n",
    "    for contour in contours:\n",
    "        # Create an individual mask for each contour\n",
    "        contour_mask = np.zeros((h, w), dtype=np.uint8)\n",
    "        cv2.fillPoly(contour_mask, [contour], 255)\n",
    "        contour_mask = cv2.dilate(contour_mask, kernel, iterations=1)\n",
    "        \n",
    "        # Combine the individual mask with the global mask\n",
    "        mask = cv2.bitwise_and(mask, cv2.bitwise_not(contour_mask))\n",
    "\n",
    "        #also add the contours\n",
    "        # Let's create a border around the image\n",
    "        border_size = 50\n",
    "        border_color = [0, 0, 0]  # Black border\n",
    "        # Use cv2.copyMakeBorder to add a border around the image\n",
    "        mask_with_border = cv2.copyMakeBorder(mask, border_size, border_size, border_size, border_size,\n",
    "                                           cv2.BORDER_CONSTANT, value=border_color)\n",
    "    \n",
    "    return mask_with_border\n",
    "\n",
    "\n",
    "# Now you have a mask with zeros in the obstacle areas and ones elsewhere\n",
    "# You can return this mask from your function or process it further as needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7151aada-0dc7-4d3d-9514-dbfca0ad35fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def detect_obstacle_contours(image, area_threshold, kernel_size):\n",
    "    \"\"\"\n",
    "    Detects and dilates obstacle contours in the given image.\n",
    "    :param image: Input image.\n",
    "    :param area_threshold: Area threshold for filtering contours.\n",
    "    :param kernel_size: Size of the kernel used for dilation.\n",
    "    :return: Image with obstacle contours drawn.\n",
    "    \"\"\"\n",
    "    hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "    mask_black = cv2.inRange(hsv, lower_black_bound, upper_black_bound)\n",
    "    contours, _ = cv2.findContours(mask_black, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    filtered_contours = [cnt for cnt in contours if cv2.contourArea(cnt) > area_threshold]\n",
    "    kernel = np.ones((kernel_size, kernel_size), np.uint8)\n",
    "    mask_dilated = cv2.dilate(mask_black, kernel, iterations=1)\n",
    "    dilated_contours, _ = cv2.findContours(mask_dilated, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    filtered_dilated_contours = [cnt for cnt in dilated_contours if cv2.contourArea(cnt) > area_threshold+1000]\n",
    "    contour_image = image.copy()\n",
    "    cv2.drawContours(contour_image, filtered_contours, -1, (0, 255, 0), 2)\n",
    "    cv2.drawContours(contour_image, filtered_dilated_contours, -1, (0, 0, 255), 2)\n",
    "    return contour_image, filtered_contours, filtered_dilated_contours\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13047d64-3da8-4f07-b297-a7fd8fc28292",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_obstacle_matrix(image, dilated_contours):\n",
    "    height, width = image.shape[:2]\n",
    "    obstacle_matrix = np.ones((height, width), dtype=np.uint8)\n",
    "\n",
    "    for contour in dilated_contours:\n",
    "        # Remplir chaque contour dilaté avec 0 (obstacle)\n",
    "        cv2.fillPoly(obstacle_matrix, [contour], 0)\n",
    "\n",
    "    return obstacle_matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee2f0da7-ba21-4f7f-8161-3b76a6e77880",
   "metadata": {},
   "source": [
    "In each frame, there are multiple elements that need to be displayed. The first category includes static elements such as the contours of obstacles and the goal point. Since these do not change over time, it is computationally more efficient to identify and locate them in the first frame, and then display them consistently in subsequent frames.\n",
    "\n",
    "On the other hand, elements related to the robot's localization must be determined in each frame. To gather information about its orientation and location, three circles are placed on the top of the robot. A green circle is located at the middle back, and two red circles are positioned on each side, with their centers aligned with the robot's ceeris. This arrangement facilitates the extraction of necessary information. By connecting the centers of the two red circles and creating a midpoint, a vector c thenan be formed by connecting this midpoint to the center of the green circ.It might also be feasible to use only two markers (circles), one at the back and one at the front, distinguished by their color. However, the marker at the front would hide the press button.le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c51b5c-0df7-4528-99d5-a5e367371f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Chemin du fichier vidéo enregistré\n",
    "video_path = 'output.avi'\n",
    "\n",
    "# Ouvrir le fichier vidéo\n",
    "video_capture = cv2.VideoCapture(video_path)\n",
    "\n",
    "# Assurez-vous que le fichier vidéo s'ouvre correctement\n",
    "if not video_capture.isOpened():\n",
    "    print(\"Erreur : Impossible d'ouvrir le fichier vidéo.\")\n",
    "    exit()\n",
    "#video_capture = cv2.VideoCapture(0, cv2.CAP_DSHOW)\n",
    "\n",
    "video_capture.set(cv2.CAP_PROP_FRAME_WIDTH, 1080)\n",
    "video_capture.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)\n",
    "window_name = 'Robot Detection'\n",
    "cv2.namedWindow(window_name, cv2.WINDOW_NORMAL)\n",
    "\n",
    "# Redimensionner la fenêtre (largeur, hauteur)\n",
    "cv2.resizeWindow(window_name, 540, 360)  # Réduire de moitié par exemple\n",
    "\n",
    "# Initial detection of obstacles\n",
    "ret, initial_frame = video_capture.read()\n",
    "if ret:\n",
    "     contour_image = detect_obstacle_contours(initial_frame, 2000, 70)\n",
    "     global_obstacle = create_obstacle_matrix(initial_frame, contour_image[2])\n",
    "     plt.imshow(cv2.cvtColor(contour_image[0], cv2.COLOR_BGR2RGB))\n",
    "     plt.title('Initial Contours')\n",
    "     plt.axis('off')\n",
    "     plt.show()\n",
    "     yellow_circles = detect_color_circle(initial_frame, lower_yellow_bound, upper_yellow_bound)\n",
    "     if yellow_circles:\n",
    "    # Stocker les coordonnées du premier cercle jaune détecté\n",
    "        yellow_circle_coords = yellow_circles[0]\n",
    "\n",
    "# Robot update frequency (10 Hz)\n",
    "update_rate = 0.1  # 10 times per second\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Capture frame-by-frame\n",
    "        ret, frame = video_capture.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Mettre à jour les contours d'obstacle pour la frame actuelle\n",
    " \n",
    "        # Detect red and green circles\n",
    "        red_circles = detect_color_circle(frame, lower_red_bound, upper_red_bound)\n",
    "        green_circles = detect_color_circle(frame, lower_green_bound, upper_green_bound)\n",
    "\n",
    "\n",
    "        \n",
    "        if yellow_circles:\n",
    "            x, y, r = yellow_circles[0] #coordinate of the goal*********************************************************\n",
    "            cv2.circle(frame, (x, y), r, (0, 255, 255), 3) # Dessiner les cercles jaunes\n",
    "\n",
    "            \n",
    "        if red_circles and green_circles:\n",
    "                    if len(red_circles) >= 2:\n",
    "                        # Calculate the midpoint between the centers of the red circles\n",
    "                        midpoint = ((red_circles[0][0] + red_circles[1][0]) // 2,\n",
    "                        (red_circles[0][1] + red_circles[1][1]) // 2)\n",
    "                        # Calculer le vecteur directionnel\n",
    "                        direction = np.array([midpoint[0] - green_circles[0][0], midpoint[1] - green_circles[0][1]])\n",
    "                    \n",
    "                        # Normaliser et allonger le vecteur\n",
    "                        length = 100  # Longueur supplémentaire\n",
    "                        direction = direction / np.linalg.norm(direction) * length\n",
    "                    \n",
    "                        # Calculer le nouveau point d'arrivée\n",
    "                        new_endpoint = (int(green_circles[0][0] + direction[0]), int(green_circles[0][1] + direction[1]))\n",
    "                    \n",
    "                        # Dessiner la flèche allongée\n",
    "                        cv2.arrowedLine(frame, green_circles[0][:2], new_endpoint, (0, 0, 0), 3)\n",
    "\n",
    "                        # Calculate the angle of orientation with respect to the x-axis\n",
    "                        dx = green_circles[0][0] - midpoint[0]\n",
    "                        dy = green_circles[0][1] - midpoint[1]\n",
    "                        angle = math.atan2(dy, dx)\n",
    "                        angle_degrees = math.degrees(angle)\n",
    "                        robot_vector = (midpoint[0], midpoint[1], angle_degrees) #information of the robot\n",
    "                        # Optionally, display the angle\n",
    "                        cv2.putText(frame, f'Angle: {angle_degrees:.2f} degrees', (10, 30), \n",
    "                                    cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n",
    "                     \n",
    "                        cv2.putText(frame, f'Midpoint: ({midpoint[0]}, {midpoint[1]})', (10, 700),\n",
    "                                    cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n",
    "            \n",
    "        cv2.drawContours(frame, contour_image[1], -1, (0, 255, 0), 2)\n",
    "        cv2.drawContours(frame, contour_image[2], -1, (0, 0, 255), 2)\n",
    "        # Dimensions de la frame\n",
    "\n",
    "        height, width = frame.shape[:2]\n",
    "\n",
    "        # Coordonnées du point de départ du repère (bas droit de l'image)\n",
    "        origin_x, origin_y = width - 150, height - 70  # Ajustement pour une longueur de 100\n",
    "        # Dessiner l'axe X\n",
    "        cv2.line(frame, (origin_x, origin_y), (origin_x + 100, origin_y), (0, 0, 255), 2)\n",
    "        # Dessiner l'axe Y \n",
    "        cv2.line(frame, (origin_x, origin_y), (origin_x, origin_y - 100), (0, 255, 0), 2)\n",
    "        # Marquer la longueur sur l'axe X\n",
    "        cv2.putText(frame, \"100\", (origin_x + 100, origin_y + 15), cv2.FONT_HERSHEY_SIMPLEX, 0.4, (0, 0, 255), 2)\n",
    "        # Marquer la longueur sur l'axe Y\n",
    "        cv2.putText(frame, \"100\", (origin_x - 30, origin_y - 100), cv2.FONT_HERSHEY_SIMPLEX, 0.4, (255, 0, 255), 2)\n",
    "        cv2.putText(frame, f'({origin_x}, {origin_y})', (origin_x-200, origin_y-10),\n",
    "        cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2) #environ 14cm pour 100pixcel\n",
    "        # ... (le reste de votre code pour afficher la frame)\n",
    "        # Display the resulting frame\n",
    "        cv2.imshow('Robot Detection', frame)\n",
    "\n",
    "        # Pause to maintain the update frequency\n",
    "        time_to_wait = max(int((start_time + update_rate - time.time()) * 1000), 1)\n",
    "        if cv2.waitKey(time_to_wait) & 0xFF == ord('q'):\n",
    "            break\n",
    "finally:\n",
    "    # Release the capture when everything is finished\n",
    "    video_capture.release()\n",
    "    cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec623cd9-75a3-4ae9-b9df-8e11b8a2ac20",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(threshold=np.inf)\n",
    "\n",
    "# Afficher la matrice global_obstacle\n",
    "print(global_obstacle)\n",
    "plt.imshow(global_obstacle, cmap='gray')\n",
    "plt.title('Matrice d\\'Obstacles')\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "x, y, r = yellow_circles[0] #coordinate of the goal*********************************************************"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dffc645-cada-46ec-949e-b630f247bfa3",
   "metadata": {},
   "source": [
    "\n",
    "\"Initially, for the setup, it's advantageous to position the camera above the play area. This placement minimizes the need to account for perspective distortions. To accurately determine the robot's position and orientation, at least two distinct markers should be placed on the robot. These markers enable precise tracking and analysis of the robot's movements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54b1493-4591-41f1-8e1f-c6d43c869c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Detect red and green circles\n",
    "red_circles = detect_color_circle(image, lower_red_bound, upper_red_bound)\n",
    "green_circles = detect_color_circle(image, lower_green_bound, upper_green_bound)\n",
    "yellow_circles = detect_color_circle(image, lower_yellow_bound, upper_yellow_bound)\n",
    "\n",
    "red_circles, green_circles, yellow_circles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da82c685-ef08-4273-b907-37b1a9e68876",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Assuming red_circles and green_circles contain the detected circles for each color\n",
    "# For demonstration, let's create dummy circle data\n",
    "# red_circles = [(x1, y1, r1), (x2, y2, r2)]\n",
    "# green_circles = [(x3, y3, r3)]\n",
    "\n",
    "# TODO: Replace the dummy values with your actual circle centers and radii\n",
    "#red_circles = [(50, 50, 30), (150, 50, 30)]  # Dummy values\n",
    "#green_circles = [(100, 150, 30)]  # Dummy values\n",
    "\n",
    "# Calculate the midpoint between the centers of the red circles\n",
    "midpoint = ((red_circles[0][0] + red_circles[1][0]) // 2,\n",
    "            (red_circles[0][1] + red_circles[1][1]) // 2)\n",
    "\n",
    "# Draw a line (and arrow) from the green circle's center to the midpoint\n",
    "cv2.arrowedLine(image,  green_circles[0][:2],midpoint, (0, 255, 0), 20)\n",
    "\n",
    "# Calculate the angle of orientation with respect to the x-axis\n",
    "dx = green_circles[0][0] - midpoint[0]\n",
    "dy = green_circles[0][1] - midpoint[1]\n",
    "angle = math.atan2(dy, dx)\n",
    "angle_degrees = math.degrees(angle)\n",
    "\n",
    "# Display the image with the drawn arrow\n",
    "plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "plt.title(f'Angle of orientation: {angle_degrees:.2f} degrees')\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "# Return the midpoint and the angle\n",
    "midpoint, angle_degrees\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67438223-d9b1-418a-8930-f751226a0e25",
   "metadata": {},
   "source": [
    "For the obstacle detection: we used black shapes that we randomly distribute around the board. It is "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e368801-d787-4c38-840d-e4f21a95c84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#image_path = 'WIN_20231124_15_09_52_Pro.jpg'\n",
    "image = initial_frame #cv2.imread(image_path)\n",
    "image = frame.copy()  # Créer une copie de l'image\n",
    "# Convert to HSV color space\n",
    "hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "# Define the lower and upper bounds for the black color\n",
    "lower_black = np.array([0, 0, 0])\n",
    "upper_black = np.array([180, 150, 40])\n",
    "\n",
    "# Create a black color mask\n",
    "mask_black = cv2.inRange(hsv, lower_black, upper_black)\n",
    "\n",
    "# Find contours in the mask\n",
    "contours, _ = cv2.findContours(mask_black, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "# Set a realistic threshold for the area of the contours\n",
    "area_threshold = 2000  # Adjust this threshold according to your needs\n",
    "\n",
    "# Filter the original contours that are larger than the threshold\n",
    "filtered_contours = [cnt for cnt in contours if cv2.contourArea(cnt) > area_threshold]\n",
    "\n",
    "# Perform dilation to increase the size of the black regions\n",
    "kernel_size = 70  # Kernel size can be adjusted to control the amount of dilation\n",
    "kernel = np.ones((kernel_size, kernel_size), np.uint8)\n",
    "mask_dilated = cv2.dilate(mask_black, kernel, iterations=1)\n",
    "\n",
    "# Find contours in the dilated mask\n",
    "dilated_contours, _ = cv2.findContours(mask_dilated, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "# Filter the dilated contours that are larger than the threshold\n",
    "filtered_dilated_contours = [cnt for cnt in dilated_contours if cv2.contourArea(cnt) > area_threshold]\n",
    "\n",
    "# Draw the filtered original contours in green\n",
    "for contour in filtered_contours:\n",
    "    cv2.drawContours(image, [contour], -1, (0, 255, 0), 2)\n",
    "\n",
    "# Draw the filtered dilated contours in red\n",
    "for contour in filtered_dilated_contours:\n",
    "    cv2.drawContours(image, [contour], -1, (0, 0, 255), 2)\n",
    "\n",
    "# Display the image with the drawn contours\n",
    "plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "plt.title('Filtered Original and Dilated Contours')\n",
    "plt.axis('off')  # Hide the axis\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "864f1ea7-1588-4b28-8bbc-6a1fd6180ce2",
   "metadata": {},
   "source": [
    "# Tool functions that help with parameter tuning:\n",
    "<a id=\"Vision-subsection-2\"></a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa54b1a6-9448-4b46-a8e2-939ef1afe52b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_detected_circles(image, circles, circle_color):\n",
    "    \"\"\"\n",
    "    Draws the detected circles on the image and plots it.\n",
    "\n",
    "    :param image: The original image.\n",
    "    :param circles: A list of circles with their coordinates and radius.\n",
    "    :param circle_color: The color to use for drawing the circles.\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    if circles is not None and len(circles) > 0:\n",
    "        for circle in circles:\n",
    "            center = (circle[0], circle[1])  # Circle center\n",
    "            radius = circle[2]  # Circle radius\n",
    "            # Draw the circle's perimeter\n",
    "            cv2.circle(image, center, radius, circle_color, 2)\n",
    "            # Draw the circle's center\n",
    "            cv2.circle(image, center, 2, circle_color, 3)\n",
    "\n",
    "    # Plot the image with detected circles\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "    plt.axis('off')  # Hide the axis\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe05eb0-2409-4b08-8e58-e852199f2b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Vérification si la frame a été capturée avec succès\n",
    "if ret:\n",
    "    # Conversion de l'image en RGB pour l'affichage avec Matplotlib\n",
    "    frame_rgb = cv2.cvtColor(initial_frame, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Affichage de l'image\n",
    "    plt.imshow(frame_rgb)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Erreur lors de la capture de l'image.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a142009-f980-44a4-85a6-6302ccfc62a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtenir les dimensions de l'image\n",
    "h, w = frame_rgb.shape[:2]\n",
    "\n",
    "# Calculer le nombre total de pixels\n",
    "total_pixels = h * w\n",
    "\n",
    "print(f\"The image is of dimension {w}x{h} (width x hight)\")\n",
    "print(f\"The total number of pixels is : {total_pixels}\")\n",
    "\n",
    "height, width = global_obstacle.shape\n",
    "\n",
    "print(\"Hauteur de la matrice:\", height)\n",
    "print(\"Largeur de la matrice:\", width)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bfd92df-e5c6-432d-ae01-eb5227452df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Load the image\n",
    "image = initial_frame #cv2.imread('WIN_20231124_15_09_52_Pro.jpg')\n",
    "image = image.copy()\n",
    "# Convert to HSV color space\n",
    "hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "# Define the lower and upper bounds for the red color\n",
    "# Note: Adjust these values according to your color calibration\n",
    "\n",
    "\n",
    "# Create a red color mask\n",
    "mask_red = cv2.inRange(hsv, lower_yellow_bound, upper_yellow_bound)\n",
    "\n",
    "# Apply the mask to the image\n",
    "red_only = cv2.bitwise_and(image, image, mask=mask_red)\n",
    "\n",
    "# Convert the result to grayscale\n",
    "gray = cv2.cvtColor(red_only, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "\n",
    "# Apply Canny edge detection to help with circle detection\n",
    "edges = cv2.Canny(gray, 100, 200)\n",
    "\n",
    "# Detect circles using the Hough Transform\n",
    "circles = cv2.HoughCircles(edges, cv2.HOUGH_GRADIENT, 1, 20,\n",
    "                           param1=20, param2=15, minRadius=10, maxRadius=50)\n",
    "\n",
    "# If circles are detected, draw them\n",
    "if circles is not None:\n",
    "    circles = np.uint16(np.around(circles))\n",
    "    for i in circles[0, :]:\n",
    "        cv2.circle(image, (i[0], i[1]), i[2], (0, 255, 0), 2)\n",
    "        cv2.circle(image, (i[0], i[1]), 2, (0, 0, 255), 3)\n",
    "\n",
    "# Plotting the different stages\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(1, 4, 1)\n",
    "plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "plt.title('Original Image')\n",
    "\n",
    "plt.subplot(1, 4, 2)\n",
    "plt.imshow(mask_red, cmap='gray')\n",
    "plt.title('Red Mask')\n",
    "\n",
    "plt.subplot(1, 4, 3)\n",
    "plt.imshow(edges, cmap='gray')\n",
    "plt.title('Canny Edges')\n",
    "\n",
    "plt.subplot(1, 4, 4)\n",
    "plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "plt.title('Detected Circles')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a355820-2984-4a83-b507-c85b3b551eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "from ipywidgets import interactive\n",
    "\n",
    "def interactive_mask(lower_h, lower_s, lower_v, upper_h, upper_s, upper_v):\n",
    "    lower_color_bound = np.array([lower_h, lower_s, lower_v])\n",
    "    upper_color_bound = np.array([upper_h, upper_s, upper_v])\n",
    "    \n",
    "    mask = cv2.inRange(hsv, lower_color_bound, upper_color_bound)\n",
    "    plt.imshow(mask, cmap='gray')\n",
    "    plt.show()\n",
    "\n",
    "# Assuming 'hsv' is your converted HSV image\n",
    "interactive(interactive_mask, lower_h=(0,255), lower_s=(0,255), lower_v=(0,255),\n",
    "            upper_h=(0,255), upper_s=(0,255), upper_v=(0,255))\n",
    "#lower_red_bound = np.array([120, 70, 120])\n",
    "#upper_red_bound = np.array([255, 255, 255])\n",
    "#lower_green_bound = np.array([60, 120, 100])\n",
    "#upper_green_bound = np.array([100, 255, 255])\n",
    "#lower_black = np.array([0, 0, 0])\n",
    "#upper_black = np.array([180, 255, 50])\n",
    "#lower_yellow_bound = np.array([0, 60, 140])\n",
    "#upper_yellow_bound = np.array([40, 105, 255])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b98467-3505-40c8-9417-8236cdbad250",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = frame #cv2.imread('WIN_20231124_15_09_52_Pro.jpg')\n",
    "red_circles = detect_color_circle(image, lower_red_bound, upper_red_bound)\n",
    "green_circles = detect_color_circle(image, lower_green_bound, upper_green_bound)\n",
    "yellow_circles = detect_color_circle(image, lower_yellow_bound, upper_yellow_bound)\n",
    "\n",
    "red_circles, green_circles, yellow_circles\n",
    "\n",
    "# Assuming red_circles and green_circles contain the detected circles for each color\n",
    "image_copy = image.copy()  # Make a copy to draw on\n",
    "plot_detected_circles(image_copy, red_circles, (200, 0, 255))  # Red color for red circles\n",
    "plot_detected_circles(image_copy, green_circles, (0, 255, 0))  # Green color for green circles\n",
    "plot_detected_circles(image_copy, yellow_circles,(255,255,153))  # yellow color for yellow circles\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e89d89a2-0751-4907-9a38-cee54fc239fd",
   "metadata": {},
   "source": [
    "## 3 Global Navigation\n",
    "<a id=\"global-navigation\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a11331",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_val_x_init = global_obstacle.shape[0]\n",
    "max_val_y_init = global_obstacle.shape[1]\n",
    "\n",
    "reduction_coeff = 20 # tune for speed\n",
    "max_val_x = int(max_val_x_init / reduction_coeff)\n",
    "max_val_y = int(max_val_y_init / reduction_coeff)\n",
    "\n",
    "occupancy_grid = np.zeros((max_val_x, max_val_y), dtype=int)\n",
    "for i in range (max_val_x):\n",
    "    for j in range (max_val_y):\n",
    "        sum_pixels = 0\n",
    "        for k in range (reduction_coeff): # dans le doute on augmente la distance de sécurité avec obstacle\n",
    "            sum_pixels = sum_pixels + binary_matrix[int(i * reduction_coeff - reduction_coeff/2 + k)][int(j * reduction_coeff - reduction_coeff/2 + k)]\n",
    "        if sum_pixels == 0:\n",
    "            occupancy_grid[i][j] = 0\n",
    "        else:\n",
    "            occupancy_grid[i][j] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca9bb6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_empty_plot(max_val):\n",
    "    \"\"\"\n",
    "    Helper function to create a figure of the desired dimensions & grid\n",
    "    \n",
    "    :param max_val: dimension of the map along the x and y dimensions\n",
    "    :return: the fig and ax objects.\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(7,7))\n",
    "    \n",
    "    major_ticks = np.arange(0, max_val+1, 5)\n",
    "    minor_ticks = np.arange(0, max_val+1, 1)\n",
    "    ax.set_xticks(major_ticks)\n",
    "    ax.set_xticks(minor_ticks, minor=True)\n",
    "    ax.set_yticks(major_ticks)\n",
    "    ax.set_yticks(minor_ticks, minor=True)\n",
    "    ax.grid(which='minor', alpha=0.2)\n",
    "    ax.grid(which='major', alpha=0.5)\n",
    "    ax.set_ylim([-1,max_val_x])\n",
    "    ax.set_xlim([-1,max_val_y])\n",
    "    ax.grid(True)\n",
    "    \n",
    "    return fig, ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf223c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing the grid\n",
    "\n",
    "fig, ax = create_empty_plot(max_val_y)\n",
    "\n",
    "cmap = colors.ListedColormap(['white', 'red']) # Select the colors with which to display obstacles and free cells\n",
    "\n",
    "# Displaying the map\n",
    "ax.imshow(occupancy_grid, cmap=cmap)\n",
    "plt.title(\"Map : free cells in white, occupied cells in red\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8716398d",
   "metadata": {},
   "source": [
    "## A* implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9959a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_movements_8n():\n",
    "    \"\"\"\n",
    "    Get all possible 8-connectivity movements. Equivalent to get_movements_in_radius(1)\n",
    "    (up, down, left, right and the 4 diagonals).\n",
    "    :return: list of movements with cost [(dx, dy, movement_cost)]\n",
    "    \"\"\"\n",
    "    s2 = math.sqrt(2)\n",
    "    return [(1, 0, 1.0),\n",
    "            (0, 1, 1.0),\n",
    "            (-1, 0, 1.0),\n",
    "            (0, -1, 1.0),\n",
    "            (1, 1, s2),\n",
    "            (-1, 1, s2),\n",
    "            (-1, -1, s2),\n",
    "            (1, -1, s2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3865680",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def reconstruct_path(cameFrom, current):\n",
    "    \"\"\"\n",
    "    Recurrently reconstructs the path from start node to the current node\n",
    "    :param cameFrom: map (dictionary) containing for each node n the node immediately \n",
    "                     preceding it on the cheapest path from start to n \n",
    "                     currently known.\n",
    "    :param current: current node (x, y)\n",
    "    :return: list of nodes from start to current node\n",
    "    \"\"\"\n",
    "    total_path = [current]\n",
    "    while current in cameFrom.keys():\n",
    "        # Add where the current node came from to the start of the list\n",
    "        total_path.insert(0, cameFrom[current]) \n",
    "        current=cameFrom[current]\n",
    "    return total_path\n",
    "\n",
    "def A_Star(start, goal, h, coords, occupancy_grid, movement_type=\"4N\", max_val_x=max_val_x, max_val_y=max_val_y):\n",
    "    \"\"\"\n",
    "    A* for 2D occupancy grid. Finds a path from start to goal.\n",
    "    h is the heuristic function. h(n) estimates the cost to reach goal from node n.\n",
    "    :param start: start node (x, y)\n",
    "    :param goal_m: goal node (x, y)\n",
    "    :param occupancy_grid: the grid map\n",
    "    :param movement: select between 4-connectivity ('4N') and 8-connectivity ('8N', default)\n",
    "    :return: a tuple that contains: (the resulting path in meters, the resulting path in data array indices)\n",
    "    \"\"\"\n",
    "    \n",
    "    # -----------------------------------------\n",
    "    # DO NOT EDIT THIS PORTION OF CODE\n",
    "    # -----------------------------------------\n",
    "    \n",
    "    # Check if the start and goal are within the boundaries of the map\n",
    "    for point in [start, goal]:\n",
    "        for coord in point:\n",
    "            assert coord>=0 and coord<max_val_y, \"start or end goal not contained in the map\"\n",
    "    \n",
    "    # check if start and goal nodes correspond to free spaces\n",
    "    if occupancy_grid[start[0], start[1]]:\n",
    "        raise Exception('Start node is not traversable')\n",
    "\n",
    "    if occupancy_grid[goal[0], goal[1]]:\n",
    "        raise Exception('Goal node is not traversable')\n",
    "    \n",
    "    # get the possible movements corresponding to the selected connectivity\n",
    "    if movement_type == '4N':\n",
    "        movements = _get_movements_4n()\n",
    "    elif movement_type == '8N':\n",
    "        movements = _get_movements_8n()\n",
    "    else:\n",
    "        raise ValueError('Unknown movement')\n",
    "    \n",
    "    # --------------------------------------------------------------------------------------------\n",
    "    # A* Algorithm implementation - feel free to change the structure / use another pseudo-code\n",
    "    # --------------------------------------------------------------------------------------------\n",
    "    \n",
    "    # The set of visited nodes that need to be (re-)expanded, i.e. for which the neighbors need to be explored\n",
    "    # Initially, only the start node is known.\n",
    "    openSet = [start]\n",
    "    \n",
    "    # The set of visited nodes that no longer need to be expanded.\n",
    "    closedSet = []\n",
    "\n",
    "    # For node n, cameFrom[n] is the node immediately preceding it on the cheapest path from start to n currently known.\n",
    "    cameFrom = dict()\n",
    "\n",
    "    # For node n, gScore[n] is the cost of the cheapest path from start to n currently known.\n",
    "    gScore = dict(zip(coords, [np.inf for x in range(len(coords))]))\n",
    "    gScore[start] = 0\n",
    "\n",
    "    # For node n, fScore[n] := gScore[n] + h(n). map with default value of Infinity\n",
    "    fScore = dict(zip(coords, [np.inf for x in range(len(coords))]))\n",
    "    fScore[start] = h[start]\n",
    "\n",
    "    # while there are still elements to investigate\n",
    "    while openSet != []:\n",
    "        \n",
    "        #the node in openSet having the lowest fScore[] value\n",
    "        fScore_openSet = {key:val for (key,val) in fScore.items() if key in openSet}\n",
    "        current = min(fScore_openSet, key=fScore_openSet.get)\n",
    "        del fScore_openSet\n",
    "        \n",
    "        #If the goal is reached, reconstruct and return the obtained path\n",
    "        if current == goal:\n",
    "            return reconstruct_path(cameFrom, current), closedSet\n",
    "\n",
    "        openSet.remove(current)\n",
    "        closedSet.append(current)\n",
    "        \n",
    "        #for each neighbor of current:\n",
    "        for dx, dy, deltacost in movements:\n",
    "            \n",
    "            neighbor = (current[0]+dx, current[1]+dy)\n",
    "            \n",
    "            # if the node is not in the map, skip\n",
    "            if (neighbor[0] >= occupancy_grid.shape[0]) or (neighbor[1] >= occupancy_grid.shape[1]) or (neighbor[0] < 0) or (neighbor[1] < 0):\n",
    "                continue\n",
    "            \n",
    "            # if the node is occupied or has already been visited, skip\n",
    "            if (occupancy_grid[neighbor[0], neighbor[1]]) or (neighbor in closedSet): \n",
    "                continue\n",
    "                \n",
    "            # d(current,neighbor) is the weight of the edge from current to neighbor\n",
    "            # tentative_gScore is the distance from start to the neighbor through current\n",
    "            tentative_gScore = gScore[current] + deltacost\n",
    "            \n",
    "            if neighbor not in openSet:\n",
    "                openSet.append(neighbor)\n",
    "                \n",
    "            if tentative_gScore < gScore[neighbor]:\n",
    "                # This path to neighbor is better than any previous one. Record it!\n",
    "                cameFrom[neighbor] = current\n",
    "                gScore[neighbor] = tentative_gScore\n",
    "                fScore[neighbor] = gScore[neighbor] + h[neighbor]\n",
    "\n",
    "    # Open set is empty but goal was never reached\n",
    "    print(\"No path found to goal\")\n",
    "    return [], closedSet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555b8fb5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-08T22:44:38.966544Z",
     "start_time": "2020-05-08T22:44:37.185492Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define the start and end goal\n",
    "start = (0,0)\n",
    "goal = (45,60)\n",
    "\n",
    "\n",
    "\n",
    "# -----------------------------------------\n",
    "# DO NOT EDIT THIS PORTION OF CODE - \n",
    "# EXECUTION AND PLOTTING OF THE ALGORITHM\n",
    "# -----------------------------------------\n",
    "    \n",
    "    \n",
    "# List of all coordinates in the grid\n",
    "x,y = np.mgrid[0:max_val_x:1, 0:max_val_y:1]\n",
    "pos = np.empty(x.shape + (2,))\n",
    "pos[:, :, 0] = x; pos[:, :, 1] = y\n",
    "pos = np.reshape(pos, (x.shape[0]*x.shape[1], 2))\n",
    "coords = list([(int(x[0]), int(x[1])) for x in pos])\n",
    "\n",
    "# Define the heuristic, here = distance to goal ignoring obstacles\n",
    "h = np.linalg.norm(pos - goal, axis=-1)\n",
    "h = dict(zip(coords, h))\n",
    "\n",
    "# Run the A* algorithm\n",
    "path, visitedNodes = A_Star(start, goal, h, coords, occupancy_grid, movement_type=\"8N\")\n",
    "path = np.array(path).reshape(-1, 2).transpose()\n",
    "visitedNodes = np.array(visitedNodes).reshape(-1, 2).transpose()\n",
    "\n",
    "path_final = path * reduction_coeff\n",
    "\n",
    "# Displaying the map\n",
    "fig_astar, ax_astar = create_empty_plot(max_val_y)\n",
    "ax_astar.imshow(occupancy_grid, cmap=cmap)\n",
    "print(path_final)\n",
    "# Plot the best path found and the list of visited nodes\n",
    "ax_astar.scatter(visitedNodes[1], visitedNodes[0], marker=\"o\", color = 'orange');\n",
    "ax_astar.plot(path[1], path[0], marker=\"o\", color = 'blue');\n",
    "ax_astar.scatter(start[1], start[0], marker=\"o\", color = 'green', s=200);\n",
    "ax_astar.scatter(goal[1], goal[0], marker=\"o\", color = 'purple', s=200);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d4a839",
   "metadata": {},
   "source": [
    "- blanc : libre\n",
    "- rouge : obstacle\n",
    "- orange : case explorée\n",
    "- bleu : chemin le plus court\n",
    "- vert : départ\n",
    "- violet : arrivée"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "552d0770",
   "metadata": {},
   "source": [
    "Maintenant que nous avons calculé le chemin le plus court, nous appelons des fonctions pour diriger le robot dans la bonne direction. Les données qui sortent de ce fichier sont dans la matrice path_final qui regroupe les coordonées des étapes recalculées à la bonne échelle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d53c07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eee4515a-9acf-4d55-a716-a1843641eb0f",
   "metadata": {},
   "source": [
    "## 4 Filtering\n",
    "<a id=\"filtering\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "562fab4c",
   "metadata": {},
   "source": [
    "# Kalman Filter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d20951",
   "metadata": {},
   "source": [
    "The localization of the Thymio robot is performed using a Kalman filter. This filtering method is well suited to estimating the position and orientation of a mobile robot from noisy or incomplete measurements. The design of the filter in this project is based on using the position ($x, y$) and orientation ($\\theta$) provided by the camera as measurements. In addition, the speed of the robot, provided by the wheel speed sensors ($v_r, v_l$), is used as a prediction. In short, the Kalman filter merges a prediction of the system's future state with a measurement of that state to estimate position probabilistically."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be88e9a",
   "metadata": {},
   "source": [
    "## State-space model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85133c02",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35bf16eb",
   "metadata": {},
   "source": [
    "To estimate the robot's future position, a state-space model needs to be developed: \n",
    "\n",
    "$$\\hat{s}_{a\\_priori}^{t+1} = A \\cdot \\hat{s}_{a\\_posteriori}^{t} + B \\cdot u^{t} + q^t$$\n",
    "\n",
    "The prediction of the future state is referred to as $\\hat{s}_{a\\_priori}^{t+1}$, i.e. the a priori estimate at time t+1. Since the state of the system is defined by its position ($x, y$) and orientation ($\\theta$), this gives: \n",
    "\n",
    "$$\\hat{s}_{a\\_priori}^{t+1} = \\begin{pmatrix}\n",
    "\\hat{x}_{a\\_priori}^{t+1} \\\\\\\\\n",
    "\\hat{y}_{a\\_priori}^{t+1} \\\\\\\\\n",
    "\\hat{\\theta}_{a\\_priori}^{t+1}\n",
    "\\end{pmatrix}$$\n",
    "\n",
    "The current state corresponds to the term $\\hat{s}_{a\\_posteriori}^{t}$, which is the a posteriori estimate at time t. In the same way as above, this gives:\n",
    "\n",
    "$$\\hat{s}_{a\\_posteriori}^{t} = \\begin{pmatrix}\n",
    "\\hat{x}_{a\\_posteriori}^{t} \\\\\\\\\n",
    "\\hat{y}_{a\\_posteriori}^{t} \\\\\\\\\n",
    "\\hat{\\theta}_{a\\_posteriori}^{t}\n",
    "\\end{pmatrix}$$\n",
    "\n",
    "The system input at time t is represented by the vector $u^{t}$. This is made up of two terms: translational speed ($v$) and rotational speed ($\\omega$). \n",
    "\n",
    "$$u^t = \\begin{pmatrix}\n",
    "v \\\\\\\\\n",
    "\\omega\n",
    "\\end{pmatrix} $$\n",
    "\n",
    "These are defined on the basis of the speeds measured by the wheel speed sensors, i.e. the right ($v_r$) and left ($v_l$) speeds, and the spacing between the two wheels ($e$).\n",
    "\n",
    "$$ v = \\cfrac{v_r + v_l}{2} \\qquad\\qquad \\omega = \\cfrac{v_r-v_l}{e} $$ \n",
    "\n",
    "Matrix A characterizes the evolution of the system state, while matrix B describes the impact of the input on the future state. An odometry-based approach allows us to determine these two matrices by considering a very short time interval ($\\delta t$). During this time interval, the robot rotates by $\\delta \\theta = \\omega \\cdot \\delta t$. Knowing this, and referring to the diagram below, the following system of equations can be established: \n",
    "\n",
    "$$\\begin{equation}\n",
    "\\begin{cases}\n",
    "\\hat{x}_{a\\_priori}^{t+1} = \\hat{x}_{a\\_posteriori}^{t} + v \\cdot \\cos\\left(\\hat{\\theta}_{a\\_posteriori}^{t} + \\delta \\theta^t \\right) \\cdot \\delta t \\\\\n",
    "\\hat{y}_{a\\_priori}^{t+1} = \\hat{y}_{a\\_posteriori}^{t} + v \\cdot \\sin\\left(\\hat{\\theta}_{a\\_posteriori}^{t} + \\delta \\theta^t \\right) \\cdot \\delta t \\\\\n",
    "\\hat{\\theta}_{a\\_priori}^{t+1} = \\hat{\\theta}_{a\\_posteriori}^{t} + \\omega \\cdot \\delta t\n",
    "\\end{cases}\n",
    "\\end{equation}$$\n",
    "\n",
    "![state-space_model](Images/schematics.png)\n",
    "\n",
    "The matrix form of this system therefore becomes:\n",
    "\n",
    "$$\\begin{equation}\n",
    "A = \\begin{bmatrix} \n",
    "1 & 0 & 0\\\\ \n",
    "0 & 1 & 0 \\\\ \n",
    "0 & 0 & 1\n",
    "\\end{bmatrix}\n",
    "\\qquad\\qquad\n",
    "B = \\begin{bmatrix} \n",
    "\\cos\\left(\\hat{\\theta}_{a\\_posteriori}^{t} + \\delta \\theta^t \\right) \\cdot \\delta t & 0\\\\\n",
    "\\sin\\left(\\hat{\\theta}_{a\\_posteriori}^{t} + \\delta \\theta^t \\right) \\cdot \\delta t & 0 \\\\\n",
    "0 & \\delta t \n",
    "\\end{bmatrix}\n",
    "\\end{equation}$$\n",
    "\n",
    "The final term $q^t$ of this state-space model represents the stochastic perturbation of the state with covariance matrix Q defined as follows:\n",
    "\n",
    "$$\n",
    "Q = \\begin{bmatrix} \n",
    "q_1 & 0 & 0\\\\ \n",
    "0 & q_2 & 0 \\\\ \n",
    "0 & 0 & q_3\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "These diagonal coefficients can be evaluated using an approach similar to that used in Exercise 8 of the MICRO-452 course.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb9ff39",
   "metadata": {},
   "source": [
    "### Measurement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "246596ca",
   "metadata": {},
   "source": [
    "Having explored the prediction phase of the state-space model, attention now turns to the second essential part: updating the measurements. This stage aims to refine the predictions by integrating real information captured by the camera. The formula governing this step is :\n",
    "\n",
    "$$ m^{t+1} = C \\cdot s^{t+1} + r^{t+1}$$ \n",
    "\n",
    "Measurements taken at time t+1 are represented here by the term $m_{t+1}$. The data collected by the camera are therefore:\n",
    "\n",
    "$$m^{t+1} = \\begin{pmatrix}\n",
    "x_{captured}^{t+1} \\\\\\\\\n",
    "y_{captured}^{t+1} \\\\\\\\\n",
    "\\theta_{captured}^{t+1}\n",
    "\\end{pmatrix}$$\n",
    "\n",
    "The robot's position ($x, y$) and orientation ($\\theta$) measured by the camera are used directly as system outputs, without any transformation. The matrix C linking the measurements to the state is therefore defined as follows:\n",
    "\n",
    "$$C = \\begin{bmatrix} \n",
    "1 & 0 & 0\\\\ \n",
    "0 & 1 & 0 \\\\ \n",
    "0 & 0 & 1\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "The term $s^{t+1}$ simply represents the state of the system at time t+1:\n",
    "\n",
    "$$s^{t+1} = \\begin{pmatrix}\n",
    "x^{t+1} \\\\\\\\\n",
    "y^{t+1} \\\\\\\\\n",
    "\\theta^{t+1}\n",
    "\\end{pmatrix}$$\n",
    "\n",
    "Finally, the last term $r^{t+1}$ of this equation represents noise on measurements with a covariance matrix R defined as follows:\n",
    "\n",
    "$$\n",
    "R = \\begin{bmatrix} \n",
    "r_1 & 0 & 0\\\\ \n",
    "0 & r_2 & 0 \\\\ \n",
    "0 & 0 & r_3\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Note: When the camera's view is obstructed, estimation is only possible using the prediction model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60fd4470",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab02846f",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a3c4e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "import scipy\n",
    "import asyncio\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from random import randrange\n",
    "from tdmclient import aw, ClientAsync"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01239575",
   "metadata": {},
   "source": [
    "### Basic functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dfe6545",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thymio connection\n",
    "async def connect_Thymio():\n",
    "    \"\"\"\n",
    "    Establish a connection with the Thymio if possible\n",
    "    \"\"\"\n",
    "    global node, client\n",
    "    try:\n",
    "        client = ClientAsync()\n",
    "        node = await asyncio.wait_for(client.wait_for_node(), timeout=2.0)\n",
    "        await node.lock()\n",
    "        print(\"Thymio connected\")\n",
    "\n",
    "    except asyncio.TimeoutError:\n",
    "        print(\"Thymio not connected: Timeout while waiting for node.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Thymio not connected: {str(e)}\")\n",
    "        \n",
    "# Thymio disconnection\n",
    "def disconnect_Thymio():\n",
    "    \"\"\"\n",
    "    Enable to disconnect the Thymio\n",
    "    \"\"\"\n",
    "    aw(node.stop())\n",
    "    aw(node.unlock())\n",
    "    print(\"Thymio disconnected\")\n",
    "\n",
    "# Thymio control motor speeds  \n",
    "def set_speeds(left_speed, right_speed):\n",
    "    \"\"\"\n",
    "    Enable to set the speed of the Thymio's wheels\n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"motor.left.target\": [left_speed],\n",
    "        \"motor.right.target\": [right_speed],\n",
    "    }\n",
    "    \n",
    "# Check\n",
    "await connect_Thymio()\n",
    "node.send_set_variables(set_speeds(40, 40))\n",
    "time.sleep(2)\n",
    "node.send_set_variables(set_speeds(0, 0))\n",
    "disconnect_Thymio()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b6a5b5",
   "metadata": {},
   "source": [
    "### Global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8819f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# General variables:\n",
    "update_time = 0.05               # Time in [s] before the next update\n",
    "thymio_speed_to_mms = 0.4348     # Ratio to convert Thymio speed into mm/s\n",
    "acquire_data = True              # Boolean that determines whether the program should collect data or not\n",
    "samples_acquired = 0             # Number of samples acquired\n",
    "nb_samples = 10                  # Number of samples to reach\n",
    "camera_on = False                # Boolean that determines whether the camera has vision or not\n",
    "\n",
    "#Thymio variables: \n",
    "robot_diameter = 94              # Distance between the two wheels\n",
    "v_r = 0                          # Speed of the right wheel \n",
    "v_l = 0                          # Speed of the left wheel\n",
    "v_r_tot = 0                      # Sum of right speeds over several samples\n",
    "v_l_tot = 0                      # Sum of left speeds over several samples\n",
    "v = 0                            # Average translation speed\n",
    "w = 0                            # Average rotation speed\n",
    "delta_theta = 0                  # Angle variation\n",
    "thymio_data = []\n",
    "\n",
    "#Thymio position and orientation\n",
    "x0_vision = 10\n",
    "y0_vision = 10\n",
    "theta0_vision = np.pi/2\n",
    "x_vision = 20\n",
    "y_vision = 20\n",
    "theta_vision = np.pi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "474f9221",
   "metadata": {},
   "source": [
    "### Filter initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "731bd83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_initialization():\n",
    "    \"\"\"\n",
    "    Initialize the various vectors and matrices requiered for filtering\n",
    "    \n",
    "    param vision_x_position: robot x position deduced from the camera vision\n",
    "    param vision_y_position: robot y position deduced from the camera vision\n",
    "    param vision_theta_orientation: robot theta orientation deduced from the camera vision\n",
    "    \"\"\"\n",
    "    \n",
    "    global s_prev_est_a_posteriori, P_prev_est_a_posteriori, A, B, u, C, Q, R\n",
    "\n",
    "    ## Previous State A Posteriori Estimation Vector\n",
    "    # Vector representing the estimated state of the system at the previous time step\n",
    "    s_prev_est_a_posteriori = np.array([[x0_vision], \n",
    "                                        [y0_vision], \n",
    "                                        [theta0_vision]])\n",
    "    #print(\"\\n\".join([f\"s_prev_est_a_posteriori = [\", \"\\n\".join([\" , \".join([f\"{x: 10.2f}\" for x in row]) for row in s_prev_est_a_posteriori]), \"]\"]))\n",
    "    \n",
    "    ## Previous State A Posteriori Covariance Matrix\n",
    "    # Matrix representing the estimated precision of the previous estimated state\n",
    "    P_prev_est_a_posteriori = np.array([[1000, 0, 0], \n",
    "                                        [0, 1000, 0], \n",
    "                                        [0, 0, 1000]]) \n",
    "    #print(\"\\n\".join([f\"P_prev_est_a_posteriori = [\", \"\\n\".join([\" , \".join([f\"{x: 10.2f}\" for x in row]) for row in P_prev_est_a_posteriori]), \"]\"]))\n",
    "\n",
    "    ## State Matrix\n",
    "    # Matrix defining how the system evolves from one time step to the next\n",
    "    A = np.array([[1, 0, 0], \n",
    "                  [0, 1, 0], \n",
    "                  [0, 0, 1]])\n",
    "    #print(\"\\n\".join([f\"A = [\", \"\\n\".join([\" , \".join([f\"{x: 10.2f}\" for x in row]) for row in A]), \"]\"]))\n",
    "    \n",
    "    ## Input Matrix \n",
    "    # Matrix describing the impact of the input on the state\n",
    "    B = np.array([[1, 0], \n",
    "                  [0, 1], \n",
    "                  [0, 0]]); \n",
    "    #print(\"\\n\".join([f\"B = [\", \"\\n\".join([\" , \".join([f\"{x: 10.2f}\" for x in row]) for row in B]), \"]\"]))\n",
    "    \n",
    "    ## Input Vector\n",
    "    # Vector representing control inputs applied to the system \n",
    "    u = np.array([[0], \n",
    "                  [0]])\n",
    "    #print(\"\\n\".join([f\"u = [\", \"\\n\".join([\" , \".join([f\"{x: 10.2f}\" for x in row]) for row in u]), \"]\"]))\n",
    "\n",
    "    ## Output Matrix\n",
    "    # Matrix linking measurements to state\n",
    "    C = np.array([[1, 0, 0], \n",
    "                  [0, 1, 0], \n",
    "                  [0, 0, 1]])\n",
    "    #print(\"\\n\".join([f\"C = [\", \"\\n\".join([\" , \".join([f\"{x: 10.2f}\" for x in row]) for row in C]), \"]\"]))\n",
    "    \n",
    "    ## Process Noise Covariance Matrix\n",
    "    # Covariance matrix representing uncertainty in system dynamics\n",
    "    Q = np.array([[1, 0, 0], \n",
    "                  [0, 1, 0], \n",
    "                  [0, 0, 0.1]])\n",
    "    #print(\"\\n\".join([f\"Q = [\", \"\\n\".join([\" , \".join([f\"{x: 10.2f}\" for x in row]) for row in Q]), \"]\"]))\n",
    "\n",
    "    ## Measurement Noise Covariance Matrix\n",
    "    # Matrix representing uncertainty of camera measurements\n",
    "    R = np.array([[0.1, 0, 0], \n",
    "                  [0, 0.1, 0], \n",
    "                  [0, 0, 0.01]])\n",
    "    #print(\"\\n\".join([f\"R = [\", \"\\n\".join([\" , \".join([f\"{x: 10.2f}\" for x in row]) for row in R]), \"]\"]))\n",
    "\n",
    "# Check\n",
    "#filter_initialization()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f23050",
   "metadata": {},
   "source": [
    "## Update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a71e6fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from threading import Timer\n",
    "\n",
    "# Define a RepeatedTimer class that uses Python's threading module to create \n",
    "# a timer that executes a specific function at regular intervals\n",
    "\n",
    "class RepeatedTimer(object):\n",
    "    def __init__(self, interval, function, *args, **kwargs):\n",
    "        self._timer     = None\n",
    "        self.interval   = interval\n",
    "        self.function   = function\n",
    "        self.args       = args\n",
    "        self.kwargs     = kwargs\n",
    "        self.is_running = False\n",
    "        self.start()\n",
    "\n",
    "    def _run(self):\n",
    "        self.is_running = False\n",
    "        self.start()\n",
    "        self.function(*self.args, **self.kwargs)\n",
    "\n",
    "    def start(self):\n",
    "        if not self.is_running:\n",
    "            self._timer = Timer(self.interval, self._run)\n",
    "            self._timer.start()\n",
    "            self.is_running = True\n",
    "\n",
    "    def stop(self):\n",
    "        self._timer.cancel()\n",
    "        self.is_running = False\n",
    "\n",
    "def get_data():\n",
    "    global v_r, v_l, v_r_tot, v_l_tot, samples_acquired, thymio_data\n",
    "    if samples_acquired<nb_samples:\n",
    "        if samples_acquired == 0:\n",
    "            thymio_data.clear() \n",
    "        thymio_data.append({\"left_speed\":node[\"motor.left.speed\"],\n",
    "                            \"right_speed\":node[\"motor.right.speed\"]})\n",
    "        v_r_tot += node[\"motor.right.speed\"]\n",
    "        v_l_tot += node[\"motor.left.speed\"]\n",
    "        samples_acquired += 1\n",
    "        #print(\"sample number : \", samples_acquired)\n",
    "        #print(\"Right = {}, Left = {}\".format(node[\"motor.right.speed\"], node[\"motor.left.speed\"]))\n",
    "    else:\n",
    "        v_r = round(v_r_tot/nb_samples)\n",
    "        v_l = round(v_l_tot/nb_samples)\n",
    "        \n",
    "async def get_speeds():\n",
    "    global samples_acquired, v_r_tot, v_l_tot\n",
    "    if acquire_data:\n",
    "        await node.wait_for_variables() # wait for Thymio variables values\n",
    "        rt = RepeatedTimer(update_time, get_data) # Auto-start get_data()\n",
    "\n",
    "        try:\n",
    "            await client.sleep(1.5*((nb_samples)*update_time)) # Time needed for the sampling (with a safety constant)\n",
    "        finally:\n",
    "            rt.stop()  \n",
    "            samples_acquired = 0\n",
    "            v_r_tot = 0\n",
    "            v_l_tot = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ce3e1f",
   "metadata": {},
   "source": [
    "### Updating input vector and matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "249d0c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_input():\n",
    "    \"\"\"\n",
    "    Update the input vector and matrix\n",
    "    \"\"\"\n",
    "    \n",
    "    global B,u\n",
    "    \n",
    "    print(\"v_r :\", v_r)\n",
    "    print(\"v_l :\", v_l)\n",
    "    \n",
    "    # Average translational speed\n",
    "    v = (v_r +v_l)/2 \n",
    "    print(\"v : \", v)\n",
    "    \n",
    "    # Average rotational speed\n",
    "    w = (v_r -v_l)/robot_diameter \n",
    "    \n",
    "    # Input vector\n",
    "    u = np.array([[v], \n",
    "                  [w]]) \n",
    "    #print(\"\\n\".join([f\"u = [\", \"\\n\".join([\" , \".join([f\"{x: 10.2f}\" for x in row]) for row in u]), \"]\"]))\n",
    "    \n",
    "    # Angle variation\n",
    "    delta_theta = w * update_time\n",
    "    \n",
    "    # Input matrix\n",
    "    B = np.array([[np.cos(delta_theta + s_prev_est_a_posteriori[2][0])*update_time, 0],\n",
    "                  [np.sin(delta_theta + s_prev_est_a_posteriori[2][0])*update_time, 0], \n",
    "                  [0, update_time]]); \n",
    "    #print(\"\\n\".join([f\"B = [\", \"\\n\".join([\" , \".join([f\"{x: 10.2f}\" for x in row]) for row in B]), \"]\"]))\n",
    "\n",
    "# Check\n",
    "#update_input()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb0c96cb",
   "metadata": {},
   "source": [
    "## Plot the speed graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b16b420",
   "metadata": {},
   "outputs": [],
   "source": [
    "def speeds_graph():\n",
    "\n",
    "    l_speed = [x[\"left_speed\"] for x in thymio_data]\n",
    "    r_speed = [x[\"right_speed\"] for x in thymio_data]\n",
    "    avg_speed = np.array([(x[\"left_speed\"]+x[\"right_speed\"])/2 for x in thymio_data])\n",
    "    #print(avg_speed)\n",
    "\n",
    "    # Print the speeds\n",
    "    plt.plot(l_speed, label=\"Left motor\")\n",
    "    plt.plot(r_speed, label=\"Right motor\")\n",
    "    plt.plot(avg_speed, label=\"Average\")\n",
    "    plt.xlabel(\"Time step\")\n",
    "    plt.ylabel(\"Measured Velocity\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    ########### Might be useful for the variance calculation ###########\n",
    "    #                                                                  #\n",
    "    # avg_speed = avg_speed[:] / thymio_speed_to_mms                   #\n",
    "    # print(avg_speed)                                                 #\n",
    "    #                                                                  #\n",
    "    # var_speed = np.var(avg_speed)                                    #\n",
    "    # print(\"The speed variance in mm^2/s^2 is {}\".format(var_speed))  #\n",
    "    #                                                                  #\n",
    "    # q_nu = var_speed/2 # variance on speed state                     #\n",
    "    # r_nu = var_speed/2 # variance on speed measurement               #\n",
    "    # print(\"q_nu = {} et r_nu = {}\".format(q_nu, r_nu))               #\n",
    "    #                                                                  #\n",
    "    # qp = 0.04 # variance on position state                           #\n",
    "    # rp = 0.25 # variance on position measurement                     #\n",
    "    #                                                                  #\n",
    "    # qt = 0.04 # variance on orientation state                        #\n",
    "    # rt = 0.25 # variance on orientation measurement                  #\n",
    "    #                                                                  #\n",
    "    ####################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e0c2f6",
   "metadata": {},
   "source": [
    "## Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca09b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kalman_filter(s_prev_est_a_posteriori, P_prev_est_a_posteriori):\n",
    "    \"\"\"\n",
    "    Estimates the current state using the input sensor data and the previous state\n",
    "    \n",
    "    param s_prev_est_a_posteriori: previous state a posteriori estimation\n",
    "    param P_prev_est_a_posteriori: previous state a posteriori covariance\n",
    "    \n",
    "    return s_est_a_posteriori: new a posteriori state estimation\n",
    "    return P_est_a_posteriori: new a posteriori state covariance\n",
    "    \"\"\"\n",
    "    \n",
    "    ## Prediciton through the a priori estimate\n",
    "    # estimated mean of the state\n",
    "    s_est_a_priori = np.dot(A, s_prev_est_a_posteriori)+ np.dot(B, u);\n",
    "    \n",
    "    # Estimated covariance of the state\n",
    "    P_est_a_priori = np.dot(A, np.dot(P_prev_est_a_posteriori, A.T)) + Q\n",
    "    \n",
    "    ## Update         \n",
    "    # m, C, and R for a posteriori estimate, depending on the detection of the camera\n",
    "    if camera_on == True:\n",
    "        m = np.array([[x_vision], \n",
    "                      [y_vision], \n",
    "                      [theta_vision]])\n",
    "        # innovation / measurement residual\n",
    "        i = m - np.dot(C, s_est_a_priori);\n",
    "        # measurement prediction covariance\n",
    "        S = np.dot(C, np.dot(P_est_a_priori, C.T)) + R;     \n",
    "        # Kalman gain (tells how much the predictions should be corrected based on the measurements)\n",
    "        K = np.dot(P_est_a_priori, np.dot(C.T, np.linalg.inv(S)));\n",
    "        # a posteriori estimate\n",
    "        s_est_a_posteriori = s_est_a_priori + np.dot(K,i);\n",
    "        P_est_a_posteriori = P_est_a_priori - np.dot(K,np.dot(C, P_est_a_priori));\n",
    "    else:\n",
    "        K = 0 # Kalman gain is null because the camera can't deliver any data\n",
    "        # a posteriori estimate\n",
    "        s_est_a_posteriori = s_est_a_priori;\n",
    "        P_est_a_posteriori = P_est_a_priori;\n",
    "     \n",
    "    return s_est_a_posteriori, P_est_a_posteriori\n",
    "\n",
    "#Check\n",
    "#kalman_filter(s_prev_est_a_posteriori, P_prev_est_a_posteriori)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b953d19",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f9dde0",
   "metadata": {},
   "outputs": [],
   "source": [
    "await connect_Thymio()\n",
    "\n",
    "speed_test_value = randrange(200)      \n",
    "    \n",
    "node.send_set_variables(set_speeds(speed_test_value, speed_test_value))\n",
    "\n",
    "time.sleep(0.5)\n",
    "\n",
    "# Test\n",
    "\n",
    "# Initialization\n",
    "filter_initialization()\n",
    "\n",
    "# Initial state\n",
    "print(\"\\n\".join([f\"Initial state = [\", \"\\n\".join([\" , \".join([f\"{x: 10.2f}\" for x in row]) for row in s_prev_est_a_posteriori]), \"]\"]))\n",
    "\n",
    "for _ in range(3):\n",
    "    await get_speeds()         # Get the speeds\n",
    "    update_input()             # Update inputs\n",
    "    speeds_graph()             # Plot the graph of the speeds\n",
    "    s_prev_est_a_posteriori, P_prev_est_a_posteriori = kalman_filter(s_prev_est_a_posteriori, P_prev_est_a_posteriori)\n",
    "    print(\"\\n\".join([f\"Next state = [\", \"\\n\".join([\" , \".join([f\"{x: 10.2f}\" for x in row]) for row in s_prev_est_a_posteriori]), \"]\"]))\n",
    "\n",
    "node.send_set_variables(set_speeds(0, 0))\n",
    "\n",
    "disconnect_Thymio()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6041ce16",
   "metadata": {},
   "source": [
    "What need to be done with the Thymio used for the final demo:\n",
    "- Evaluate the Q and R matrices\n",
    "- Find the speed in pixels per second : $v_{px/s} = v_{thymio} \\cdot thymio\\_speed\\_to\\_mms \\cdot mm\\_to\\_px$\n",
    "- Time in the B matrix: update_time or 1.5*((nb_samples)*update_time)? Depends on how we decide to get the speed... "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e109c17-35c3-46cc-899d-fce3f57f88b2",
   "metadata": {},
   "source": [
    "## 5 Local Navigation\n",
    "<a id=\"local-navigation\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2340502c",
   "metadata": {},
   "source": [
    "## 6 Motion control\n",
    "<a id=\"local-navigation\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79056640",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c9867d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def motor_forward(motor_speed):\n",
    "    th.set_var(\"motor.left.target\", motor_speed)\n",
    "    th.set_var(\"motor.right.target\", motor_speed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c134cd74",
   "metadata": {},
   "source": [
    "## 7 Main\n",
    "<a id=\"main\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0068c5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fdfc6d8a",
   "metadata": {},
   "source": [
    "### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3cdbc42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from ipywidgets import interactive\n",
    "from matplotlib import colors\n",
    "from PIL import Image\n",
    "import math\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "089b65cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialisation\n",
    "\n",
    "\n",
    "# Main loop\n",
    "while 1:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b83967b-ca11-47f2-bd4e-4084c88d5829",
   "metadata": {},
   "source": [
    "## 8 Conclusion\n",
    "<a id=\"conclusion\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8458a9b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
